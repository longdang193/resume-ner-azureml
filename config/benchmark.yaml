# Benchmarking configuration for inference performance measurement

# Run mode configuration (inherits from HPO if null)
run:
  # Run mode determines overall behavior:
  # - null: Inherit from HPO config (default behavior)
  # - reuse_if_exists: Reuse existing benchmark results if found
  # - force_new: Always create new benchmark run (ignores existing)
  mode: force_new

benchmarking:
  # Batch sizes to test during benchmarking
  batch_sizes: [1]  # [1, 8, 16]
  
  # Number of iterations per batch size for statistical significance
  iterations: 10
  
  # Number of warmup iterations before measurement
  warmup_iterations: 10
  
  # Maximum sequence length for benchmarking
  max_length: 512
  
  # Device preference (null = auto-detect, "cuda", or "cpu")
  device: null
  
  # Test data source (relative to config dir or absolute path)
  # Can reference data config's test split or use separate test file
  # If null, will try dataset/test.json, then dataset/validation.json
  test_data: null

# Output configuration
output:
  # Filename for benchmark results
  filename: "benchmark.json"
  
