{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Tiny Smoke Dataset\n",
        "\n",
        "This notebook creates **tiny subsets** of the main dataset for fast smoke testing and validation.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Random sampling**: All datasets use random sampling with different seeds\n",
        "- **Seed-based organization**: All datasets are organized by seed (seed0, seed1, seed2, ...)\n",
        "- **Configurable count**: Number of datasets created is controlled by `config/test/dataset_creation.yaml`\n",
        "- **Edge case detection**: Helps catch issues like UTF-8 surrogates, special characters, and data format problems\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- Random datasets: `../dataset_tiny/seed{N}/{train.json, validation.json}` (N = 0, 1, 2, ..., num_datasets-1)\n",
        "  - Each seed uses random sampling with its respective seed value\n",
        "- Data config: `../config/data/resume_tiny.yaml`\n",
        "\n",
        "After running this, you can point `01_orchestrate_training.ipynb` at `resume_tiny.yaml` for fast orchestration tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded configuration from: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\\test\\dataset_creation.yaml\n",
            "Raw train file: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset\\train.json\n",
            "Base dataset directory: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\n",
            "Will create 5 random datasets (seed0, seed1, ..., seed4)\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "import sys\n",
        "\n",
        "# Auto-detect project root directory\n",
        "# Try to find root by looking for config directory\n",
        "notebook_dir = Path.cwd()\n",
        "# If we're in tests/, go up one level\n",
        "if notebook_dir.name == \"tests\":\n",
        "    root_dir = notebook_dir.parent\n",
        "else:\n",
        "    # Otherwise, assume we're at root or search upward\n",
        "    root_dir = notebook_dir\n",
        "    # Look for config/test/dataset_creation.yaml\n",
        "    for candidate in [root_dir, root_dir.parent]:\n",
        "        config_candidate = candidate / \"config\" / \"test\" / \"dataset_creation.yaml\"\n",
        "        if config_candidate.exists():\n",
        "            root_dir = candidate\n",
        "            break\n",
        "\n",
        "# Add src to path to import shared utilities\n",
        "if str(root_dir / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(root_dir / \"src\"))\n",
        "\n",
        "from shared.yaml_utils import load_yaml\n",
        "\n",
        "# Load configuration from central config file\n",
        "config_path = root_dir / \"config\" / \"test\" / \"dataset_creation.yaml\"\n",
        "config = load_yaml(config_path)\n",
        "\n",
        "# Extract configuration parameters (all paths are relative to project root)\n",
        "RAW_DATA_DIR = root_dir / config[\"raw_data_dir\"]\n",
        "RAW_TRAIN_FILE = RAW_DATA_DIR / config[\"raw_train_file\"]\n",
        "\n",
        "BASE_DATASET_DIR = root_dir / config[\"base_dataset_dir\"]\n",
        "NUM_TRAIN_SAMPLES = config[\"num_train_samples\"]\n",
        "NUM_VAL_SAMPLES = config[\"num_val_samples\"]\n",
        "MAX_TEXT_LENGTH_CHARS = config[\"max_text_length_chars\"]\n",
        "NUM_DATASETS = config[\"num_datasets\"]\n",
        "\n",
        "BASE_CONFIG_PATH = root_dir / config[\"base_config_path\"]\n",
        "TINY_CONFIG_PATH = root_dir / config[\"tiny_config_path\"]\n",
        "\n",
        "# Metadata for generated config\n",
        "DATASET_NAME = config[\"dataset_name\"]\n",
        "DATASET_VERSION = config[\"dataset_version\"]\n",
        "DATASET_DESCRIPTION = config[\"dataset_description\"]\n",
        "\n",
        "print(f\"Loaded configuration from: {config_path}\")\n",
        "print(f\"Raw train file: {RAW_TRAIN_FILE.resolve()}\")\n",
        "print(f\"Base dataset directory: {BASE_DATASET_DIR.resolve()}\")\n",
        "print(f\"Will create {NUM_DATASETS} random datasets (seed0, seed1, ..., seed{NUM_DATASETS-1})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 5960 total samples\n",
            "Filtered to 1836 samples with text <= 2500 chars\n"
          ]
        }
      ],
      "source": [
        "def load_full_dataset(train_file_path: Path) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load the full training dataset from JSON file.\n",
        "    \n",
        "    Args:\n",
        "        train_file_path: Path to train.json file.\n",
        "        \n",
        "    Returns:\n",
        "        List of sample dictionaries.\n",
        "        \n",
        "    Raises:\n",
        "        FileNotFoundError: If train file doesn't exist.\n",
        "        ValueError: If file doesn't contain a non-empty list.\n",
        "    \"\"\"\n",
        "    if not train_file_path.exists():\n",
        "        raise FileNotFoundError(f\"Train file not found: {train_file_path}\")\n",
        "    \n",
        "    with train_file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    if not isinstance(dataset, list) or not dataset:\n",
        "        raise ValueError(\"Expected train.json to be a non-empty list of samples\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "def filter_short_samples(\n",
        "    samples: List[Dict[str, Any]], \n",
        "    max_length: int\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Filter samples to those with text length <= max_length.\n",
        "    \n",
        "    Args:\n",
        "        samples: List of sample dictionaries.\n",
        "        max_length: Maximum text length in characters.\n",
        "        \n",
        "    Returns:\n",
        "        Filtered list of samples with short text.\n",
        "    \"\"\"\n",
        "    short_samples = []\n",
        "    for sample in samples:\n",
        "        text = sample.get(\"text\", \"\")\n",
        "        if isinstance(text, str) and len(text) <= max_length:\n",
        "            short_samples.append(sample)\n",
        "    return short_samples\n",
        "\n",
        "\n",
        "def save_dataset_split(\n",
        "    train_samples: List[Dict[str, Any]],\n",
        "    val_samples: List[Dict[str, Any]],\n",
        "    output_dir: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Save train and validation samples to JSON files.\n",
        "    \n",
        "    Args:\n",
        "        train_samples: List of training samples.\n",
        "        val_samples: List of validation samples.\n",
        "        output_dir: Directory to save the files.\n",
        "    \"\"\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    train_file = output_dir / \"train.json\"\n",
        "    val_file = output_dir / \"validation.json\"\n",
        "    \n",
        "    with train_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(train_samples, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    with val_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(val_samples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "full_dataset = load_full_dataset(RAW_TRAIN_FILE)\n",
        "short_samples = filter_short_samples(full_dataset, MAX_TEXT_LENGTH_CHARS)\n",
        "\n",
        "required_samples = NUM_TRAIN_SAMPLES + NUM_VAL_SAMPLES\n",
        "if len(short_samples) < required_samples:\n",
        "    raise ValueError(\n",
        "        f\"Not enough short samples (<= {MAX_TEXT_LENGTH_CHARS} chars). \"\n",
        "        f\"Found {len(short_samples)}, need at least {required_samples}.\"\n",
        "    )\n",
        "\n",
        "print(f\"Loaded {len(full_dataset)} total samples\")\n",
        "print(f\"Filtered to {len(short_samples)} samples with text <= {MAX_TEXT_LENGTH_CHARS} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created random dataset (seed=0) in c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\n",
            "  Train: 8 samples\n",
            "  Validation: 2 samples\n",
            "Created random dataset (seed=1) in c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed1\n",
            "  Train: 8 samples\n",
            "  Validation: 2 samples\n",
            "Created random dataset (seed=2) in c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed2\n",
            "  Train: 8 samples\n",
            "  Validation: 2 samples\n",
            "Created random dataset (seed=3) in c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed3\n",
            "  Train: 8 samples\n",
            "  Validation: 2 samples\n",
            "Created random dataset (seed=4) in c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed4\n",
            "  Train: 8 samples\n",
            "  Validation: 2 samples\n"
          ]
        }
      ],
      "source": [
        "def create_random_dataset(\n",
        "    short_samples: List[Dict[str, Any]],\n",
        "    seed: int,\n",
        "    output_dir: Path,\n",
        "    num_train: int,\n",
        "    num_val: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Create a randomly sampled dataset with a specific random seed.\n",
        "    \n",
        "    Args:\n",
        "        short_samples: List of filtered short samples.\n",
        "        seed: Random seed for reproducibility.\n",
        "        output_dir: Directory to save the dataset.\n",
        "        num_train: Number of training samples.\n",
        "        num_val: Number of validation samples.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    sampled = random.sample(short_samples, num_train + num_val)\n",
        "    \n",
        "    train_samples = sampled[:num_train]\n",
        "    val_samples = sampled[num_train:]\n",
        "    \n",
        "    save_dataset_split(train_samples, val_samples, output_dir)\n",
        "    \n",
        "    print(f\"Created random dataset (seed={seed}) in {output_dir}\")\n",
        "    print(f\"  Train: {len(train_samples)} samples\")\n",
        "    print(f\"  Validation: {len(val_samples)} samples\")\n",
        "\n",
        "\n",
        "# Create all random datasets (number controlled by config)\n",
        "for seed in range(NUM_DATASETS):\n",
        "    dataset_dir = BASE_DATASET_DIR / f\"seed{seed}\"\n",
        "    create_random_dataset(\n",
        "        short_samples,\n",
        "        seed,\n",
        "        dataset_dir,\n",
        "        NUM_TRAIN_SAMPLES,\n",
        "        NUM_VAL_SAMPLES\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All datasets are now created in Cell 3 using seed-based approach\n",
        "# This cell is kept for backward compatibility but is no longer needed\n",
        "# All seed-based datasets (including deterministic seed0) are created above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created data config: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\\data\\resume_tiny.yaml\n"
          ]
        }
      ],
      "source": [
        "def create_data_config(\n",
        "    base_config_path: Path,\n",
        "    output_config_path: Path,\n",
        "    dataset_name: str,\n",
        "    dataset_version: str,\n",
        "    description: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Create a data config YAML by copying and modifying a base config.\n",
        "    \n",
        "    Args:\n",
        "        base_config_path: Path to base data config.\n",
        "        output_config_path: Path to save the new config.\n",
        "        dataset_name: Name for the new dataset.\n",
        "        dataset_version: Version string for the dataset.\n",
        "        description: Description of the dataset.\n",
        "        \n",
        "    Raises:\n",
        "        FileNotFoundError: If base config doesn't exist.\n",
        "    \"\"\"\n",
        "    if not base_config_path.exists():\n",
        "        raise FileNotFoundError(f\"Base data config not found: {base_config_path}\")\n",
        "    \n",
        "    with base_config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    config[\"name\"] = dataset_name\n",
        "    config[\"version\"] = dataset_version\n",
        "    config[\"description\"] = description\n",
        "    \n",
        "    with output_config_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(config, f, sort_keys=False)\n",
        "\n",
        "\n",
        "# DATASET_NAME, DATASET_VERSION, and DATASET_DESCRIPTION are loaded from config in Cell 1\n",
        "\n",
        "create_data_config(\n",
        "    BASE_CONFIG_PATH,\n",
        "    TINY_CONFIG_PATH,\n",
        "    DATASET_NAME,\n",
        "    DATASET_VERSION,\n",
        "    DATASET_DESCRIPTION\n",
        ")\n",
        "\n",
        "print(f\"Created data config: {TINY_CONFIG_PATH.resolve()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
