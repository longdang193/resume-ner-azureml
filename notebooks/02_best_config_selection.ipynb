{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk ‚Üí drive restore ‚Üí MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Naming Convention\n",
    "\n",
    "- HPO experiment: `{experiment_name}-hpo-{backbone}` (contains parent runs with `code.stage=\"hpo\"` and refit runs with `code.stage=\"hpo_refit\"`)\n",
    "- Benchmark experiment: `{experiment_name}-benchmark` (single experiment, no backbone suffix)\n",
    "- Training experiment: `{experiment_name}-training` (exists but currently unused - refit runs are in HPO experiments)\n",
    "- Conversion experiment: `{experiment_name}-conversion`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def validate_path_before_mkdir(path: Path, context: str = \"directory\") -> Path:\n",
    "    \"\"\"\n",
    "    Validate path before creating directory to prevent creating invalid files.\n",
    "\n",
    "    Args:\n",
    "        path: Path to validate\n",
    "        context: Context string for error messages\n",
    "\n",
    "    Returns:\n",
    "        Validated and resolved path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If path is invalid\n",
    "    \"\"\"\n",
    "    if not path or not str(path):\n",
    "        raise ValueError(f\"Invalid {context} path: {path}\")\n",
    "\n",
    "    # Ensure path is absolute\n",
    "    if not path.is_absolute():\n",
    "        path = path.resolve()\n",
    "\n",
    "    path_str = str(path)\n",
    "\n",
    "    # Basic invalid cases\n",
    "    if path_str in (\"\", \".\", \"..\"):\n",
    "        raise ValueError(\n",
    "            f\"Invalid {context} path (too short or relative): {path_str}\"\n",
    "        )\n",
    "\n",
    "    # Split path\n",
    "    path_parts = path_str.replace(\"\\\\\", \"/\").split(\"/\")\n",
    "\n",
    "    # Check if last part looks like a version number (e.g. \"1.0.0\")\n",
    "    import re\n",
    "\n",
    "    if path_parts:\n",
    "        last_part = path_parts[-1]\n",
    "\n",
    "        if re.match(r\"^[\\d\\.]+$\", last_part):\n",
    "            # Reject single-part paths like \"1.0.0\"\n",
    "            if len(path_parts) == 1:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid {context} path (looks like version number): {path_str}\"\n",
    "                )\n",
    "\n",
    "    # Validate path has reasonable structure\n",
    "    if len(path_parts) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Invalid {context} path (too short, appears to be filename): {path_str}\"\n",
    "        )\n",
    "\n",
    "    # Safety: path exists but is a file\n",
    "    if path.exists() and path.is_file():\n",
    "        import logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.error(f\"Path exists as file, not directory: {path}\")\n",
    "        raise ValueError(\n",
    "            f\"Cannot create {context}, path exists as file: {path}\"\n",
    "        )\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "\n",
    "print(f\"‚úì Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - assuming repository already exists\n",
      "‚úì Repository root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "‚úì Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "‚úì Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "‚úì Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b gg_hpo https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b gg_hpo https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"‚úì Local environment detected - assuming repository already exists\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, try to find repo root\n",
    "    ROOT_DIR = Path.cwd()\n",
    "    # Try to find the repo root by looking for config directory\n",
    "    while ROOT_DIR.parent != ROOT_DIR:\n",
    "        if (ROOT_DIR / \"config\").exists() and (ROOT_DIR / \"src\").exists():\n",
    "            break\n",
    "        ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"‚úì Repository root: {ROOT_DIR}\")\n",
    "print(f\"‚úì Config directory: {CONFIG_DIR}\")\n",
    "print(f\"‚úì Source directory: {SRC_DIR}\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"‚úì Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded experiment config: resume_ner_baseline\n",
      "‚úì Loaded tags config\n",
      "‚úì Loaded best model selection config\n",
      "‚úì Loaded conversion config\n",
      "‚úì Loaded artifact acquisition config\n",
      "\n",
      "‚úì Experiment names defined:\n",
      "  - Benchmark: resume_ner_baseline-benchmark\n",
      "  - Training: resume_ner_baseline-training\n",
      "  - Conversion: resume_ner_baseline-conversion\n",
      "\n",
      "Note: Experiment discovery will happen after MLflow setup (Cell 4)\n"
     ]
    }
   ],
   "source": [
    "from orchestration.config_loader import load_experiment_config\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from shared.yaml_utils import load_yaml\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "print(f\"‚úì Loaded experiment config: {experiment_config.name}\")\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_yaml(CONFIG_DIR / \"tags.yaml\")\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"‚úì Loaded tags config\")\n",
    "print(f\"‚úì Loaded best model selection config\")\n",
    "print(f\"‚úì Loaded conversion config\")\n",
    "print(f\"‚úì Loaded artifact acquisition config\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"\\n‚úì Experiment names defined:\")\n",
    "print(f\"  - Benchmark: {benchmark_experiment_name}\")\n",
    "print(f\"  - Training: {training_experiment_name}\")\n",
    "print(f\"  - Conversion: {conversion_experiment_name}\")\n",
    "print(f\"\\nNote: Experiment discovery will happen after MLflow setup (Cell 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 17:19:45,853 - shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-04 17:19:48,688 - shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-04 17:19:48,689 - shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-04 17:19:48,691 - shared.mlflow_setup - INFO - Loading credentials from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-04 17:19:48,692 - shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-04 17:19:48,693 - shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-04 17:19:48,693 - shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-04 17:19:48,694 - shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-04 17:19:48,695 - shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n",
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-04 17:19:53,862 - shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-04 17:19:57,450 - shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "‚úì MLflow experiment: resume_ner_baseline-training\n",
      "\n",
      "‚úì Found 2 HPO experiment(s):\n",
      "  - resume_ner_baseline-hpo-distilbert (backbone: distilbert)\n",
      "  - resume_ner_baseline-hpo-distilroberta (backbone: distilroberta)\n",
      "‚úì Benchmark experiment: resume_ner_baseline-benchmark\n",
      "‚úì Training experiment: resume_ner_baseline-training (for final training runs)\n",
      "‚úì Conversion experiment: resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "from shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"‚úì MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úì Found {len(hpo_experiments)} HPO experiment(s):\")\n",
    "for backbone, exp_info in hpo_experiments.items():\n",
    "    print(f\"  - {exp_info['name']} (backbone: {backbone})\")\n",
    "\n",
    "if benchmark_experiment:\n",
    "    print(f\"‚úì Benchmark experiment: {benchmark_experiment['name']}\")\n",
    "else:\n",
    "    print(f\"‚ö† Benchmark experiment not found: {benchmark_experiment_name}\")\n",
    "\n",
    "print(f\"‚úì Training experiment: {training_experiment_name} (for final training runs)\")\n",
    "print(f\"‚úì Conversion experiment: {conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from orchestration.drive_backup import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"‚ö† Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"‚úì Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                return drive_store.restore(local_path, is_directory=is_directory)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"‚úì Google Drive mounted\")\n",
    "        print(f\"‚úì Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"‚ö† Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"‚úì Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs, join to training runs via grouping tags, and select the best model using normalized composite scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 17:19:57,794 - orchestration.jobs.selection.mlflow_selection - INFO - Finding best model from MLflow\n",
      "2026-01-04 17:19:57,795 - orchestration.jobs.selection.mlflow_selection - INFO -   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "2026-01-04 17:19:57,796 - orchestration.jobs.selection.mlflow_selection - INFO -   HPO experiments: 2\n",
      "2026-01-04 17:19:57,796 - orchestration.jobs.selection.mlflow_selection - INFO -   Objective metric: macro-f1\n",
      "2026-01-04 17:19:57,797 - orchestration.jobs.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30\n",
      "2026-01-04 17:19:57,797 - orchestration.jobs.selection.mlflow_selection - INFO - Querying benchmark runs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Finding best model from MLflow...\n",
      "   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "   HPO experiments: 2\n",
      "   Objective metric: macro-f1\n",
      "   Composite weights: F1=0.70, Latency=0.30\n",
      "\n",
      "üìä Querying benchmark runs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 17:19:58,239 - orchestration.jobs.selection.mlflow_selection - INFO - Found 5 finished benchmark runs\n",
      "2026-01-04 17:19:58,240 - orchestration.jobs.selection.mlflow_selection - INFO - Found 5 benchmark runs with required metrics and grouping tags\n",
      "2026-01-04 17:19:58,241 - orchestration.jobs.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 5 benchmark runs with required metrics and grouping tags\n",
      "\n",
      "üîó Preloading trial runs (metrics) and refit runs (artifacts) from HPO experiments...\n",
      "   resume_ner_baseline-hpo-distilbert: 45 finished runs, 18 trial runs, 9 refit runs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 17:19:58,785 - orchestration.jobs.selection.mlflow_selection - INFO - Built trial lookup with 10 unique (study_hash, trial_hash) pairs\n",
      "2026-01-04 17:19:58,786 - orchestration.jobs.selection.mlflow_selection - INFO - Built refit lookup with 10 unique (study_hash, trial_hash) pairs\n",
      "2026-01-04 17:19:58,786 - orchestration.jobs.selection.mlflow_selection - INFO - Joining benchmark runs with trial runs and refit runs...\n",
      "2026-01-04 17:19:58,786 - orchestration.jobs.selection.mlflow_selection - INFO - Found 5 candidate(s) with both benchmark and training metrics\n",
      "2026-01-04 17:19:58,787 - orchestration.jobs.selection.mlflow_selection - INFO - Computing composite scores...\n",
      "2026-01-04 17:19:58,787 - orchestration.jobs.selection.mlflow_selection - INFO - Best model selected:\n",
      "2026-01-04 17:19:58,788 - orchestration.jobs.selection.mlflow_selection - INFO -   Artifact Run ID: b0b34e9c-e0de-4d9c-8712-957c66d2781c\n",
      "2026-01-04 17:19:58,788 - orchestration.jobs.selection.mlflow_selection - INFO -   Trial Run ID: be0a7efb-d02e-48ca-b1f3-5867b03ca4aa\n",
      "2026-01-04 17:19:58,788 - orchestration.jobs.selection.mlflow_selection - INFO -   Backbone: distilroberta\n",
      "2026-01-04 17:19:58,789 - orchestration.jobs.selection.mlflow_selection - INFO -   F1 Score: 0.4422\n",
      "2026-01-04 17:19:58,789 - orchestration.jobs.selection.mlflow_selection - INFO -   Latency: 4.15 ms\n",
      "2026-01-04 17:19:58,790 - orchestration.jobs.selection.mlflow_selection - INFO -   Composite Score: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_ner_baseline-hpo-distilroberta: 5 finished runs, 2 trial runs, 1 refit runs\n",
      "   Built trial lookup with 10 unique (study_hash, trial_hash) pairs\n",
      "   Built refit lookup with 10 unique (study_hash, trial_hash) pairs\n",
      "\n",
      "üîó Joining benchmark runs with trial runs (metrics) and refit runs (artifacts)...\n",
      "   Found 5 candidate(s) with both benchmark and training metrics\n",
      "\n",
      "‚úÖ Best model selected:\n",
      "   Run ID: b0b34e9c-e0de-4d9c-8712-957c66d2781c\n",
      "   Experiment: resume_ner_baseline-hpo-distilroberta\n",
      "   Backbone: distilroberta\n",
      "   F1 Score: 0.4422\n",
      "   Latency: 4.15 ms\n",
      "   Composite Score: 0.9206\n",
      "[ACQUIRE] Acquiring checkpoint for run b0b34e9c...\n",
      "\n",
      "[Strategy 1] Local disk selection...\n",
      "\n",
      "[Strategy 3] MLflow download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [04:09<00:00, 249.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [OK] Downloaded checkpoint from MLflow: \"c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\best_model_checkpoint\\best_trial_checkpoint\\best_trial_checkpoint\"\n",
      "\n",
      "‚úì Best model checkpoint available at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\best_model_checkpoint\\best_trial_checkpoint\\best_trial_checkpoint\n"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs.selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from orchestration.jobs.selection.artifact_acquisition import (\n",
    "    acquire_best_model_checkpoint as acquire_checkpoint_improved,\n",
    ")\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "\n",
    "def acquire_best_model_checkpoint(\n",
    "    best_run_info: Dict[str, Any],\n",
    "    root_dir: Path,\n",
    "    config_dir: Path,\n",
    "    restore_from_drive: Optional[Callable[[Path, bool], bool]] = None,\n",
    ") -> Path:\n",
    "    \"\"\"Wrapper for artifact acquisition function.\"\"\"\n",
    "    return acquire_checkpoint_improved(\n",
    "        best_run_info=best_run_info,\n",
    "        root_dir=root_dir,\n",
    "        config_dir=config_dir,\n",
    "        acquisition_config=acquisition_config,\n",
    "        selection_config=selection_config,\n",
    "        platform=PLATFORM,\n",
    "        restore_from_drive=restore_from_drive,\n",
    "        in_colab=IN_COLAB,\n",
    "    )\n",
    "\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Find best model\n",
    "best_model = find_best_model_from_mlflow(\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    "    hpo_experiments=hpo_experiments,\n",
    "    tags_config=tags_config,\n",
    "    selection_config=selection_config,\n",
    "    use_python_filtering=True,\n",
    ")\n",
    "\n",
    "if best_model is None:\n",
    "    raise ValueError(\"Could not find best model from MLflow.\")\n",
    "\n",
    "# Acquire checkpoint\n",
    "OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"best_model_selection\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Best model checkpoint available at: {best_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is final training using standardized tag (from config)\n",
    "stage_tag = tags_config[\"process\"][\"stage\"]\n",
    "trained_on_full_data_tag = tags_config[\"training\"][\"trained_on_full_data\"]\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "\n",
    "# Check if used full data using standardized tag\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "# Decision\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "print(f\"Selected run is final_training: {is_final_training}\")\n",
    "print(f\"Selected run used full data: {used_full_data}\")\n",
    "print(f\"Decision: {'Skip final training' if SKIP_FINAL_TRAINING else 'Run final training'}\")\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    print(\"‚úì Using selected checkpoint directly (skipping retraining)\")\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n",
    "else:\n",
    "    print(\"üîÑ Selected run is not final training - will proceed with final training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Training (Conditional)\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"üîÑ Starting final training with best configuration...\")\n",
    "    \n",
    "    from orchestration.final_training_config import create_final_training_config\n",
    "    from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "    from orchestration.fingerprints import compute_spec_fp, compute_exec_fp\n",
    "    from orchestration.config_loader import load_all_configs\n",
    "    from shared.platform_detection import detect_platform\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Extract best config from MLflow run\n",
    "    best_params = best_model[\"params\"]\n",
    "    \n",
    "    # Create final training config\n",
    "    all_configs = load_all_configs(experiment_config)\n",
    "    environment = detect_platform()\n",
    "    \n",
    "    # Compute fingerprints\n",
    "    spec_fp = compute_spec_fp(\n",
    "        model_config=all_configs.get(\"model\", {}),\n",
    "        data_config=all_configs.get(\"data\", {}),\n",
    "        train_config=all_configs.get(\"train\", {}),\n",
    "        seed=int(best_params.get(\"random_seed\", 42)),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        git_sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "            cwd=ROOT_DIR,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        git_sha = None\n",
    "    \n",
    "    exec_fp = compute_exec_fp(\n",
    "        git_sha=git_sha,\n",
    "        env_config=all_configs.get(\"env\", {}),\n",
    "    )\n",
    "    \n",
    "    # Create training context\n",
    "    backbone_name = best_params.get(\"backbone\", \"distilbert\")\n",
    "    if \"-\" in backbone_name:\n",
    "        backbone_name = backbone_name.split(\"-\")[0]\n",
    "    \n",
    "    from orchestration.final_training_config import _compute_next_variant\n",
    "    variant = _compute_next_variant(\n",
    "        ROOT_DIR,\n",
    "        CONFIG_DIR,\n",
    "        spec_fp,\n",
    "        exec_fp,\n",
    "        backbone_name,\n",
    "    )\n",
    "    \n",
    "    training_context = create_naming_context(\n",
    "        process_type=\"final_training\",\n",
    "        model=backbone_name,\n",
    "        spec_fp=spec_fp,\n",
    "        exec_fp=exec_fp,\n",
    "        environment=environment,\n",
    "        variant=variant,\n",
    "    )\n",
    "    \n",
    "    final_output_dir = build_output_path(ROOT_DIR, training_context)\n",
    "    \n",
    "    # Create final training config with best hyperparameters\n",
    "    final_training_config = {\n",
    "        \"backbone\": best_params.get(\"backbone\"),\n",
    "        \"learning_rate\": float(best_params.get(\"learning_rate\", 2e-5)),\n",
    "        \"batch_size\": int(best_params.get(\"batch_size\", 16)),\n",
    "        \"dropout\": float(best_params.get(\"dropout\", 0.1)),\n",
    "        \"weight_decay\": float(best_params.get(\"weight_decay\", 0.01)),\n",
    "        \"epochs\": int(best_params.get(\"epochs\", 10)),\n",
    "        \"random_seed\": int(best_params.get(\"random_seed\", 42)),\n",
    "        \"early_stopping_enabled\": best_params.get(\"early_stopping_enabled\", \"true\").lower() == \"true\",\n",
    "        \"use_combined_data\": True,  # Use full dataset for final training\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì Final training config: {final_training_config}\")\n",
    "    print(f\"‚úì Output directory: {final_output_dir}\")\n",
    "    \n",
    "    # Load dataset path from config\n",
    "    from shared.yaml_utils import load_yaml\n",
    "    data_config = all_configs.get(\"data\", {})\n",
    "    dataset_path = data_config.get(\"dataset_path\", \"data/resume_tiny\")\n",
    "    DATASET_LOCAL_PATH = ROOT_DIR / dataset_path if not Path(dataset_path).is_absolute() else Path(dataset_path)\n",
    "    \n",
    "    # Run training as a module\n",
    "    training_args = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"training.train\",\n",
    "        \"--data-asset\",\n",
    "        str(DATASET_LOCAL_PATH),\n",
    "        \"--config-dir\",\n",
    "        str(CONFIG_DIR),\n",
    "        \"--backbone\",\n",
    "        final_training_config[\"backbone\"],\n",
    "        \"--learning-rate\",\n",
    "        str(final_training_config[\"learning_rate\"]),\n",
    "        \"--batch-size\",\n",
    "        str(final_training_config[\"batch_size\"]),\n",
    "        \"--dropout\",\n",
    "        str(final_training_config[\"dropout\"]),\n",
    "        \"--weight-decay\",\n",
    "        str(final_training_config[\"weight_decay\"]),\n",
    "        \"--epochs\",\n",
    "        str(final_training_config[\"epochs\"]),\n",
    "        \"--random-seed\",\n",
    "        str(final_training_config[\"random_seed\"]),\n",
    "        \"--early-stopping-enabled\",\n",
    "        str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
    "        \"--use-combined-data\",\n",
    "        str(final_training_config[\"use_combined_data\"]).lower(),\n",
    "    ]\n",
    "    \n",
    "    training_env = os.environ.copy()\n",
    "    training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
    "    \n",
    "    # Add src directory to PYTHONPATH\n",
    "    pythonpath = training_env.get(\"PYTHONPATH\", \"\")\n",
    "    if pythonpath:\n",
    "        training_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "    else:\n",
    "        training_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "    \n",
    "    mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "    if mlflow_tracking_uri:\n",
    "        training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "    training_env[\"MLFLOW_EXPERIMENT_NAME\"] = training_experiment_name\n",
    "    \n",
    "    print(\"üîÑ Running final training...\")\n",
    "    result = subprocess.run(\n",
    "        training_args,\n",
    "        cwd=ROOT_DIR,\n",
    "        env=training_env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Final training failed with return code {result.returncode}\\n\"\n",
    "            f\"STDOUT: {result.stdout}\\n\"\n",
    "            f\"STDERR: {result.stderr}\"\n",
    "        )\n",
    "    else:\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "    \n",
    "    # Set final checkpoint directory\n",
    "    final_checkpoint_dir = final_output_dir / \"checkpoint\"\n",
    "    if not final_checkpoint_dir.exists():\n",
    "        # Try actual checkpoint location\n",
    "        actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "        if actual_checkpoint.exists():\n",
    "            final_checkpoint_dir = actual_checkpoint\n",
    "    \n",
    "    print(f\"‚úì Final training completed. Checkpoint: {final_checkpoint_dir}\")\n",
    "    \n",
    "    # Try to set tag in MLflow run\n",
    "    try:\n",
    "        from orchestration.jobs.tracking.finder.run_finder import find_mlflow_run\n",
    "        from orchestration.jobs.tracking.trackers.training_tracker import MLflowTrainingTracker\n",
    "        \n",
    "        training_tracker = MLflowTrainingTracker(training_experiment_name)\n",
    "        report = find_mlflow_run(\n",
    "            experiment_name=training_experiment_name,\n",
    "            context=training_context,\n",
    "            output_dir=final_output_dir,\n",
    "            strict=False,\n",
    "            root_dir=ROOT_DIR,\n",
    "            config_dir=CONFIG_DIR,\n",
    "        )\n",
    "        \n",
    "        if report.found and report.run_id:\n",
    "            with mlflow.start_run(run_id=report.run_id):\n",
    "                mlflow.set_tag(\"code.trained_on_full_data\", \"true\")\n",
    "                print(f\"‚úì Set code.trained_on_full_data tag in MLflow run {report.run_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not set MLflow tag: {e}\")\n",
    "else:\n",
    "    print(\"‚úì Skipping final training - using selected checkpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Starting model conversion to ONNX...\")\n",
    "\n",
    "from orchestration.jobs.tracking.trackers.conversion_tracker import MLflowConversionTracker\n",
    "from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "from orchestration.fingerprints import build_parent_training_id, compute_conv_fp\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup conversion tracker\n",
    "conversion_tracker = MLflowConversionTracker(conversion_experiment_name)\n",
    "\n",
    "# Determine checkpoint path\n",
    "checkpoint_path = final_checkpoint_dir\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    raise ValueError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "\n",
    "# Extract backbone from best model\n",
    "backbone = best_model[\"backbone\"]\n",
    "\n",
    "# Conversion settings\n",
    "# Load conversion settings from config\n",
    "quantization = conversion_config[\"onnx\"][\"quantization\"]\n",
    "opset_version = conversion_config[\"onnx\"][\"opset_version\"]\n",
    "conversion_target = conversion_config[\"target\"][\"format\"]\n",
    "\n",
    "print(f\"‚úì Checkpoint path: {checkpoint_path}\")\n",
    "print(f\"‚úì Backbone: {backbone}\")\n",
    "print(f\"‚úì Conversion target: {conversion_target}\")\n",
    "print(f\"‚úì Quantization: {quantization}\")\n",
    "print(f\"‚úì ONNX opset version: {opset_version}\")\n",
    "\n",
    "# Extract or compute fingerprints for conversion context\n",
    "# Try to get from best model tags first\n",
    "spec_fp = best_model[\"tags\"].get(\"code.spec_fp\")\n",
    "exec_fp = best_model[\"tags\"].get(\"code.exec_fp\")\n",
    "variant = int(best_model[\"tags\"].get(\"code.variant\", \"1\")) if best_model[\"tags\"].get(\"code.variant\") else 1\n",
    "\n",
    "# If not in tags, compute from config\n",
    "if not spec_fp or not exec_fp:\n",
    "    from orchestration.fingerprints import compute_spec_fp, compute_exec_fp\n",
    "    from orchestration.config_loader import load_all_configs\n",
    "    import subprocess\n",
    "    \n",
    "    all_configs = load_all_configs(experiment_config)\n",
    "    \n",
    "    spec_fp = compute_spec_fp(\n",
    "        model_config=all_configs.get(\"model\", {}),\n",
    "        data_config=all_configs.get(\"data\", {}),\n",
    "        train_config=all_configs.get(\"train\", {}),\n",
    "        seed=int(best_model[\"params\"].get(\"random_seed\", 42)),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        git_sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "            cwd=ROOT_DIR,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        git_sha = None\n",
    "    \n",
    "    exec_fp = compute_exec_fp(\n",
    "        git_sha=git_sha,\n",
    "        env_config=all_configs.get(\"env\", {}),\n",
    "    )\n",
    "\n",
    "# Build parent_training_id\n",
    "parent_training_id = build_parent_training_id(spec_fp, exec_fp, variant)\n",
    "\n",
    "# Compute conversion fingerprint\n",
    "conv_fp = compute_conv_fp(backbone, quantization, opset_version)\n",
    "\n",
    "# Create conversion context\n",
    "conversion_context = create_naming_context(\n",
    "    process_type=\"conversion\",\n",
    "    model=backbone.split(\"-\")[0] if \"-\" in backbone else backbone,\n",
    "    environment=PLATFORM,\n",
    "    parent_training_id=parent_training_id,\n",
    "    conv_fp=conv_fp,\n",
    ")\n",
    "\n",
    "# Build conversion output path using canonical structure\n",
    "conversion_output_dir = build_output_path(ROOT_DIR, conversion_context)\n",
    "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Conversion output: {conversion_output_dir}\")\n",
    "\n",
    "# Run conversion\n",
    "try:\n",
    "    # Import conversion function\n",
    "    from model_conversion.onnx_exporter import export_to_onnx\n",
    "    \n",
    "    # Start conversion run\n",
    "    conversion_run_name = f\"conversion_{backbone}_{conversion_target}\"\n",
    "    \n",
    "    with conversion_tracker.start_conversion_run(\n",
    "        run_name=conversion_run_name,\n",
    "        conversion_type=conversion_target,\n",
    "        source_training_run=best_model[\"run_id\"],\n",
    "        output_dir=conversion_output_dir,\n",
    "    ) as conversion_handle:\n",
    "        \n",
    "        # Log conversion parameters\n",
    "        conversion_tracker.log_conversion_parameters(\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            conversion_target=conversion_target,\n",
    "            quantization=quantization,\n",
    "            opset_version=opset_version,\n",
    "            backbone=backbone,\n",
    "        )\n",
    "        \n",
    "        # Perform conversion\n",
    "        onnx_model_path = export_to_onnx(\n",
    "            checkpoint_dir=checkpoint_path,\n",
    "            output_dir=conversion_output_dir,\n",
    "            quantize_int8=(quantization == \"int8\"),\n",
    "        )\n",
    "        \n",
    "        # Calculate original checkpoint size for compression ratio\n",
    "        original_size = None\n",
    "        if checkpoint_path.exists():\n",
    "            total_size = sum(f.stat().st_size for f in checkpoint_path.rglob(\"*\") if f.is_file())\n",
    "            original_size = total_size / (1024 * 1024)  # MB\n",
    "        \n",
    "        # Log conversion results\n",
    "        conversion_tracker.log_conversion_results(\n",
    "            conversion_success=True,\n",
    "            onnx_model_path=onnx_model_path,\n",
    "            original_checkpoint_size=original_size,\n",
    "            smoke_test_passed=True,  # Could add actual smoke test\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Model converted successfully!\")\n",
    "        print(f\"‚úì ONNX model saved to: {onnx_model_path}\")\n",
    "        \n",
    "        if conversion_handle:\n",
    "            print(f\"‚úì Conversion logged to MLflow run: {conversion_handle.run_id}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Conversion failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Summary\n",
    "\n",
    "Display summary of best model selection, artifact locations, and conversion status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì Best Model Selected:\")\n",
    "print(f\"   Run ID: {best_model['run_id']}\")\n",
    "print(f\"   Run Name: {best_model['run_name']}\")\n",
    "print(f\"   Backbone: {best_model['backbone']}\")\n",
    "print(f\"   Macro-F1: {best_model['primary_metric']:.4f}\")\n",
    "if best_model['latency_ms']:\n",
    "    print(f\"   Latency: {best_model['latency_ms']:.2f} ms\")\n",
    "if best_model['throughput']:\n",
    "    print(f\"   Throughput: {best_model['throughput']:.2f} docs/sec\")\n",
    "print(f\"   Confidence: {best_model['confidence']}\")\n",
    "print(f\"   Source: {best_model['source_type']}\")\n",
    "\n",
    "print(f\"\\n‚úì Artifacts:\")\n",
    "print(f\"   Checkpoint: {final_checkpoint_dir}\")\n",
    "\n",
    "if 'onnx_model_path' in locals():\n",
    "    print(f\"   ONNX Model: {onnx_model_path}\")\n",
    "\n",
    "print(f\"\\n‚úì MLflow Tracking:\")\n",
    "print(f\"   Training Experiment: {training_experiment_name}\")\n",
    "print(f\"   Benchmark Experiment: {benchmark_experiment_name}\")\n",
    "print(f\"   Conversion Experiment: {conversion_experiment_name}\")\n",
    "\n",
    "print(f\"\\n‚úì Final Training:\")\n",
    "print(f\"   {'Skipped (already final training on full data)' if SKIP_FINAL_TRAINING else 'Completed'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Best model selection and conversion pipeline completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
