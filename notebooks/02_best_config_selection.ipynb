{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "**Prerequisites**: Run `01_orchestrate_training_colab.ipynb` first to:\n",
    "- Train models via HPO\n",
    "- Run benchmarking on best trials (using `evaluation.benchmarking.benchmark_best_trials`)\n",
    "\n",
    "Then this notebook:\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk ‚Üí drive restore ‚Üí MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"‚úì Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"‚úì All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - detecting repository root...\n",
      "‚úì Repository: /workspaces/resume-ner-azureml (config=config, src=src)\n",
      "‚úì Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"‚úì Local environment detected - detecting repository root...\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, detect repo root by searching for config/ and src/ directories\n",
    "    # Start from current working directory and search up\n",
    "    current_dir = Path.cwd()\n",
    "    ROOT_DIR = None\n",
    "    \n",
    "    # Check current directory first\n",
    "    if (current_dir / \"config\").exists() and (current_dir / \"src\").exists():\n",
    "        ROOT_DIR = current_dir\n",
    "    else:\n",
    "        # Search up the directory tree\n",
    "        for parent in current_dir.parents:\n",
    "            if (parent / \"config\").exists() and (parent / \"src\").exists():\n",
    "                ROOT_DIR = parent\n",
    "                break\n",
    "    \n",
    "    if ROOT_DIR is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not find repository root. Searched from: {current_dir}\\n\"\n",
    "            \"Please ensure you're running from within the repository or a subdirectory.\"\n",
    "        )\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"‚úì Repository: {ROOT_DIR} (config={CONFIG_DIR.name}, src={SRC_DIR.name})\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"‚úì Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: orchestration module is deprecated. Use 'infrastructure', 'common', or 'data' modules instead. This will be removed in 2 releases.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "2026-01-13 21:20:00,535 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded configs: experiment=resume_ner_baseline, tags, selection, conversion, acquisition\n",
      "‚úì Experiment names: benchmark=resume_ner_baseline-benchmark, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'constants' from 'orchestration' is deprecated. Please import from 'constants' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'fingerprints' from 'orchestration' is deprecated. Please import from 'fingerprints' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'metadata/index_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'metadata/metadata_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'drive_backup' from 'orchestration' is deprecated. Please import from 'storage' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'benchmark_utils' from 'orchestration' is deprecated. Please import from 'benchmarking.utils' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'config_loader' from 'orchestration' is deprecated. Please import from 'config.loader' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'conversion_config' from 'orchestration' is deprecated. Please import from 'config.conversion' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'final_training_config' from 'orchestration' is deprecated. Please import from 'config.training' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'environment' from 'orchestration' is deprecated. Please import from 'config.environment' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'config_compat' from 'orchestration' is deprecated. Please import from 'config.validation' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'data_assets' from 'orchestration' is deprecated. Please import from 'azureml.data_assets' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'normalize' from 'orchestration' is deprecated. Please import from 'core.normalize' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_20022/2783883798.py:5: DeprecationWarning: Importing 'tokens' from 'orchestration' is deprecated. Please import from 'core.tokens' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n"
     ]
    }
   ],
   "source": [
    "from infrastructure.config.loader import load_experiment_config\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "    # Note: Still in orchestration.jobs.tracking for now\n",
    "from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_tags_registry(CONFIG_DIR)\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"‚úì Loaded configs: experiment={experiment_config.name}, tags, selection, conversion, acquisition\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"‚úì Experiment names: benchmark={benchmark_experiment_name}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 21:20:02,571 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-13 21:20:02,572 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-13 21:20:02,573 - common.shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-13 21:20:02,574 - common.shared.mlflow_setup - INFO - Loading credentials from /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-13 21:20:02,576 - common.shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-13 21:20:02,578 - common.shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-13 21:20:02,578 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-13 21:20:02,579 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-13 21:20:02,581 - common.shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì azureml.mlflow is available - Azure ML tracking will be used if configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-13 21:20:03,918 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-13 21:20:04,813 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "‚úì MLflow experiment: resume_ner_baseline-training\n",
      "‚úì Experiments: 2 HPO (distilbert, distilroberta), benchmark=found, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "# Check if azureml.mlflow is available\n",
    "try:\n",
    "    import azureml.mlflow  # noqa: F401\n",
    "    print(\"‚úì azureml.mlflow is available - Azure ML tracking will be used if configured\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† azureml.mlflow is not available - will fallback to local SQLite tracking\")\n",
    "    print(\"  To use Azure ML tracking, install: pip install azureml-mlflow\")\n",
    "    print(\"  Then restart the kernel and re-run this cell\")\n",
    "\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"‚úì MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "hpo_backbones = \", \".join(hpo_experiments.keys())\n",
    "print(f\"‚úì Experiments: {len(hpo_experiments)} HPO ({hpo_backbones}), benchmark={'found' if benchmark_experiment else 'not found'}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"‚ö† Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"‚úì Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"‚úì Google Drive mounted\")\n",
    "        print(f\"‚úì Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"‚ö† Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"‚úì Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optional - Run Benchmarking on Champions\n",
    "\n",
    "**Optional Step**: If you haven't run benchmarking in `01_orchestrate_training_colab.ipynb`, you can run it here before selecting the best model. This step will:\n",
    "1. Select champions (best trials) from HPO runs using Phase 2 selection logic\n",
    "2. Run benchmarking on each champion to measure inference performance\n",
    "3. Save benchmark results to MLflow for use in Step 7\n",
    "\n",
    "**Note**: If benchmark runs already exist in MLflow, you can skip this step and proceed directly to Step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running benchmarking on champions...\n",
      "‚úì Found 2 HPO experiment(s)\n",
      "üèÜ Selecting champions per backbone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 21:20:05,402 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'\n",
      "2026-01-13 21:20:05,597 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)\n",
      "2026-01-13 21:20:05,597 - evaluation.selection.trial_finder - INFO - Filtered out 1 parent run(s) (only child/trial runs have metrics). 2 child runs remaining.\n",
      "2026-01-13 21:20:05,598 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 21:20:05,599 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)\n",
      "2026-01-13 21:20:05,871 - evaluation.selection.trial_finder - INFO - Found refit run 0f5c4ff8-c3c... for champion trial 1c7b99ca-816... (selected latest from 1 refit run(s))\n",
      "2026-01-13 21:20:06,140 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilroberta, trying legacy stage='hpo'\n",
      "2026-01-13 21:20:06,277 - evaluation.selection.trial_finder - INFO - Found 2 runs with stage tag for distilroberta (backbone=distilroberta)\n",
      "2026-01-13 21:20:06,278 - evaluation.selection.trial_finder - INFO - Filtered out 1 parent run(s) (only child/trial runs have metrics). 1 child runs remaining.\n",
      "2026-01-13 21:20:06,279 - evaluation.selection.trial_finder - INFO - Grouped runs for distilroberta: 1 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 21:20:06,279 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilroberta (0 skipped due to min_trials requirement)\n",
      "2026-01-13 21:20:06,505 - evaluation.selection.trial_finder - INFO - Found refit run feab317f-0aa... for champion trial 3ed121cb-2e0... (selected latest from 1 refit run(s))\n",
      "2026-01-13 21:20:06,657 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/notebooks/config/tags.yaml, using defaults\n",
      "2026-01-13 21:20:06,777 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=483b7b56039b65df...)\n",
      "2026-01-13 21:20:07,091 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Strategy 1: Checking local disk for checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta (study=7eabeb17..., trial=2a27e95a...)\n",
      "2026-01-13 21:20:07,093 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] ‚úì Found local checkpoint at: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-7eabeb17/trial-2a27e95a/refit/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è  Skipping 1 already-benchmarked champion(s)\n",
      "\n",
      "üìä Benchmarking 1 champion(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 21:20:07,284 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Copied local checkpoint to: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_7eabeb17_2a27e95a\n",
      "2026-01-13 21:20:07,304 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilroberta (2a27e95ac108a0ff)...\n",
      "2026-01-13 21:20:07,305 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=3ed121cb-2e0..., refit=feab317f-0aa..., sweep=None...\n",
      "2026-01-13 21:20:07,306 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_7eabeb17_2a27e95a --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-7eabeb17/trial-2a27e95a/bench-fcfe0227/benchmark.json\n",
      "2026-01-13 21:20:10,128 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_7eabeb17_2a27e95a\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_7eabeb17_2a27e95a...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_7eabeb17_2a27e95a...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 240.54 ms\n",
      "  P95 latency: 371.52 ms\n",
      "  Throughput: 4.16 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-7eabeb17/trial-2a27e95a/bench-fcfe0227/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-7eabeb17/trial-2a27e95a/bench-fcfe0227/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 21:20:20,625 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=2a27e95ac108a0ff, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-13 21:20:20,641 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 21:20:20,642 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 21:20:20,643 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:5c439a30a476f3640c8308a626e7ec2e3f76..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 21:20:20,654 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 3 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 21:20:20,655 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-13 21:20:20,656 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-13 21:20:20,657 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ‚úì Successfully reserved version 1 for counter_key resume-ner:benchmarking:5c439a30a476f3640c8308a626... (run_id: pending_2026...)\n",
      "2026-01-13 21:20:20,658 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilroberta_benchmark_study-7eabeb17_trial-2a27e95a_bench-fcfe0227_1\n",
      "2026-01-13 21:20:20,855 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilroberta', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='2a27e95ac108a0ff', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='7eabeb17d0cf1eadea36d4a628e7d5ecfced553eed328f3120311a7a2bd0ceec', trial_key_hash='2a27e95ac108a0ff129a8fc5dd1cf98f27c99385570d953fc39f39c3008d6219', benchmark_config_hash='fcfe0227aaa5d5def0429d8fe83aa518a92bfb771539f3cd2f906280b151a0b7'), study_key_hash=7eabeb17d0cf1ead..., trial_key_hash=2a27e95ac108a0ff..., context.model=distilroberta, context.process_type=benchmarking\n",
      "2026-01-13 21:20:20,858 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilroberta, code.stage=benchmarking\n",
      "2026-01-13 21:20:20,859 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=3ed121cb-2e0..., parent_kind=trial\n",
      "2026-01-13 21:20:21,001 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 1 in run name 'local_distilroberta_benchmark_study-7eabeb17_trial-2a27e95a_bench-fcfe0227_1'\n",
      "2026-01-13 21:20:21,002 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 21:20:21,003 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 21:20:21,003 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 1 for run 373816cd-bfb..., counter_key=resume-ner:benchmarking:5c439a30a476f3640c8308a626...\n",
      "2026-01-13 21:20:21,004 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:5c439a30a476f3640c8308a626e7ec2e3f76..., version=1, run_id=373816cd-bfb..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 21:20:21,005 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 4 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 21:20:21,005 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ‚úì Found and committed reservation: version=1, status changed from 'reserved' to 'committed', run_id=373816cd-bfb..., counter_key=resume-ner:benchmarking:5c439a30a476f3640c8308a626...\n",
      "2026-01-13 21:20:21,007 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ‚úì Successfully saved committed version 1 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 21:20:21,007 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] ‚úì Successfully committed version 1 for benchmark run 373816cd-bfb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run local_distilroberta_benchmark_study-7eabeb17_trial-2a27e95a_bench-fcfe0227_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/373816cd-bfb5-4632-a5fd-2ae4fb326a43\n",
      "üß™ View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 21:20:24,498 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-7eabeb17/trial-2a27e95a/bench-fcfe0227/benchmark.json\n",
      "2026-01-13 21:20:24,498 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Benchmarking complete. Results saved to MLflow experiment: resume_ner_baseline-benchmark\n"
     ]
    }
   ],
   "source": [
    "# Optional: Run benchmarking on champions if not already done\n",
    "# Skip this cell if benchmark runs already exist in MLflow\n",
    "\n",
    "RUN_BENCHMARKING = True  # Set to True to run benchmarking\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    from evaluation.selection.trial_finder import select_champions_for_backbones\n",
    "    from evaluation.benchmarking.orchestrator import (\n",
    "        benchmark_champions,\n",
    "        filter_missing_benchmarks,\n",
    "    )\n",
    "    from infrastructure.naming.mlflow.hpo_keys import (\n",
    "        compute_data_fingerprint,\n",
    "        compute_eval_fingerprint,\n",
    "    )\n",
    "    from common.shared.platform_detection import detect_platform\n",
    "    from common.shared.yaml_utils import load_yaml\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    from infrastructure.naming.experiments import build_mlflow_experiment_name\n",
    "    from orchestration import STAGE_HPO\n",
    "    from orchestration.jobs.tracking.mlflow_tracker import MLflowBenchmarkTracker\n",
    "    \n",
    "    print(\"üîÑ Running benchmarking on champions...\")\n",
    "    \n",
    "    # Step 1: Load configs and setup MLflow client\n",
    "    from infrastructure.config.loader import load_experiment_config, load_all_configs\n",
    "    \n",
    "    selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "    benchmark_config = load_yaml(CONFIG_DIR / \"benchmark.yaml\")\n",
    "    \n",
    "    # Load all configs using the standard loader (consistent with other notebooks)\n",
    "    experiment_config = load_experiment_config(CONFIG_DIR, experiment_name)\n",
    "    configs = load_all_configs(experiment_config)\n",
    "    data_config = configs.get(\"data\", {})\n",
    "    hpo_config = configs.get(\"hpo\", {})\n",
    "    mlflow_client = MlflowClient()\n",
    "    \n",
    "    # Step 2: Build HPO experiments dict (backbone -> {name, id})\n",
    "    hpo_experiments = {}\n",
    "    for exp in mlflow_client.search_experiments():\n",
    "        if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "            backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "            hpo_experiments[backbone] = {\n",
    "                \"name\": exp.name,\n",
    "                \"id\": exp.experiment_id\n",
    "            }\n",
    "    \n",
    "    if not hpo_experiments:\n",
    "        print(\"‚ö† No HPO experiments found. Skipping benchmarking.\")\n",
    "    else:\n",
    "        # Step 3: Select champions per backbone (Phase 2)\n",
    "        backbone_values = list(hpo_experiments.keys())\n",
    "        print(f\"‚úì Found {len(hpo_experiments)} HPO experiment(s)\")\n",
    "        print(\"üèÜ Selecting champions per backbone...\")\n",
    "        \n",
    "        champions = select_champions_for_backbones(\n",
    "            backbone_values=backbone_values,\n",
    "            hpo_experiments=hpo_experiments,\n",
    "            selection_config=selection_config,\n",
    "            mlflow_client=mlflow_client,\n",
    "        )\n",
    "        \n",
    "        if not champions:\n",
    "            print(\"‚ö† No champions found. Skipping benchmarking.\")\n",
    "            \n",
    "            # Add diagnostics to help debug\n",
    "            print(\"\\nüîç Diagnostics:\")\n",
    "            from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "            tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "            \n",
    "            for backbone, exp_info in hpo_experiments.items():\n",
    "                backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "                runs = mlflow_client.search_runs(\n",
    "                    experiment_ids=[exp_info[\"id\"]],\n",
    "                    filter_string=\"\",\n",
    "                    max_results=100,\n",
    "                )\n",
    "                finished_runs = [r for r in runs if r.info.status == \"FINISHED\"]\n",
    "                print(f\"\\n  {backbone}: {len(finished_runs)} finished run(s)\")\n",
    "                \n",
    "                # Check for required tags for champion selection\n",
    "                if finished_runs:\n",
    "                    # Separate parent and child runs\n",
    "                    # Child runs: have mlflow.parentRunId tag\n",
    "                    # Parent runs: don't have mlflow.parentRunId tag\n",
    "                    child_runs = [r for r in finished_runs if r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    parent_run_ids = {r.data.tags.get(\"mlflow.parentRunId\") for r in child_runs if r.data.tags.get(\"mlflow.parentRunId\")}\n",
    "                    parent_runs = [r for r in finished_runs if r.info.run_id in parent_run_ids or not r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    \n",
    "                    print(f\"    Parent runs: {len(parent_runs)}, Child runs: {len(child_runs)}\")\n",
    "                    \n",
    "                    # Check child runs (what select_champion_per_backbone queries)\n",
    "                    if child_runs:\n",
    "                        sample_child = child_runs[0]\n",
    "                        tags = sample_child.data.tags\n",
    "                        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        \n",
    "                        print(f\"    Sample child run:\")\n",
    "                        print(f\"      - stage: {tags.get(stage_tag, 'missing')}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - trial_key_hash: {'present' if tags.get(trial_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                    \n",
    "                    # Check parent runs (where Phase 2 tags should be)\n",
    "                    if parent_runs:\n",
    "                        sample_parent = parent_runs[0]\n",
    "                        tags = sample_parent.data.tags\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        data_fp_tag = tags_registry.key(\"fingerprint\", \"data\")\n",
    "                        eval_fp_tag = tags_registry.key(\"fingerprint\", \"eval\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        \n",
    "                        print(f\"    Sample parent run:\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                        print(f\"      - data_fp: {'present' if tags.get(data_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - eval_fp: {'present' if tags.get(eval_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "            \n",
    "            print(\"\\nüí° Troubleshooting tips:\")\n",
    "            print(\"  1. **Artifact filter issue**: If you see 'Artifact filter removed X runs',\")\n",
    "            print(\"     the runs don't have 'code.artifact.available' tag set to 'true'.\")\n",
    "            print(\"     Options:\")\n",
    "            print(\"     a) Set require_artifact_available: false in config/best_model_selection.yaml\")\n",
    "            print(\"     b) Set code.artifact.available='true' tag on the runs (if artifacts exist)\")\n",
    "            print(\"  2. Ensure HPO runs have Phase 2 tags set (schema_version, fingerprints, etc.)\")\n",
    "            print(\"  3. Check that runs meet minimum trial requirements (min_trials_per_group in selection config)\")\n",
    "            print(\"  4. Check selection config in config/best_model_selection.yaml\")\n",
    "            print(\"\\n   You can still proceed to Step 7 if benchmark runs already exist from notebook 01.\")\n",
    "        else:\n",
    "            # Step 3.2: Extract fingerprints for benchmark key building (Phase 3)\n",
    "            from infrastructure.tracking.mlflow.hash_utils import derive_eval_config\n",
    "            \n",
    "            data_fp = compute_data_fingerprint(data_config)\n",
    "            # Derive eval_config consistently using centralized utility\n",
    "            train_config = configs.get(\"train\", {})\n",
    "            eval_config = derive_eval_config(train_config, hpo_config)\n",
    "            eval_fp = compute_eval_fingerprint(eval_config)\n",
    "            \n",
    "            # Step 3.3: Filter missing benchmarks (Phase 3 idempotency)\n",
    "            benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "            benchmark_experiment = None\n",
    "            for exp in mlflow_client.search_experiments():\n",
    "                if exp.name == benchmark_experiment_name:\n",
    "                    benchmark_experiment = {\n",
    "                        \"name\": exp.name,\n",
    "                        \"id\": exp.experiment_id\n",
    "                    }\n",
    "                    break\n",
    "            \n",
    "            if not benchmark_experiment:\n",
    "                # Create benchmark experiment if it doesn't exist\n",
    "                benchmark_experiment_id = mlflow_client.create_experiment(benchmark_experiment_name)\n",
    "                benchmark_experiment = {\n",
    "                    \"name\": benchmark_experiment_name,\n",
    "                    \"id\": benchmark_experiment_id\n",
    "                }\n",
    "            \n",
    "            # Get run mode for idempotency check\n",
    "            from evaluation.benchmarking.orchestrator import get_benchmark_run_mode\n",
    "            run_mode = get_benchmark_run_mode(benchmark_config, hpo_config)\n",
    "            \n",
    "            champions_to_benchmark = filter_missing_benchmarks(\n",
    "                champions=champions,\n",
    "                benchmark_experiment=benchmark_experiment,\n",
    "                benchmark_config=benchmark_config,\n",
    "                data_fingerprint=data_fp,\n",
    "                eval_fingerprint=eval_fp,\n",
    "                root_dir=ROOT_DIR,\n",
    "                environment=detect_platform(),\n",
    "                mlflow_client=mlflow_client,\n",
    "                run_mode=run_mode,\n",
    "            )\n",
    "            \n",
    "            skipped_count = len(champions) - len(champions_to_benchmark)\n",
    "            if skipped_count > 0:\n",
    "                print(f\"‚è≠Ô∏è  Skipping {skipped_count} already-benchmarked champion(s)\")\n",
    "            \n",
    "            # Step 3.4: Benchmark only missing champions (Phase 3)\n",
    "            if champions_to_benchmark:\n",
    "                print(f\"\\nüìä Benchmarking {len(champions_to_benchmark)} champion(s)...\")\n",
    "                \n",
    "                # Setup test data path (matching notebook 01's logic)\n",
    "                from pathlib import Path\n",
    "                test_data_path = None\n",
    "                \n",
    "                # First: check benchmark config for explicit test_data path\n",
    "                if benchmark_config.get(\"benchmarking\", {}).get(\"test_data\"):\n",
    "                    test_data_path = Path(benchmark_config[\"benchmarking\"][\"test_data\"])\n",
    "                    if not test_data_path.is_absolute():\n",
    "                        test_data_path = CONFIG_DIR / test_data_path\n",
    "                else:\n",
    "                    # Fallback: use data config's local_path (matching notebook 01)\n",
    "                    if data_config.get(\"local_path\"):\n",
    "                        local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "                        dataset_path = (CONFIG_DIR / local_path_str).resolve()\n",
    "                        \n",
    "                        # Handle seed subdirectory for dataset_tiny (matching notebook 01)\n",
    "                        seed = data_config.get(\"seed\")\n",
    "                        if seed is not None and \"dataset_tiny\" in str(dataset_path):\n",
    "                            dataset_path = dataset_path / f\"seed{seed}\"\n",
    "                        \n",
    "                        # Try test.json in dataset directory\n",
    "                        test_candidates = [\n",
    "                            dataset_path / \"test.json\",\n",
    "                            dataset_path / \"validation.json\",\n",
    "                        ]\n",
    "                        for path in test_candidates:\n",
    "                            if path.exists():\n",
    "                                test_data_path = path\n",
    "                                break\n",
    "                    \n",
    "                    # Final fallback: try common locations relative to config\n",
    "                    if not test_data_path:\n",
    "                        possible_paths = [\n",
    "                            CONFIG_DIR / \"dataset\" / \"test.json\",\n",
    "                            CONFIG_DIR / \"dataset\" / \"validation.json\",\n",
    "                        ]\n",
    "                        for path in possible_paths:\n",
    "                            if path.exists():\n",
    "                                test_data_path = path\n",
    "                                break\n",
    "                \n",
    "                if test_data_path and test_data_path.exists():\n",
    "                    # Setup benchmark tracker\n",
    "                    benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "                    \n",
    "                    # Extract benchmark config parameters\n",
    "                    benchmark_params = benchmark_config.get(\"benchmarking\", {})\n",
    "                    benchmark_batch_sizes = benchmark_params.get(\"batch_sizes\", [1])\n",
    "                    benchmark_iterations = benchmark_params.get(\"iterations\", 10)\n",
    "                    benchmark_warmup = benchmark_params.get(\"warmup_iterations\", 10)\n",
    "                    benchmark_max_length = benchmark_params.get(\"max_length\", 512)\n",
    "                    benchmark_device = benchmark_params.get(\"device\")\n",
    "                    \n",
    "                    # Acquire checkpoints for champions (needed for benchmarking)\n",
    "                    from evaluation.selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "                    acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "                    \n",
    "                    # Acquire checkpoints for champions before benchmarking\n",
    "                    # Phase 3: benchmark_champions() expects checkpoint_path to be set\n",
    "                    # and uses all champion data (run_ids, hashes) directly (no redundant lookups)\n",
    "                    for backbone, champion_data in champions_to_benchmark.items():\n",
    "                        champion = champion_data[\"champion\"]\n",
    "                        run_id = champion.get(\"run_id\")\n",
    "                        refit_run_id = champion.get(\"refit_run_id\")\n",
    "                        trial_run_id = champion.get(\"trial_run_id\")\n",
    "                        \n",
    "                        sweep_run_id = champion.get(\"sweep_run_id\")  # Optional: parent HPO run_id\n",
    "                        if not run_id:\n",
    "                            continue\n",
    "                        \n",
    "                        # Acquire checkpoint using single source of truth\n",
    "                        # Note: All champion data (run_ids, hashes) will be passed to benchmark_champions()\n",
    "                        # which uses them directly without redundant MLflow lookups (Phase 3 optimization)\n",
    "                        best_run_info = {\n",
    "                            \"run_id\": refit_run_id or run_id,\n",
    "                            \"refit_run_id\": refit_run_id,\n",
    "                            \"trial_run_id\": trial_run_id,\n",
    "                            \"sweep_run_id\": sweep_run_id,  # Optional: parent HPO run_id\n",
    "                            \"study_key_hash\": champion.get(\"study_key_hash\"),\n",
    "                            \"trial_key_hash\": champion.get(\"trial_key_hash\"),\n",
    "                            \"backbone\": backbone,\n",
    "                        }\n",
    "                        \n",
    "                        checkpoint_dir = acquire_best_model_checkpoint(\n",
    "                            best_run_info=best_run_info,\n",
    "                            root_dir=ROOT_DIR,\n",
    "                            config_dir=CONFIG_DIR,\n",
    "                            acquisition_config=acquisition_config,\n",
    "                            selection_config=selection_config,\n",
    "                            platform=PLATFORM,\n",
    "                            restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "                            in_colab=IN_COLAB,\n",
    "                        )\n",
    "                        \n",
    "                        # Update champion with checkpoint path\n",
    "                        champion[\"checkpoint_path\"] = Path(checkpoint_dir) if checkpoint_dir else None\n",
    "                    \n",
    "                    # Filter out champions without checkpoints\n",
    "                    champions_to_benchmark = {\n",
    "                        k: v for k, v in champions_to_benchmark.items()\n",
    "                        if v[\"champion\"].get(\"checkpoint_path\")\n",
    "                    }\n",
    "                    \n",
    "                    if champions_to_benchmark:\n",
    "                        benchmark_results = benchmark_champions(\n",
    "                            champions=champions_to_benchmark,\n",
    "                            test_data_path=test_data_path,\n",
    "                            root_dir=ROOT_DIR,\n",
    "                            environment=detect_platform(),\n",
    "                            data_config=data_config,\n",
    "                            hpo_config=hpo_config,\n",
    "                            benchmark_config=benchmark_config,\n",
    "                            benchmark_experiment=benchmark_experiment,\n",
    "                            benchmark_batch_sizes=benchmark_batch_sizes,\n",
    "                            benchmark_iterations=benchmark_iterations,\n",
    "                            benchmark_warmup=benchmark_warmup,\n",
    "                            benchmark_max_length=benchmark_max_length,\n",
    "                            benchmark_device=benchmark_device,\n",
    "                            benchmark_tracker=benchmark_tracker,\n",
    "                            backup_enabled=BACKUP_ENABLED,\n",
    "                            backup_to_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            ensure_restored_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            mlflow_client=mlflow_client,\n",
    "                        )\n",
    "                        \n",
    "                        print(f\"\\n‚úì Benchmarking complete. Results saved to MLflow experiment: {benchmark_experiment_name}\")\n",
    "                    else:\n",
    "                        print(\"‚ö† No champions with checkpoints available for benchmarking.\")\n",
    "                else:\n",
    "                    print(f\"‚ö† Test data not found. Skipping benchmarking.\")\n",
    "                    print(f\"   Tried paths:\")\n",
    "                    if benchmark_config.get(\"benchmarking\", {}).get(\"test_data\"):\n",
    "                        print(f\"     - {Path(benchmark_config['benchmarking']['test_data'])}\")\n",
    "                    if data_config.get(\"local_path\"):\n",
    "                        local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "                        dataset_path = (CONFIG_DIR / local_path_str).resolve()\n",
    "                        seed = data_config.get(\"seed\")\n",
    "                        if seed is not None and \"dataset_tiny\" in str(dataset_path):\n",
    "                            dataset_path = dataset_path / f\"seed{seed}\"\n",
    "                        print(f\"     - {dataset_path / 'test.json'}\")\n",
    "                        print(f\"     - {dataset_path / 'validation.json'}\")\n",
    "                    print(f\"     - {CONFIG_DIR / 'dataset' / 'test.json'}\")\n",
    "                    print(f\"     - {CONFIG_DIR / 'dataset' / 'validation.json'}\")\n",
    "                    print(f\"   üí° Tip: Set 'benchmarking.test_data' in config/benchmark.yaml to specify exact path\")\n",
    "            else:\n",
    "                print(\"‚úì All champions already benchmarked - nothing to do!\")\n",
    "else:\n",
    "    print(\"‚è≠ Skipping benchmarking (RUN_BENCHMARKING=False).\")\n",
    "    print(\"   If benchmark runs don't exist, set RUN_BENCHMARKING=True or run benchmarking in notebook 01.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs (created by `01_orchestrate_training_colab.ipynb` or Step 6 above using `evaluation.benchmarking.benchmark_best_trials`), join to training runs via grouping tags, and select the best model using normalized composite scoring.\n",
    "\n",
    "**Note**: Benchmark runs must exist in MLflow before running this step. If no benchmark runs are found, either:\n",
    "- Set `RUN_BENCHMARKING=True` in Step 6 above, or\n",
    "- Go back to `01_orchestrate_training_colab.ipynb` and run the benchmarking step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Check if we should reuse cached selection\n",
    "run_mode = selection_config.get(\"run\", {}).get(\"mode\", \"reuse_if_exists\")\n",
    "best_model = None\n",
    "cache_data = None\n",
    "\n",
    "print(f\"\\nüìã Best Model Selection Mode: {run_mode}\")\n",
    "\n",
    "if run_mode == \"reuse_if_exists\":\n",
    "    from selection.cache import load_cached_best_model\n",
    "\n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    cache_data = load_cached_best_model(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment_id=benchmark_experiment[\"id\"],\n",
    "        tracking_uri=tracking_uri,\n",
    "    )\n",
    "\n",
    "    if cache_data:\n",
    "        best_model = cache_data[\"best_model\"]\n",
    "        # Success message already printed by load_cached_best_model\n",
    "    else:\n",
    "        print(f\"\\n‚Ñπ Cache not available or invalid - will query MLflow for fresh selection\")\n",
    "elif run_mode == \"force_new\":\n",
    "    print(f\"  Mode is 'force_new' - skipping cache, querying MLflow...\")\n",
    "else:\n",
    "    print(f\"  ‚ö† Unknown run mode '{run_mode}', defaulting to querying MLflow...\")\n",
    "\n",
    "if best_model is None:\n",
    "    # Find best model\n",
    "    best_model = find_best_model_from_mlflow(\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tags_config=tags_config,\n",
    "        selection_config=selection_config\n",
    "    )\n",
    "\n",
    "    if best_model is None:\n",
    "        # Provide diagnostic information\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "\n",
    "        client = MlflowClient()\n",
    "        tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "\n",
    "        # Check benchmark experiment\n",
    "        benchmark_runs = client.search_runs(\n",
    "            experiment_ids=[benchmark_experiment[\"id\"]],\n",
    "            filter_string=\"\",\n",
    "            max_results=100,\n",
    "        )\n",
    "        finished_benchmark_runs = [r for r in benchmark_runs if r.info.status == \"FINISHED\"]\n",
    "\n",
    "        # Check HPO experiments\n",
    "        hpo_run_counts = {}\n",
    "        hpo_trial_runs = []\n",
    "        hpo_refit_runs = []\n",
    "        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "\n",
    "        for backbone, exp_info in hpo_experiments.items():\n",
    "            hpo_runs = client.search_runs(\n",
    "                experiment_ids=[exp_info[\"id\"]],\n",
    "                filter_string=\"\",\n",
    "                max_results=100,\n",
    "            )\n",
    "            finished_hpo_runs = [r for r in hpo_runs if r.info.status == \"FINISHED\"]\n",
    "            hpo_run_counts[backbone] = len(finished_hpo_runs)\n",
    "\n",
    "            # Separate trial and refit runs\n",
    "            for run in finished_hpo_runs:\n",
    "                stage = run.data.tags.get(stage_tag, \"\")\n",
    "                if stage == \"hpo\" or stage == \"hpo_trial\":\n",
    "                    hpo_trial_runs.append(run)\n",
    "                elif stage == \"hpo_refit\":\n",
    "                    hpo_refit_runs.append(run)\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from benchmark runs\n",
    "        benchmark_pairs = set()\n",
    "        for run in finished_benchmark_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                benchmark_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO trial runs\n",
    "        hpo_trial_pairs = set()\n",
    "        for run in hpo_trial_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_trial_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO refit runs\n",
    "        hpo_refit_pairs = set()\n",
    "        for run in hpo_refit_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_refit_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Find matching pairs\n",
    "        matching_pairs = benchmark_pairs & hpo_trial_pairs\n",
    "\n",
    "        error_msg = (\n",
    "            \"Could not find best model from MLflow.\\n\\n\"\n",
    "            \"Diagnostics:\\n\"\n",
    "            f\"  - Benchmark experiment '{benchmark_experiment['name']}': \"\n",
    "            f\"{len(finished_benchmark_runs)} finished run(s) found\\n\"\n",
    "            f\"    - Unique (study_hash, trial_hash) pairs: {len(benchmark_pairs)}\\n\"\n",
    "        )\n",
    "\n",
    "        if hpo_run_counts:\n",
    "            error_msg += \"  - HPO experiments:\\n\"\n",
    "            for backbone, count in hpo_run_counts.items():\n",
    "                error_msg += f\"    - {backbone}: {count} finished run(s) found\\n\"\n",
    "            error_msg += (\n",
    "                f\"    - HPO trial runs: {len(hpo_trial_runs)} with {len(hpo_trial_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "                f\"    - HPO refit runs: {len(hpo_refit_runs)} with {len(hpo_refit_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "            )\n",
    "\n",
    "        error_msg += (\n",
    "            f\"\\n  - Matching pairs: {len(matching_pairs)} out of {len(benchmark_pairs)} benchmark pairs\\n\"\n",
    "        )\n",
    "\n",
    "        if len(matching_pairs) == 0 and len(benchmark_pairs) > 0 and len(hpo_trial_pairs) > 0:\n",
    "            # Show sample hashes for debugging\n",
    "            error_msg += \"\\n  Sample benchmark (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(benchmark_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "\n",
    "            error_msg += \"\\n  Sample HPO trial (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(hpo_trial_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "\n",
    "            error_msg += (\n",
    "                \"\\n  ‚ö†Ô∏è  Hash mismatch detected! This usually means:\\n\"\n",
    "                \"     - Benchmark runs were created from different trials than current HPO runs\\n\"\n",
    "                \"     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\\n\"\n",
    "                \"     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\\n\"\n",
    "            )\n",
    "\n",
    "        error_msg += (\n",
    "            \"\\nPossible causes:\\n\"\n",
    "            \"  1. No benchmark runs have been executed yet. Run benchmark jobs first.\\n\"\n",
    "            \"  2. Benchmark runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  3. HPO runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  4. No matching runs found between benchmark and HPO experiments (hash mismatch).\\n\"\n",
    "            \"\\nCheck the logs above for detailed information about what was found.\"\n",
    "        )\n",
    "\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    # Save to cache\n",
    "    from selection.cache import save_best_model_cache\n",
    "\n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    inputs_summary = {}\n",
    "\n",
    "    timestamped_file, latest_file, index_file = save_best_model_cache(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tracking_uri=tracking_uri,\n",
    "        inputs_summary=inputs_summary,\n",
    "    )\n",
    "    print(f\"‚úì Saved best model selection to cache\")\n",
    "\n",
    "# Extract lineage information from best_model for final training tags\n",
    "from training_exec import extract_lineage_from_best_model\n",
    "lineage = extract_lineage_from_best_model(best_model)\n",
    "\n",
    "# Acquire checkpoint\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    acquisition_config=acquisition_config,\n",
    "    selection_config=selection_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Best model checkpoint available at: {best_checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config.key(\"process\", \"stage\")\n",
    "trained_on_full_data_tag = tags_config.key(\"training\", \"trained_on_full_data\")\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"üîÑ Starting final training with best configuration...\")\n",
    "    from training_exec import execute_final_training\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    final_checkpoint_dir = execute_final_training(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Skipping final training - using selected checkpoint\")\n",
    "\n",
    "# Backup final checkpoint to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and final_checkpoint_dir:\n",
    "    checkpoint_path = Path(final_checkpoint_dir).resolve()\n",
    "    # Check if checkpoint is already in Drive\n",
    "    if str(checkpoint_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n‚úì Final training checkpoint is already in Google Drive\")\n",
    "        print(f\"  Drive path: {checkpoint_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nüì¶ Backing up final training checkpoint to Google Drive...\")\n",
    "            result = drive_store.backup(checkpoint_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"‚úì Successfully backed up final checkpoint to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"‚ö† Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Drive backup error: {e}\")\n",
    "            print(f\"  Checkpoint is still available locally at: {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parent training information for conversion\n",
    "from common.shared.json_cache import load_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "\n",
    "if not final_training_metadata_path.exists():\n",
    "    raise ValueError(\n",
    "        f\"Metadata file not found: {final_training_metadata_path}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "metadata = load_json(final_training_metadata_path)\n",
    "parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    raise ValueError(\n",
    "        f\"Missing required fingerprints in metadata: spec_fp={parent_spec_fp}, exec_fp={parent_exec_fp}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "if parent_training_run_id:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}..., run_id={parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}... (run_id not found)\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\nüîÑ Starting model conversion...\")\n",
    "from conversion import execute_conversion\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "conversion_output_dir = execute_conversion(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    ")\n",
    "\n",
    "# Find ONNX model file (search recursively, as model may be in onnx_model/ subdirectory)\n",
    "onnx_files = list(conversion_output_dir.rglob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\n‚úì Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Warning: No ONNX model file found in {conversion_output_dir} (searched recursively)\")\n",
    "\n",
    "# Backup conversion output to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and conversion_output_dir:\n",
    "    output_path = Path(conversion_output_dir).resolve()\n",
    "    # Check if output is already in Drive\n",
    "    if str(output_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n‚úì Conversion output is already in Google Drive\")\n",
    "        print(f\"  Drive path: {output_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nüì¶ Backing up conversion output to Google Drive...\")\n",
    "            result = drive_store.backup(output_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"‚úì Successfully backed up conversion output to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"‚ö† Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Drive backup error: {e}\")\n",
    "            print(f\"  Output is still available locally at: {conversion_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
