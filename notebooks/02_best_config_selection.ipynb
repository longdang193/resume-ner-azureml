{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk â†’ drive restore â†’ MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"âœ“ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"âœ“ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Local environment detected - assuming repository already exists\n",
      "âœ“ Repository root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "âœ“ Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "âœ“ Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "âœ“ Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"âœ“ Local environment detected - assuming repository already exists\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, try to find repo root\n",
    "    ROOT_DIR = Path.cwd()\n",
    "    # Try to find the repo root by looking for config directory\n",
    "    while ROOT_DIR.parent != ROOT_DIR:\n",
    "        if (ROOT_DIR / \"config\").exists() and (ROOT_DIR / \"src\").exists():\n",
    "            break\n",
    "        ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"âœ“ Repository root: {ROOT_DIR}\")\n",
    "print(f\"âœ“ Config directory: {CONFIG_DIR}\")\n",
    "print(f\"âœ“ Source directory: {SRC_DIR}\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"âœ“ Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded experiment config: resume_ner_baseline\n",
      "âœ“ Loaded tags config\n",
      "âœ“ Loaded best model selection config\n",
      "âœ“ Loaded conversion config\n",
      "âœ“ Loaded artifact acquisition config\n",
      "\n",
      "âœ“ Experiment names defined:\n",
      "  - Benchmark: resume_ner_baseline-benchmark\n",
      "  - Training: resume_ner_baseline-training\n",
      "  - Conversion: resume_ner_baseline-conversion\n",
      "\n",
      "Note: Experiment discovery will happen after MLflow setup (Cell 4)\n"
     ]
    }
   ],
   "source": [
    "from orchestration.config_loader import load_experiment_config\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from shared.yaml_utils import load_yaml\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "print(f\"âœ“ Loaded experiment config: {experiment_config.name}\")\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_yaml(CONFIG_DIR / \"tags.yaml\")\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"âœ“ Loaded tags config\")\n",
    "print(f\"âœ“ Loaded best model selection config\")\n",
    "print(f\"âœ“ Loaded conversion config\")\n",
    "print(f\"âœ“ Loaded artifact acquisition config\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"\\nâœ“ Experiment names defined:\")\n",
    "print(f\"  - Benchmark: {benchmark_experiment_name}\")\n",
    "print(f\"  - Training: {training_experiment_name}\")\n",
    "print(f\"  - Conversion: {conversion_experiment_name}\")\n",
    "print(f\"\\nNote: Experiment discovery will happen after MLflow setup (Cell 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 00:04:47,011 - shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-06 00:04:50,264 - shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-06 00:04:50,265 - shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-06 00:04:50,266 - shared.mlflow_setup - INFO - Loading credentials from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-06 00:04:50,267 - shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-06 00:04:50,269 - shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-06 00:04:50,270 - shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-06 00:04:50,271 - shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-06 00:04:50,272 - shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n",
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-06 00:04:53,425 - shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-06 00:04:54,881 - shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "âœ“ MLflow experiment: resume_ner_baseline-training\n",
      "\n",
      "âœ“ Found 2 HPO experiment(s):\n",
      "  - resume_ner_baseline-hpo-distilbert (backbone: distilbert)\n",
      "  - resume_ner_baseline-hpo-distilroberta (backbone: distilroberta)\n",
      "âœ“ Benchmark experiment: resume_ner_baseline-benchmark\n",
      "âœ“ Training experiment: resume_ner_baseline-training (for final training runs)\n",
      "âœ“ Conversion experiment: resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "from shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"âœ“ MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(hpo_experiments)} HPO experiment(s):\")\n",
    "for backbone, exp_info in hpo_experiments.items():\n",
    "    print(f\"  - {exp_info['name']} (backbone: {backbone})\")\n",
    "\n",
    "if benchmark_experiment:\n",
    "    print(f\"âœ“ Benchmark experiment: {benchmark_experiment['name']}\")\n",
    "else:\n",
    "    print(f\"âš  Benchmark experiment not found: {benchmark_experiment_name}\")\n",
    "\n",
    "print(f\"âœ“ Training experiment: {training_experiment_name} (for final training runs)\")\n",
    "print(f\"âœ“ Conversion experiment: {conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from orchestration.drive_backup import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"âš  Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"âœ“ Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"âœ“ Google Drive mounted\")\n",
    "        print(f\"âœ“ Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"âš  Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"âœ“ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"âœ“ Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs, join to training runs via grouping tags, and select the best model using normalized composite scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 00:04:55,179 - orchestration.jobs.selection.mlflow_selection - INFO - Finding best model from MLflow\n",
      "2026-01-06 00:04:55,181 - orchestration.jobs.selection.mlflow_selection - INFO -   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "2026-01-06 00:04:55,181 - orchestration.jobs.selection.mlflow_selection - INFO -   HPO experiments: 2\n",
      "2026-01-06 00:04:55,182 - orchestration.jobs.selection.mlflow_selection - INFO -   Objective metric: macro-f1\n",
      "2026-01-06 00:04:55,182 - orchestration.jobs.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30\n",
      "2026-01-06 00:04:55,183 - orchestration.jobs.selection.mlflow_selection - INFO - Querying benchmark runs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Finding best model from MLflow...\n",
      "   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "   HPO experiments: 2\n",
      "   Objective metric: macro-f1\n",
      "   Composite weights: F1=0.70, Latency=0.30\n",
      "\n",
      "ðŸ“Š Querying benchmark runs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 00:04:55,331 - orchestration.jobs.selection.mlflow_selection - INFO - Found 2 finished benchmark runs\n",
      "2026-01-06 00:04:55,331 - orchestration.jobs.selection.mlflow_selection - INFO - Found 2 benchmark runs with required metrics and grouping tags\n",
      "2026-01-06 00:04:55,332 - orchestration.jobs.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 2 benchmark runs with required metrics and grouping tags\n",
      "\n",
      "ðŸ”— Preloading trial runs (metrics) and refit runs (artifacts) from HPO experiments...\n",
      "   resume_ner_baseline-hpo-distilbert: 10 finished runs, 4 trial runs, 2 refit runs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 00:04:55,619 - orchestration.jobs.selection.mlflow_selection - INFO - Built trial lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "2026-01-06 00:04:55,619 - orchestration.jobs.selection.mlflow_selection - INFO - Built refit lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "2026-01-06 00:04:55,620 - orchestration.jobs.selection.mlflow_selection - INFO - Joining benchmark runs with trial runs and refit runs...\n",
      "2026-01-06 00:04:55,621 - orchestration.jobs.selection.mlflow_selection - INFO - Found 2 candidate(s) with both benchmark and training metrics\n",
      "2026-01-06 00:04:55,622 - orchestration.jobs.selection.mlflow_selection - INFO - Computing composite scores...\n",
      "2026-01-06 00:04:55,623 - orchestration.jobs.selection.mlflow_selection - INFO - Best model selected:\n",
      "2026-01-06 00:04:55,623 - orchestration.jobs.selection.mlflow_selection - INFO -   Artifact Run ID: 63f2e01b-ccc3-4b91-b708-0c5907ed819b\n",
      "2026-01-06 00:04:55,624 - orchestration.jobs.selection.mlflow_selection - INFO -   Trial Run ID: d23ca1ad-9410-496e-955b-3b6c35463462\n",
      "2026-01-06 00:04:55,624 - orchestration.jobs.selection.mlflow_selection - INFO -   Backbone: distilbert\n",
      "2026-01-06 00:04:55,625 - orchestration.jobs.selection.mlflow_selection - INFO -   F1 Score: 0.4316\n",
      "2026-01-06 00:04:55,625 - orchestration.jobs.selection.mlflow_selection - INFO -   Latency: 6.00 ms\n",
      "2026-01-06 00:04:55,626 - orchestration.jobs.selection.mlflow_selection - INFO -   Composite Score: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_ner_baseline-hpo-distilroberta: 0 finished runs, 0 trial runs, 0 refit runs\n",
      "   Built trial lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "   Built refit lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "\n",
      "ðŸ”— Joining benchmark runs with trial runs (metrics) and refit runs (artifacts)...\n",
      "   Found 2 candidate(s) with both benchmark and training metrics\n",
      "\n",
      "âœ… Best model selected:\n",
      "   Run ID: 63f2e01b-ccc3-4b91-b708-0c5907ed819b\n",
      "   Experiment: resume_ner_baseline-hpo-distilbert\n",
      "   Backbone: distilbert\n",
      "   F1 Score: 0.4316\n",
      "   Latency: 6.00 ms\n",
      "   Composite Score: 0.7000\n",
      "âœ“ Extracted lineage information: ['hpo_study_key_hash', 'hpo_trial_key_hash', 'hpo_trial_run_id', 'hpo_refit_run_id']\n",
      "\n",
      "âœ“ Best model checkpoint available at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\best_model_selection\\local\\distilbert\\sel_350a79aa_f349f4af\n"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs.selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from orchestration.jobs.selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Find best model\n",
    "best_model = find_best_model_from_mlflow(\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    "    hpo_experiments=hpo_experiments,\n",
    "    tags_config=tags_config,\n",
    "    selection_config=selection_config,\n",
    "    use_python_filtering=True,\n",
    ")\n",
    "\n",
    "if best_model is None:\n",
    "    raise ValueError(\"Could not find best model from MLflow.\")\n",
    "\n",
    "# Extract lineage information from best_model for final training tags\n",
    "from orchestration.jobs.final_training import extract_lineage_from_best_model\n",
    "lineage = extract_lineage_from_best_model(best_model)\n",
    "print(f\"âœ“ Extracted lineage information: {list(lineage.keys())}\")\n",
    "\n",
    "# Acquire checkpoint\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    acquisition_config=acquisition_config,\n",
    "    selection_config=selection_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Best model checkpoint available at: {best_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config[\"process\"][\"stage\"]\n",
    "trained_on_full_data_tag = tags_config[\"training\"][\"trained_on_full_data\"]\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting final training with best configuration...\n",
      "âœ“ Final training config loaded from final_training.yaml\n",
      "âœ“ Output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_e2ae9d18a17895d2\\v1\n",
      "âœ“ Found existing completed final training run\n",
      "  Output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_e2ae9d18a17895d2\\v1\n",
      "  Checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_e2ae9d18a17895d2\\v1\\checkpoint\n",
      "  Reusing existing checkpoint (run.mode: reuse_if_exists)\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"ðŸ”„ Starting final training with best configuration...\")\n",
    "    from orchestration.jobs.final_training import execute_final_training\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    final_checkpoint_dir = execute_final_training(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ“ Skipping final training - using selected checkpoint\")\n",
    "\n",
    "# Backup final checkpoint to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and final_checkpoint_dir:\n",
    "    try:\n",
    "        # Ensure checkpoint path is absolute and resolved\n",
    "        from pathlib import Path\n",
    "        checkpoint_path = Path(final_checkpoint_dir).resolve()\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Backing up final training checkpoint to Google Drive...\")\n",
    "        print(f\"  Local path: {checkpoint_path}\")\n",
    "        print(f\"  Root dir: {drive_store.root_dir}\")\n",
    "        print(f\"  Backup root: {drive_store.backup_root}\")\n",
    "        \n",
    "        result = drive_store.backup(checkpoint_path, expect=\"dir\")\n",
    "        if result.ok:\n",
    "            print(f\"âœ“ Successfully backed up final checkpoint to Google Drive\")\n",
    "            print(f\"  Drive path: {result.dst}\")\n",
    "        else:\n",
    "            print(f\"âš  Drive backup failed, but checkpoint is available locally\")\n",
    "            print(f\"  Reason: {result.reason}\")\n",
    "            if result.error:\n",
    "                print(f\"  Error: {result.error}\")\n",
    "            print(f\"  Attempted drive path: {result.dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Drive backup error: {e}\")\n",
    "        print(f\"  Checkpoint is still available locally at: {final_checkpoint_dir}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metadata' from 'orchestration.metadata_manager' (c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\\orchestration\\metadata_manager.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract parent training information for conversion\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01morchestration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_metadata\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load metadata from final training output directory\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_metadata' from 'orchestration.metadata_manager' (c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\\orchestration\\metadata_manager.py)"
     ]
    }
   ],
   "source": [
    "# Extract parent training information for conversion\n",
    "from orchestration.metadata_manager import load_metadata\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "parent_spec_fp = None\n",
    "parent_exec_fp = None\n",
    "parent_training_run_id = None\n",
    "\n",
    "if final_training_metadata_path.exists():\n",
    "    try:\n",
    "        metadata = load_metadata(final_training_metadata_path)\n",
    "        parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "        parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "        parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Warning: Could not load metadata: {e}\")\n",
    "\n",
    "# Fallback: try to extract from path if metadata missing\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    path_parts = final_checkpoint_dir.parts\n",
    "    try:\n",
    "        spec_idx = next(i for i, p in enumerate(path_parts) if p.startswith(\"spec_\"))\n",
    "        spec_exec_part = path_parts[spec_idx]\n",
    "        # Extract spec_fp and exec_fp from \"spec_{spec_fp}_exec_{exec_fp}\"\n",
    "        parts = spec_exec_part.split(\"_\")\n",
    "        if len(parts) >= 4:\n",
    "            parent_spec_fp = parts[1]  # spec_fp\n",
    "            parent_exec_fp = parts[3]  # exec_fp\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract fingerprints from path\")\n",
    "    except (StopIteration, ValueError, IndexError) as e:\n",
    "        raise ValueError(\n",
    "            f\"Could not extract parent training fingerprints from path: {final_checkpoint_dir}\\n\"\n",
    "            \"Please ensure final training completed successfully and metadata.json exists.\"\n",
    "        ) from e\n",
    "\n",
    "print(f\"âœ“ Parent training fingerprints:\")\n",
    "print(f\"  - spec_fp: {parent_spec_fp}\")\n",
    "print(f\"  - exec_fp: {parent_exec_fp}\")\n",
    "if parent_training_run_id:\n",
    "    print(f\"  - MLflow run ID: {parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"  âš  Warning: Parent training run ID not found - lineage will be partial\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\nðŸ”„ Starting model conversion...\")\n",
    "from orchestration.jobs.conversion import execute_conversion\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "conversion_output_dir = execute_conversion(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    ")\n",
    "\n",
    "# Find ONNX model file\n",
    "onnx_files = list(conversion_output_dir.glob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\nâœ“ Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\nâš  Warning: No ONNX model file found in {conversion_output_dir}\")\n",
    "\n",
    "# Backup conversion output to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and conversion_output_dir:\n",
    "    try:\n",
    "        # Ensure output path is absolute and resolved\n",
    "        output_path = Path(conversion_output_dir).resolve()\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Backing up conversion output to Google Drive...\")\n",
    "        print(f\"  Local path: {output_path}\")\n",
    "        \n",
    "        result = drive_store.backup(output_path, expect=\"dir\")\n",
    "        if result.ok:\n",
    "            print(f\"âœ“ Successfully backed up conversion output to Google Drive\")\n",
    "            print(f\"  Drive path: {result.dst}\")\n",
    "        else:\n",
    "            print(f\"âš  Drive backup failed, but output is available locally\")\n",
    "            print(f\"  Reason: {result.reason}\")\n",
    "            if result.error:\n",
    "                print(f\"  Error: {result.error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Drive backup error: {e}\")\n",
    "        print(f\"  Output is still available locally at: {conversion_output_dir}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
