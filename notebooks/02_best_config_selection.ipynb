{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "**Prerequisites**: Run `01_orchestrate_training_colab.ipynb` first to:\n",
    "- Train models via HPO\n",
    "- Run benchmarking on best trials (using `evaluation.benchmarking.benchmark_best_trials`)\n",
    "\n",
    "Then this notebook:\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk → drive restore → MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"✓ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"✓ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected - detecting repository root...\n",
      "✓ Repository: /workspaces/resume-ner-azureml (config=config, src=src)\n",
      "✓ Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"✓ Local environment detected - detecting repository root...\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, detect repo root by searching for config/ and src/ directories\n",
    "    # Start from current working directory and search up\n",
    "    current_dir = Path.cwd()\n",
    "    ROOT_DIR = None\n",
    "    \n",
    "    # Check current directory first\n",
    "    if (current_dir / \"config\").exists() and (current_dir / \"src\").exists():\n",
    "        ROOT_DIR = current_dir\n",
    "    else:\n",
    "        # Search up the directory tree\n",
    "        for parent in current_dir.parents:\n",
    "            if (parent / \"config\").exists() and (parent / \"src\").exists():\n",
    "                ROOT_DIR = parent\n",
    "                break\n",
    "    \n",
    "    if ROOT_DIR is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not find repository root. Searched from: {current_dir}\\n\"\n",
    "            \"Please ensure you're running from within the repository or a subdirectory.\"\n",
    "        )\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"✓ Repository: {ROOT_DIR} (config={CONFIG_DIR.name}, src={SRC_DIR.name})\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"✓ Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded configs: experiment=resume_ner_baseline, tags, selection, conversion, acquisition\n",
      "✓ Experiment names: benchmark=resume_ner_baseline-benchmark, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "from infrastructure.config.loader import load_experiment_config\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "    # Note: Still in orchestration.jobs.tracking for now\n",
    "from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_tags_registry(CONFIG_DIR)\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"✓ Loaded configs: experiment={experiment_config.name}, tags, selection, conversion, acquisition\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"✓ Experiment names: benchmark={benchmark_experiment_name}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:54:48,248 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-13 16:54:48,251 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: True, client_secret: True, tenant_id: True\n",
      "2026-01-13 16:54:48,252 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-13 16:54:48,252 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-13 16:54:48,253 - common.shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ azureml.mlflow is available - Azure ML tracking will be used if configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:54:48,383 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tracking_uri \u001b[38;5;241m=\u001b[39m \u001b[43msetup_mlflow_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_experiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_to_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ MLflow tracking URI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracking_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ MLflow experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_experiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/resume-ner-azureml/src/common/shared/mlflow_setup.py:735\u001b[0m, in \u001b[0;36msetup_mlflow_from_config\u001b[0;34m(experiment_name, config_dir, fallback_to_local)\u001b[0m\n\u001b[1;32m    732\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzure ML disabled in config, using local tracking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# Setup MLflow with or without Azure ML\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msetup_mlflow_cross_platform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mml_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_to_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback_to_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/resume-ner-azureml/src/common/shared/mlflow_setup.py:280\u001b[0m, in \u001b[0;36msetup_mlflow_cross_platform\u001b[0;34m(experiment_name, ml_client, fallback_to_local)\u001b[0m\n\u001b[1;32m    278\u001b[0m tracking_uri \u001b[38;5;241m=\u001b[39m _get_azure_ml_tracking_uri(ml_client)\n\u001b[1;32m    279\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(tracking_uri)\n\u001b[0;32m--> 280\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Set Azure ML artifact upload timeout (default 300s, increase to 600s for large artifacts)\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:189\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[1;32m    191\u001b[0m             _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist. Creating a new experiment.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    193\u001b[0m                 experiment_name,\n\u001b[1;32m    194\u001b[0m             )\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:280\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:903\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 903\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:222\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[0m\n\u001b[1;32m    220\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m method_to_info[api]\n\u001b[1;32m    221\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m response_proto \u001b[38;5;129;01mor\u001b[39;00m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:591\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    590\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 591\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:236\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, retry_timeout_seconds, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fetch_auth(host_creds\u001b[38;5;241m.\u001b[39mauth)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost_creds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with timeout exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m To increase the timeout, set the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m to a larger value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mto\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m env_value \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_ALLOW_HTTP_REDIRECTS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/opentelemetry/instrumentation/requests/__init__.py:303\u001b[0m, in \u001b[0;36m_instrument.<locals>.instrumented_send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m start_time \u001b[38;5;241m=\u001b[39m default_timer()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_send\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# *** PROCEED\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# pylint: disable=W0703\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     exception \u001b[38;5;241m=\u001b[39m exc\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/opentelemetry/instrumentation/urllib3/__init__.py:325\u001b[0m, in \u001b[0;36m_instrument.<locals>.instrumented_urlopen\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minstrumented_urlopen\u001b[39m(wrapped, instance, args, kwargs):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_http_instrumentation_enabled():\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     url \u001b[38;5;241m=\u001b[39m _get_url(instance, args, kwargs, url_filter)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m excluded_urls \u001b[38;5;129;01mand\u001b[39;00m excluded_urls\u001b[38;5;241m.\u001b[39murl_disabled(url):\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/urllib3/connection.py:571\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/envs/resume-ner-training/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if azureml.mlflow is available\n",
    "try:\n",
    "    import azureml.mlflow  # noqa: F401\n",
    "    print(\"✓ azureml.mlflow is available - Azure ML tracking will be used if configured\")\n",
    "except ImportError:\n",
    "    print(\"⚠ azureml.mlflow is not available - will fallback to local SQLite tracking\")\n",
    "    print(\"  To use Azure ML tracking, install: pip install azureml-mlflow\")\n",
    "    print(\"  Then restart the kernel and re-run this cell\")\n",
    "\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"✓ MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"✓ MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "hpo_backbones = \", \".join(hpo_experiments.keys())\n",
    "print(f\"✓ Experiments: {len(hpo_experiments)} HPO ({hpo_backbones}), benchmark={'found' if benchmark_experiment else 'not found'}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"⚠ Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"✓ Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"✓ Google Drive mounted\")\n",
    "        print(f\"✓ Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"⚠ Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"✓ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"✓ Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optional - Run Benchmarking on Champions\n",
    "\n",
    "**Optional Step**: If you haven't run benchmarking in `01_orchestrate_training_colab.ipynb`, you can run it here before selecting the best model. This step will:\n",
    "1. Select champions (best trials) from HPO runs using Phase 2 selection logic\n",
    "2. Run benchmarking on each champion to measure inference performance\n",
    "3. Save benchmark results to MLflow for use in Step 7\n",
    "\n",
    "**Note**: If benchmark runs already exist in MLflow, you can skip this step and proceed directly to Step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running benchmarking on champions...\n",
      "✓ Found 2 HPO experiment(s)\n",
      "🏆 Selecting champions per backbone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:47:58,559 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'\n",
      "2026-01-13 16:47:58,766 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)\n",
      "2026-01-13 16:47:58,766 - evaluation.selection.trial_finder - INFO - Filtered out 1 parent run(s) (only child/trial runs have metrics). 2 child runs remaining.\n",
      "2026-01-13 16:47:58,767 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 16:47:58,770 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)\n",
      "2026-01-13 16:47:59,322 - evaluation.selection.trial_finder - INFO - Found refit run d2367a4f-eba... for champion trial dd68676b-776... (selected latest from 1 refit run(s))\n",
      "2026-01-13 16:47:59,515 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilroberta, trying legacy stage='hpo'\n",
      "2026-01-13 16:47:59,619 - evaluation.selection.trial_finder - INFO - Found 0 runs with stage tag for distilroberta (backbone=distilroberta)\n",
      "2026-01-13 16:47:59,620 - evaluation.selection.trial_finder - INFO - Grouped runs for distilroberta: 0 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 16:47:59,621 - evaluation.selection.trial_finder - WARNING - No valid groups found for distilroberta. No trial runs found in HPO experiment 'resume_ner_baseline-hpo-distilroberta'. This may indicate:\n",
      "  - HPO was not run for this backbone\n",
      "  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)\n",
      "  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)\n",
      "Skipping champion selection for distilroberta.\n",
      "2026-01-13 16:47:59,726 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions\n",
      "2026-01-13 16:47:59,899 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Strategy 1: Checking local disk for checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert (study=28b7f255..., trial=973c6e7b...)\n",
      "2026-01-13 16:47:59,900 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] No local checkpoint found at: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert\n",
      "2026-01-13 16:47:59,901 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Strategy 3: Downloading checkpoint from MLflow (run d2367a4f-eba...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Benchmarking 1 champion(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:44<00:00, 44.73s/it]\n",
      "2026-01-13 16:48:45,653 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Successfully downloaded checkpoint from run d2367a4f-eba...\n",
      "2026-01-13 16:48:48,886 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (973c6e7b3da2a400)...\n",
      "2026-01-13 16:48:48,887 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=dd68676b-776..., refit=d2367a4f-eba..., sweep=None...\n",
      "2026-01-13 16:48:48,888 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_28b7f255_973c6e7b/best_trial_checkpoint.tar.gz/best_trial_checkpoint --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-28b7f255/trial-973c6e7b/bench-8f19ff84/benchmark.json\n",
      "2026-01-13 16:48:53,233 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_28b7f255_973c6e7b/best_trial_checkpoint.tar.gz/best_trial_checkpoint\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_28b7f255_973c6e7b/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_28b7f255_973c6e7b/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 169.87 ms\n",
      "  P95 latency: 177.41 ms\n",
      "  Throughput: 5.89 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-28b7f255/trial-973c6e7b/bench-8f19ff84/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-28b7f255/trial-973c6e7b/bench-8f19ff84/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:49:11,505 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=973c6e7b3da2a400, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-13 16:49:11,531 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 16:49:11,531 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 16:49:11,532 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:47c10fbe6cb075fe409f85d70cdde9c16820..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 16:49:11,533 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 16:49:11,534 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-13 16:49:11,534 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-13 16:49:11,536 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:benchmarking:47c10fbe6cb075fe409f85d70c... (run_id: pending_2026...)\n",
      "2026-01-13 16:49:11,536 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-28b7f255_trial-973c6e7b_bench-8f19ff84_1\n",
      "2026-01-13 16:49:11,724 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilbert', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='973c6e7b3da2a400', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='28b7f2559ddbbb2a0df8408f418667a451b61b3873f58ca9ef02d7fa01ba87f4', trial_key_hash='973c6e7b3da2a4007879a87b7384feaf203b2077beafb90daefdfa8520588096', benchmark_config_hash='8f19ff84f89bdc5875d04250e5a4c87a0b954fbb2909e3e34b1bcd78018b8cc1'), study_key_hash=28b7f2559ddbbb2a..., trial_key_hash=973c6e7b3da2a400..., context.model=distilbert, context.process_type=benchmarking\n",
      "2026-01-13 16:49:11,726 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilbert, code.stage=benchmarking\n",
      "2026-01-13 16:49:11,727 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=dd68676b-776..., parent_kind=trial\n",
      "2026-01-13 16:49:11,850 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 1 in run name 'local_distilbert_benchmark_study-28b7f255_trial-973c6e7b_bench-8f19ff84_1'\n",
      "2026-01-13 16:49:11,850 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 16:49:11,851 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 16:49:11,852 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 1 for run 6e95634e-b83..., counter_key=resume-ner:benchmarking:47c10fbe6cb075fe409f85d70c...\n",
      "2026-01-13 16:49:11,852 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:47c10fbe6cb075fe409f85d70cdde9c16820..., version=1, run_id=6e95634e-b83..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 16:49:11,853 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 1 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 16:49:11,854 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Found and committed reservation: version=1, status changed from 'reserved' to 'committed', run_id=6e95634e-b83..., counter_key=resume-ner:benchmarking:47c10fbe6cb075fe409f85d70c...\n",
      "2026-01-13 16:49:11,855 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Successfully saved committed version 1 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 16:49:11,856 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] ✓ Successfully committed version 1 for benchmark run 6e95634e-b83...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilbert_benchmark_study-28b7f255_trial-973c6e7b_bench-8f19ff84_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/6e95634e-b833-45ff-b40a-9560bd5ca7d0\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:49:15,589 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-28b7f255/trial-973c6e7b/bench-8f19ff84/benchmark.json\n",
      "2026-01-13 16:49:15,589 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Benchmarking complete. Results saved to MLflow experiment: resume_ner_baseline-benchmark\n"
     ]
    }
   ],
   "source": [
    "# Optional: Run benchmarking on champions if not already done\n",
    "# Skip this cell if benchmark runs already exist in MLflow\n",
    "\n",
    "RUN_BENCHMARKING = True  # Set to True to run benchmarking\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    from evaluation.selection.trial_finder import select_champions_for_backbones\n",
    "    from evaluation.benchmarking.orchestrator import (\n",
    "        benchmark_champions,\n",
    "        filter_missing_benchmarks,\n",
    "    )\n",
    "    from infrastructure.naming.mlflow.hpo_keys import (\n",
    "        compute_data_fingerprint,\n",
    "        compute_eval_fingerprint,\n",
    "    )\n",
    "    from common.shared.platform_detection import detect_platform\n",
    "    from common.shared.yaml_utils import load_yaml\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    from infrastructure.naming.experiments import build_mlflow_experiment_name\n",
    "    from orchestration import STAGE_HPO\n",
    "    from orchestration.jobs.tracking.mlflow_tracker import MLflowBenchmarkTracker\n",
    "    \n",
    "    print(\"🔄 Running benchmarking on champions...\")\n",
    "    \n",
    "    # Step 1: Load configs and setup MLflow client\n",
    "    from infrastructure.config.loader import load_experiment_config, load_all_configs\n",
    "    \n",
    "    selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "    benchmark_config = load_yaml(CONFIG_DIR / \"benchmark.yaml\")\n",
    "    \n",
    "    # Load all configs using the standard loader (consistent with other notebooks)\n",
    "    experiment_config = load_experiment_config(CONFIG_DIR, experiment_name)\n",
    "    configs = load_all_configs(experiment_config)\n",
    "    data_config = configs.get(\"data\", {})\n",
    "    hpo_config = configs.get(\"hpo\", {})\n",
    "    mlflow_client = MlflowClient()\n",
    "    \n",
    "    # Step 2: Build HPO experiments dict (backbone -> {name, id})\n",
    "    hpo_experiments = {}\n",
    "    for exp in mlflow_client.search_experiments():\n",
    "        if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "            backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "            hpo_experiments[backbone] = {\n",
    "                \"name\": exp.name,\n",
    "                \"id\": exp.experiment_id\n",
    "            }\n",
    "    \n",
    "    if not hpo_experiments:\n",
    "        print(\"⚠ No HPO experiments found. Skipping benchmarking.\")\n",
    "    else:\n",
    "        # Step 3: Select champions per backbone (Phase 2)\n",
    "        backbone_values = list(hpo_experiments.keys())\n",
    "        print(f\"✓ Found {len(hpo_experiments)} HPO experiment(s)\")\n",
    "        print(\"🏆 Selecting champions per backbone...\")\n",
    "        \n",
    "        champions = select_champions_for_backbones(\n",
    "            backbone_values=backbone_values,\n",
    "            hpo_experiments=hpo_experiments,\n",
    "            selection_config=selection_config,\n",
    "            mlflow_client=mlflow_client,\n",
    "        )\n",
    "        \n",
    "        if not champions:\n",
    "            print(\"⚠ No champions found. Skipping benchmarking.\")\n",
    "            \n",
    "            # Add diagnostics to help debug\n",
    "            print(\"\\n🔍 Diagnostics:\")\n",
    "            from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "            tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "            \n",
    "            for backbone, exp_info in hpo_experiments.items():\n",
    "                backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "                runs = mlflow_client.search_runs(\n",
    "                    experiment_ids=[exp_info[\"id\"]],\n",
    "                    filter_string=\"\",\n",
    "                    max_results=100,\n",
    "                )\n",
    "                finished_runs = [r for r in runs if r.info.status == \"FINISHED\"]\n",
    "                print(f\"\\n  {backbone}: {len(finished_runs)} finished run(s)\")\n",
    "                \n",
    "                # Check for required tags for champion selection\n",
    "                if finished_runs:\n",
    "                    # Separate parent and child runs\n",
    "                    # Child runs: have mlflow.parentRunId tag\n",
    "                    # Parent runs: don't have mlflow.parentRunId tag\n",
    "                    child_runs = [r for r in finished_runs if r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    parent_run_ids = {r.data.tags.get(\"mlflow.parentRunId\") for r in child_runs if r.data.tags.get(\"mlflow.parentRunId\")}\n",
    "                    parent_runs = [r for r in finished_runs if r.info.run_id in parent_run_ids or not r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    \n",
    "                    print(f\"    Parent runs: {len(parent_runs)}, Child runs: {len(child_runs)}\")\n",
    "                    \n",
    "                    # Check child runs (what select_champion_per_backbone queries)\n",
    "                    if child_runs:\n",
    "                        sample_child = child_runs[0]\n",
    "                        tags = sample_child.data.tags\n",
    "                        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        \n",
    "                        print(f\"    Sample child run:\")\n",
    "                        print(f\"      - stage: {tags.get(stage_tag, 'missing')}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - trial_key_hash: {'present' if tags.get(trial_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                    \n",
    "                    # Check parent runs (where Phase 2 tags should be)\n",
    "                    if parent_runs:\n",
    "                        sample_parent = parent_runs[0]\n",
    "                        tags = sample_parent.data.tags\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        data_fp_tag = tags_registry.key(\"fingerprint\", \"data\")\n",
    "                        eval_fp_tag = tags_registry.key(\"fingerprint\", \"eval\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        \n",
    "                        print(f\"    Sample parent run:\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                        print(f\"      - data_fp: {'present' if tags.get(data_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - eval_fp: {'present' if tags.get(eval_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "            \n",
    "            print(\"\\n💡 Troubleshooting tips:\")\n",
    "            print(\"  1. **Artifact filter issue**: If you see 'Artifact filter removed X runs',\")\n",
    "            print(\"     the runs don't have 'code.artifact.available' tag set to 'true'.\")\n",
    "            print(\"     Options:\")\n",
    "            print(\"     a) Set require_artifact_available: false in config/best_model_selection.yaml\")\n",
    "            print(\"     b) Set code.artifact.available='true' tag on the runs (if artifacts exist)\")\n",
    "            print(\"  2. Ensure HPO runs have Phase 2 tags set (schema_version, fingerprints, etc.)\")\n",
    "            print(\"  3. Check that runs meet minimum trial requirements (min_trials_per_group in selection config)\")\n",
    "            print(\"  4. Check selection config in config/best_model_selection.yaml\")\n",
    "            print(\"\\n   You can still proceed to Step 7 if benchmark runs already exist from notebook 01.\")\n",
    "        else:\n",
    "            # Step 3.2: Extract fingerprints for benchmark key building (Phase 3)\n",
    "            from infrastructure.tracking.mlflow.hash_utils import derive_eval_config\n",
    "            \n",
    "            data_fp = compute_data_fingerprint(data_config)\n",
    "            # Derive eval_config consistently using centralized utility\n",
    "            train_config = configs.get(\"train\", {})\n",
    "            eval_config = derive_eval_config(train_config, hpo_config)\n",
    "            eval_fp = compute_eval_fingerprint(eval_config)\n",
    "            \n",
    "            # Step 3.3: Filter missing benchmarks (Phase 3 idempotency)\n",
    "            benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "            benchmark_experiment = None\n",
    "            for exp in mlflow_client.search_experiments():\n",
    "                if exp.name == benchmark_experiment_name:\n",
    "                    benchmark_experiment = {\n",
    "                        \"name\": exp.name,\n",
    "                        \"id\": exp.experiment_id\n",
    "                    }\n",
    "                    break\n",
    "            \n",
    "            if not benchmark_experiment:\n",
    "                # Create benchmark experiment if it doesn't exist\n",
    "                benchmark_experiment_id = mlflow_client.create_experiment(benchmark_experiment_name)\n",
    "                benchmark_experiment = {\n",
    "                    \"name\": benchmark_experiment_name,\n",
    "                    \"id\": benchmark_experiment_id\n",
    "                }\n",
    "            \n",
    "            # Get run mode for idempotency check\n",
    "            from evaluation.benchmarking.orchestrator import get_benchmark_run_mode\n",
    "            run_mode = get_benchmark_run_mode(benchmark_config, hpo_config)\n",
    "            \n",
    "            champions_to_benchmark = filter_missing_benchmarks(\n",
    "                champions=champions,\n",
    "                benchmark_experiment=benchmark_experiment,\n",
    "                benchmark_config=benchmark_config,\n",
    "                data_fingerprint=data_fp,\n",
    "                eval_fingerprint=eval_fp,\n",
    "                root_dir=ROOT_DIR,\n",
    "                environment=detect_platform(),\n",
    "                mlflow_client=mlflow_client,\n",
    "                run_mode=run_mode,\n",
    "            )\n",
    "            \n",
    "            skipped_count = len(champions) - len(champions_to_benchmark)\n",
    "            if skipped_count > 0:\n",
    "                print(f\"⏭️  Skipping {skipped_count} already-benchmarked champion(s)\")\n",
    "            \n",
    "            # Step 3.4: Benchmark only missing champions (Phase 3)\n",
    "            if champions_to_benchmark:\n",
    "                print(f\"\\n📊 Benchmarking {len(champions_to_benchmark)} champion(s)...\")\n",
    "                \n",
    "                # Setup test data path (matching notebook 01's logic)\n",
    "                from pathlib import Path\n",
    "                test_data_path = None\n",
    "                \n",
    "                # First: check benchmark config for explicit test_data path\n",
    "                if benchmark_config.get(\"benchmarking\", {}).get(\"test_data\"):\n",
    "                    test_data_path = Path(benchmark_config[\"benchmarking\"][\"test_data\"])\n",
    "                    if not test_data_path.is_absolute():\n",
    "                        test_data_path = CONFIG_DIR / test_data_path\n",
    "                else:\n",
    "                    # Fallback: use data config's local_path (matching notebook 01)\n",
    "                    if data_config.get(\"local_path\"):\n",
    "                        local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "                        dataset_path = (CONFIG_DIR / local_path_str).resolve()\n",
    "                        \n",
    "                        # Handle seed subdirectory for dataset_tiny (matching notebook 01)\n",
    "                        seed = data_config.get(\"seed\")\n",
    "                        if seed is not None and \"dataset_tiny\" in str(dataset_path):\n",
    "                            dataset_path = dataset_path / f\"seed{seed}\"\n",
    "                        \n",
    "                        # Try test.json in dataset directory\n",
    "                        test_candidates = [\n",
    "                            dataset_path / \"test.json\",\n",
    "                            dataset_path / \"validation.json\",\n",
    "                        ]\n",
    "                        for path in test_candidates:\n",
    "                            if path.exists():\n",
    "                                test_data_path = path\n",
    "                                break\n",
    "                    \n",
    "                    # Final fallback: try common locations relative to config\n",
    "                    if not test_data_path:\n",
    "                        possible_paths = [\n",
    "                            CONFIG_DIR / \"dataset\" / \"test.json\",\n",
    "                            CONFIG_DIR / \"dataset\" / \"validation.json\",\n",
    "                        ]\n",
    "                        for path in possible_paths:\n",
    "                            if path.exists():\n",
    "                                test_data_path = path\n",
    "                                break\n",
    "                \n",
    "                if test_data_path and test_data_path.exists():\n",
    "                    # Setup benchmark tracker\n",
    "                    benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "                    \n",
    "                    # Extract benchmark config parameters\n",
    "                    benchmark_params = benchmark_config.get(\"benchmarking\", {})\n",
    "                    benchmark_batch_sizes = benchmark_params.get(\"batch_sizes\", [1])\n",
    "                    benchmark_iterations = benchmark_params.get(\"iterations\", 10)\n",
    "                    benchmark_warmup = benchmark_params.get(\"warmup_iterations\", 10)\n",
    "                    benchmark_max_length = benchmark_params.get(\"max_length\", 512)\n",
    "                    benchmark_device = benchmark_params.get(\"device\")\n",
    "                    \n",
    "                    # Acquire checkpoints for champions (needed for benchmarking)\n",
    "                    from evaluation.selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "                    acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "                    \n",
    "                    # Acquire checkpoints for champions before benchmarking\n",
    "                    # Phase 3: benchmark_champions() expects checkpoint_path to be set\n",
    "                    # and uses all champion data (run_ids, hashes) directly (no redundant lookups)\n",
    "                    for backbone, champion_data in champions_to_benchmark.items():\n",
    "                        champion = champion_data[\"champion\"]\n",
    "                        run_id = champion.get(\"run_id\")\n",
    "                        refit_run_id = champion.get(\"refit_run_id\")\n",
    "                        trial_run_id = champion.get(\"trial_run_id\")\n",
    "                        \n",
    "                        sweep_run_id = champion.get(\"sweep_run_id\")  # Optional: parent HPO run_id\n",
    "                        if not run_id:\n",
    "                            continue\n",
    "                        \n",
    "                        # Acquire checkpoint using single source of truth\n",
    "                        # Note: All champion data (run_ids, hashes) will be passed to benchmark_champions()\n",
    "                        # which uses them directly without redundant MLflow lookups (Phase 3 optimization)\n",
    "                        best_run_info = {\n",
    "                            \"run_id\": refit_run_id or run_id,\n",
    "                            \"refit_run_id\": refit_run_id,\n",
    "                            \"trial_run_id\": trial_run_id,\n",
    "                            \"sweep_run_id\": sweep_run_id,  # Optional: parent HPO run_id\n",
    "                            \"study_key_hash\": champion.get(\"study_key_hash\"),\n",
    "                            \"trial_key_hash\": champion.get(\"trial_key_hash\"),\n",
    "                            \"backbone\": backbone,\n",
    "                        }\n",
    "                        \n",
    "                        checkpoint_dir = acquire_best_model_checkpoint(\n",
    "                            best_run_info=best_run_info,\n",
    "                            root_dir=ROOT_DIR,\n",
    "                            config_dir=CONFIG_DIR,\n",
    "                            acquisition_config=acquisition_config,\n",
    "                            selection_config=selection_config,\n",
    "                            platform=PLATFORM,\n",
    "                            restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "                            in_colab=IN_COLAB,\n",
    "                        )\n",
    "                        \n",
    "                        # Update champion with checkpoint path\n",
    "                        champion[\"checkpoint_path\"] = Path(checkpoint_dir) if checkpoint_dir else None\n",
    "                    \n",
    "                    # Filter out champions without checkpoints\n",
    "                    champions_to_benchmark = {\n",
    "                        k: v for k, v in champions_to_benchmark.items()\n",
    "                        if v[\"champion\"].get(\"checkpoint_path\")\n",
    "                    }\n",
    "                    \n",
    "                    if champions_to_benchmark:\n",
    "                        benchmark_results = benchmark_champions(\n",
    "                            champions=champions_to_benchmark,\n",
    "                            test_data_path=test_data_path,\n",
    "                            root_dir=ROOT_DIR,\n",
    "                            environment=detect_platform(),\n",
    "                            data_config=data_config,\n",
    "                            hpo_config=hpo_config,\n",
    "                            benchmark_config=benchmark_config,\n",
    "                            benchmark_experiment=benchmark_experiment,\n",
    "                            benchmark_batch_sizes=benchmark_batch_sizes,\n",
    "                            benchmark_iterations=benchmark_iterations,\n",
    "                            benchmark_warmup=benchmark_warmup,\n",
    "                            benchmark_max_length=benchmark_max_length,\n",
    "                            benchmark_device=benchmark_device,\n",
    "                            benchmark_tracker=benchmark_tracker,\n",
    "                            backup_enabled=BACKUP_ENABLED,\n",
    "                            backup_to_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            ensure_restored_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                            mlflow_client=mlflow_client,\n",
    "                        )\n",
    "                        \n",
    "                        print(f\"\\n✓ Benchmarking complete. Results saved to MLflow experiment: {benchmark_experiment_name}\")\n",
    "                    else:\n",
    "                        print(\"⚠ No champions with checkpoints available for benchmarking.\")\n",
    "                else:\n",
    "                    print(f\"⚠ Test data not found. Skipping benchmarking.\")\n",
    "                    print(f\"   Tried paths:\")\n",
    "                    if benchmark_config.get(\"benchmarking\", {}).get(\"test_data\"):\n",
    "                        print(f\"     - {Path(benchmark_config['benchmarking']['test_data'])}\")\n",
    "                    if data_config.get(\"local_path\"):\n",
    "                        local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "                        dataset_path = (CONFIG_DIR / local_path_str).resolve()\n",
    "                        seed = data_config.get(\"seed\")\n",
    "                        if seed is not None and \"dataset_tiny\" in str(dataset_path):\n",
    "                            dataset_path = dataset_path / f\"seed{seed}\"\n",
    "                        print(f\"     - {dataset_path / 'test.json'}\")\n",
    "                        print(f\"     - {dataset_path / 'validation.json'}\")\n",
    "                    print(f\"     - {CONFIG_DIR / 'dataset' / 'test.json'}\")\n",
    "                    print(f\"     - {CONFIG_DIR / 'dataset' / 'validation.json'}\")\n",
    "                    print(f\"   💡 Tip: Set 'benchmarking.test_data' in config/benchmark.yaml to specify exact path\")\n",
    "            else:\n",
    "                print(\"✓ All champions already benchmarked - nothing to do!\")\n",
    "else:\n",
    "    print(\"⏭ Skipping benchmarking (RUN_BENCHMARKING=False).\")\n",
    "    print(\"   If benchmark runs don't exist, set RUN_BENCHMARKING=True or run benchmarking in notebook 01.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs (created by `01_orchestrate_training_colab.ipynb` or Step 6 above using `evaluation.benchmarking.benchmark_best_trials`), join to training runs via grouping tags, and select the best model using normalized composite scoring.\n",
    "\n",
    "**Note**: Benchmark runs must exist in MLflow before running this step. If no benchmark runs are found, either:\n",
    "- Set `RUN_BENCHMARKING=True` in Step 6 above, or\n",
    "- Go back to `01_orchestrate_training_colab.ipynb` and run the benchmarking step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Check if we should reuse cached selection\n",
    "run_mode = selection_config.get(\"run\", {}).get(\"mode\", \"reuse_if_exists\")\n",
    "best_model = None\n",
    "cache_data = None\n",
    "\n",
    "print(f\"\\n📋 Best Model Selection Mode: {run_mode}\")\n",
    "\n",
    "if run_mode == \"reuse_if_exists\":\n",
    "    from selection.cache import load_cached_best_model\n",
    "\n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    cache_data = load_cached_best_model(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment_id=benchmark_experiment[\"id\"],\n",
    "        tracking_uri=tracking_uri,\n",
    "    )\n",
    "\n",
    "    if cache_data:\n",
    "        best_model = cache_data[\"best_model\"]\n",
    "        # Success message already printed by load_cached_best_model\n",
    "    else:\n",
    "        print(f\"\\nℹ Cache not available or invalid - will query MLflow for fresh selection\")\n",
    "elif run_mode == \"force_new\":\n",
    "    print(f\"  Mode is 'force_new' - skipping cache, querying MLflow...\")\n",
    "else:\n",
    "    print(f\"  ⚠ Unknown run mode '{run_mode}', defaulting to querying MLflow...\")\n",
    "\n",
    "if best_model is None:\n",
    "    # Find best model\n",
    "    best_model = find_best_model_from_mlflow(\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tags_config=tags_config,\n",
    "        selection_config=selection_config\n",
    "    )\n",
    "\n",
    "    if best_model is None:\n",
    "        # Provide diagnostic information\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "\n",
    "        client = MlflowClient()\n",
    "        tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "\n",
    "        # Check benchmark experiment\n",
    "        benchmark_runs = client.search_runs(\n",
    "            experiment_ids=[benchmark_experiment[\"id\"]],\n",
    "            filter_string=\"\",\n",
    "            max_results=100,\n",
    "        )\n",
    "        finished_benchmark_runs = [r for r in benchmark_runs if r.info.status == \"FINISHED\"]\n",
    "\n",
    "        # Check HPO experiments\n",
    "        hpo_run_counts = {}\n",
    "        hpo_trial_runs = []\n",
    "        hpo_refit_runs = []\n",
    "        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "\n",
    "        for backbone, exp_info in hpo_experiments.items():\n",
    "            hpo_runs = client.search_runs(\n",
    "                experiment_ids=[exp_info[\"id\"]],\n",
    "                filter_string=\"\",\n",
    "                max_results=100,\n",
    "            )\n",
    "            finished_hpo_runs = [r for r in hpo_runs if r.info.status == \"FINISHED\"]\n",
    "            hpo_run_counts[backbone] = len(finished_hpo_runs)\n",
    "\n",
    "            # Separate trial and refit runs\n",
    "            for run in finished_hpo_runs:\n",
    "                stage = run.data.tags.get(stage_tag, \"\")\n",
    "                if stage == \"hpo\" or stage == \"hpo_trial\":\n",
    "                    hpo_trial_runs.append(run)\n",
    "                elif stage == \"hpo_refit\":\n",
    "                    hpo_refit_runs.append(run)\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from benchmark runs\n",
    "        benchmark_pairs = set()\n",
    "        for run in finished_benchmark_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                benchmark_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO trial runs\n",
    "        hpo_trial_pairs = set()\n",
    "        for run in hpo_trial_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_trial_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO refit runs\n",
    "        hpo_refit_pairs = set()\n",
    "        for run in hpo_refit_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_refit_pairs.add((study_hash, trial_hash))\n",
    "\n",
    "        # Find matching pairs\n",
    "        matching_pairs = benchmark_pairs & hpo_trial_pairs\n",
    "\n",
    "        error_msg = (\n",
    "            \"Could not find best model from MLflow.\\n\\n\"\n",
    "            \"Diagnostics:\\n\"\n",
    "            f\"  - Benchmark experiment '{benchmark_experiment['name']}': \"\n",
    "            f\"{len(finished_benchmark_runs)} finished run(s) found\\n\"\n",
    "            f\"    - Unique (study_hash, trial_hash) pairs: {len(benchmark_pairs)}\\n\"\n",
    "        )\n",
    "\n",
    "        if hpo_run_counts:\n",
    "            error_msg += \"  - HPO experiments:\\n\"\n",
    "            for backbone, count in hpo_run_counts.items():\n",
    "                error_msg += f\"    - {backbone}: {count} finished run(s) found\\n\"\n",
    "            error_msg += (\n",
    "                f\"    - HPO trial runs: {len(hpo_trial_runs)} with {len(hpo_trial_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "                f\"    - HPO refit runs: {len(hpo_refit_runs)} with {len(hpo_refit_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "            )\n",
    "\n",
    "        error_msg += (\n",
    "            f\"\\n  - Matching pairs: {len(matching_pairs)} out of {len(benchmark_pairs)} benchmark pairs\\n\"\n",
    "        )\n",
    "\n",
    "        if len(matching_pairs) == 0 and len(benchmark_pairs) > 0 and len(hpo_trial_pairs) > 0:\n",
    "            # Show sample hashes for debugging\n",
    "            error_msg += \"\\n  Sample benchmark (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(benchmark_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "\n",
    "            error_msg += \"\\n  Sample HPO trial (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(hpo_trial_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "\n",
    "            error_msg += (\n",
    "                \"\\n  ⚠️  Hash mismatch detected! This usually means:\\n\"\n",
    "                \"     - Benchmark runs were created from different trials than current HPO runs\\n\"\n",
    "                \"     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\\n\"\n",
    "                \"     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\\n\"\n",
    "            )\n",
    "\n",
    "        error_msg += (\n",
    "            \"\\nPossible causes:\\n\"\n",
    "            \"  1. No benchmark runs have been executed yet. Run benchmark jobs first.\\n\"\n",
    "            \"  2. Benchmark runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  3. HPO runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  4. No matching runs found between benchmark and HPO experiments (hash mismatch).\\n\"\n",
    "            \"\\nCheck the logs above for detailed information about what was found.\"\n",
    "        )\n",
    "\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    # Save to cache\n",
    "    from selection.cache import save_best_model_cache\n",
    "\n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    inputs_summary = {}\n",
    "\n",
    "    timestamped_file, latest_file, index_file = save_best_model_cache(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tracking_uri=tracking_uri,\n",
    "        inputs_summary=inputs_summary,\n",
    "    )\n",
    "    print(f\"✓ Saved best model selection to cache\")\n",
    "\n",
    "# Extract lineage information from best_model for final training tags\n",
    "from training_exec import extract_lineage_from_best_model\n",
    "lineage = extract_lineage_from_best_model(best_model)\n",
    "\n",
    "# Acquire checkpoint\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    acquisition_config=acquisition_config,\n",
    "    selection_config=selection_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Best model checkpoint available at: {best_checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config.key(\"process\", \"stage\")\n",
    "trained_on_full_data_tag = tags_config.key(\"training\", \"trained_on_full_data\")\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"🔄 Starting final training with best configuration...\")\n",
    "    from training_exec import execute_final_training\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    final_checkpoint_dir = execute_final_training(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "else:\n",
    "    print(\"✓ Skipping final training - using selected checkpoint\")\n",
    "\n",
    "# Backup final checkpoint to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and final_checkpoint_dir:\n",
    "    checkpoint_path = Path(final_checkpoint_dir).resolve()\n",
    "    # Check if checkpoint is already in Drive\n",
    "    if str(checkpoint_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n✓ Final training checkpoint is already in Google Drive\")\n",
    "        print(f\"  Drive path: {checkpoint_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\n📦 Backing up final training checkpoint to Google Drive...\")\n",
    "            result = drive_store.backup(checkpoint_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"✓ Successfully backed up final checkpoint to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"⚠ Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Drive backup error: {e}\")\n",
    "            print(f\"  Checkpoint is still available locally at: {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parent training information for conversion\n",
    "from common.shared.json_cache import load_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "\n",
    "if not final_training_metadata_path.exists():\n",
    "    raise ValueError(\n",
    "        f\"Metadata file not found: {final_training_metadata_path}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "metadata = load_json(final_training_metadata_path)\n",
    "parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    raise ValueError(\n",
    "        f\"Missing required fingerprints in metadata: spec_fp={parent_spec_fp}, exec_fp={parent_exec_fp}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "if parent_training_run_id:\n",
    "    print(f\"✓ Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}..., run_id={parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"✓ Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}... (run_id not found)\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\n🔄 Starting model conversion...\")\n",
    "from conversion import execute_conversion\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "conversion_output_dir = execute_conversion(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    ")\n",
    "\n",
    "# Find ONNX model file (search recursively, as model may be in onnx_model/ subdirectory)\n",
    "onnx_files = list(conversion_output_dir.rglob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\n✓ Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Warning: No ONNX model file found in {conversion_output_dir} (searched recursively)\")\n",
    "\n",
    "# Backup conversion output to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and conversion_output_dir:\n",
    "    output_path = Path(conversion_output_dir).resolve()\n",
    "    # Check if output is already in Drive\n",
    "    if str(output_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n✓ Conversion output is already in Google Drive\")\n",
    "        print(f\"  Drive path: {output_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\n📦 Backing up conversion output to Google Drive...\")\n",
    "            result = drive_store.backup(output_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"✓ Successfully backed up conversion output to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"⚠ Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Drive backup error: {e}\")\n",
    "            print(f\"  Output is still available locally at: {conversion_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
