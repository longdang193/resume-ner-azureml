{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk â†’ drive restore â†’ MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"âœ“ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IS_LOCAL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install required packages\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mIS_LOCAL\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor local environment, please:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Create conda environment: conda env create -f config/environment/conda.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IS_LOCAL' is not defined"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"âœ“ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Local environment detected - assuming repository already exists\n",
      "âœ“ Repository root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "âœ“ Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "âœ“ Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "âœ“ Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b gg_final_training https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b gg_final_training https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"âœ“ Local environment detected - assuming repository already exists\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, try to find repo root\n",
    "    ROOT_DIR = Path.cwd()\n",
    "    # Try to find the repo root by looking for config directory\n",
    "    while ROOT_DIR.parent != ROOT_DIR:\n",
    "        if (ROOT_DIR / \"config\").exists() and (ROOT_DIR / \"src\").exists():\n",
    "            break\n",
    "        ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"âœ“ Repository root: {ROOT_DIR}\")\n",
    "print(f\"âœ“ Config directory: {CONFIG_DIR}\")\n",
    "print(f\"âœ“ Source directory: {SRC_DIR}\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"âœ“ Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded experiment config: resume_ner_baseline\n",
      "âœ“ Loaded tags config\n",
      "âœ“ Loaded best model selection config\n",
      "âœ“ Loaded conversion config\n",
      "âœ“ Loaded artifact acquisition config\n",
      "\n",
      "âœ“ Experiment names defined:\n",
      "  - Benchmark: resume_ner_baseline-benchmark\n",
      "  - Training: resume_ner_baseline-training\n",
      "  - Conversion: resume_ner_baseline-conversion\n",
      "\n",
      "Note: Experiment discovery will happen after MLflow setup (Cell 4)\n"
     ]
    }
   ],
   "source": [
    "from orchestration.config_loader import load_experiment_config\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from shared.yaml_utils import load_yaml\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "print(f\"âœ“ Loaded experiment config: {experiment_config.name}\")\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_yaml(CONFIG_DIR / \"tags.yaml\")\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"âœ“ Loaded tags config\")\n",
    "print(f\"âœ“ Loaded best model selection config\")\n",
    "print(f\"âœ“ Loaded conversion config\")\n",
    "print(f\"âœ“ Loaded artifact acquisition config\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"\\nâœ“ Experiment names defined:\")\n",
    "print(f\"  - Benchmark: {benchmark_experiment_name}\")\n",
    "print(f\"  - Training: {training_experiment_name}\")\n",
    "print(f\"  - Conversion: {conversion_experiment_name}\")\n",
    "print(f\"\\nNote: Experiment discovery will happen after MLflow setup (Cell 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 00:07:21,889 - shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-05 00:07:24,629 - shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-05 00:07:24,630 - shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-05 00:07:24,631 - shared.mlflow_setup - INFO - Loading credentials from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-05 00:07:24,633 - shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-05 00:07:24,634 - shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-05 00:07:24,635 - shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-05 00:07:24,636 - shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-05 00:07:24,637 - shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n",
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-05 00:07:27,635 - shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-05 00:07:29,617 - shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "âœ“ MLflow experiment: resume_ner_baseline-training\n",
      "\n",
      "âœ“ Found 2 HPO experiment(s):\n",
      "  - resume_ner_baseline-hpo-distilbert (backbone: distilbert)\n",
      "  - resume_ner_baseline-hpo-distilroberta (backbone: distilroberta)\n",
      "âœ“ Benchmark experiment: resume_ner_baseline-benchmark\n",
      "âœ“ Training experiment: resume_ner_baseline-training (for final training runs)\n",
      "âœ“ Conversion experiment: resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "from shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"âœ“ MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(hpo_experiments)} HPO experiment(s):\")\n",
    "for backbone, exp_info in hpo_experiments.items():\n",
    "    print(f\"  - {exp_info['name']} (backbone: {backbone})\")\n",
    "\n",
    "if benchmark_experiment:\n",
    "    print(f\"âœ“ Benchmark experiment: {benchmark_experiment['name']}\")\n",
    "else:\n",
    "    print(f\"âš  Benchmark experiment not found: {benchmark_experiment_name}\")\n",
    "\n",
    "print(f\"âœ“ Training experiment: {training_experiment_name} (for final training runs)\")\n",
    "print(f\"âœ“ Conversion experiment: {conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from orchestration.drive_backup import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"âš  Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"âœ“ Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"âœ“ Google Drive mounted\")\n",
    "        print(f\"âœ“ Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"âš  Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"âœ“ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"âœ“ Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs, join to training runs via grouping tags, and select the best model using normalized composite scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'orchestration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01morchestration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjobs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlflow_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_best_model_from_mlflow\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01morchestration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjobs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact_acquisition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acquire_best_model_checkpoint\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'orchestration'"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs.selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from orchestration.jobs.selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Find best model\n",
    "best_model = find_best_model_from_mlflow(\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    "    hpo_experiments=hpo_experiments,\n",
    "    tags_config=tags_config,\n",
    "    selection_config=selection_config,\n",
    "    use_python_filtering=True,\n",
    ")\n",
    "\n",
    "if best_model is None:\n",
    "    raise ValueError(\"Could not find best model from MLflow.\")\n",
    "\n",
    "# Acquire checkpoint\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    acquisition_config=acquisition_config,\n",
    "    selection_config=selection_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Best model checkpoint available at: {best_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config[\"process\"][\"stage\"]\n",
    "trained_on_full_data_tag = tags_config[\"training\"][\"trained_on_full_data\"]\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Training (Conditional)\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"ðŸ”„ Starting final training with best configuration...\")\n",
    "    \n",
    "    from orchestration.final_training_config import create_final_training_config\n",
    "    from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "    from orchestration.fingerprints import compute_spec_fp, compute_exec_fp\n",
    "    from orchestration.config_loader import load_all_configs\n",
    "    from shared.platform_detection import detect_platform\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Extract best config from MLflow run\n",
    "    best_params = best_model[\"params\"]\n",
    "    \n",
    "    # Create final training config\n",
    "    all_configs = load_all_configs(experiment_config)\n",
    "    environment = detect_platform()\n",
    "    \n",
    "    # Compute fingerprints\n",
    "    spec_fp = compute_spec_fp(\n",
    "        model_config=all_configs.get(\"model\", {}),\n",
    "        data_config=all_configs.get(\"data\", {}),\n",
    "        train_config=all_configs.get(\"train\", {}),\n",
    "        seed=int(best_params.get(\"random_seed\", 42)),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        git_sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "            cwd=ROOT_DIR,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        git_sha = None\n",
    "    \n",
    "    exec_fp = compute_exec_fp(\n",
    "        git_sha=git_sha,\n",
    "        env_config=all_configs.get(\"env\", {}),\n",
    "    )\n",
    "    \n",
    "    # Create training context\n",
    "    backbone_name = best_params.get(\"backbone\", \"distilbert\")\n",
    "    if \"-\" in backbone_name:\n",
    "        backbone_name = backbone_name.split(\"-\")[0]\n",
    "    \n",
    "    from orchestration.final_training_config import _compute_next_variant\n",
    "    variant = _compute_next_variant(\n",
    "        ROOT_DIR,\n",
    "        CONFIG_DIR,\n",
    "        spec_fp,\n",
    "        exec_fp,\n",
    "        backbone_name,\n",
    "    )\n",
    "    \n",
    "    training_context = create_naming_context(\n",
    "        process_type=\"final_training\",\n",
    "        model=backbone_name,\n",
    "        spec_fp=spec_fp,\n",
    "        exec_fp=exec_fp,\n",
    "        environment=environment,\n",
    "        variant=variant,\n",
    "    )\n",
    "    \n",
    "    final_output_dir = build_output_path(ROOT_DIR, training_context)\n",
    "    \n",
    "    # Create final training config with best hyperparameters\n",
    "    final_training_config = {\n",
    "        \"backbone\": best_params.get(\"backbone\"),\n",
    "        \"learning_rate\": float(best_params.get(\"learning_rate\", 2e-5)),\n",
    "        \"batch_size\": int(best_params.get(\"batch_size\", 16)),\n",
    "        \"dropout\": float(best_params.get(\"dropout\", 0.1)),\n",
    "        \"weight_decay\": float(best_params.get(\"weight_decay\", 0.01)),\n",
    "        \"epochs\": int(best_params.get(\"epochs\", 10)),\n",
    "        \"random_seed\": int(best_params.get(\"random_seed\", 42)),\n",
    "        \"early_stopping_enabled\": best_params.get(\"early_stopping_enabled\", \"true\").lower() == \"true\",\n",
    "        \"use_combined_data\": True,  # Use full dataset for final training\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Final training config: {final_training_config}\")\n",
    "    print(f\"âœ“ Output directory: {final_output_dir}\")\n",
    "    \n",
    "    # Load dataset path from config\n",
    "    from shared.yaml_utils import load_yaml\n",
    "    data_config = all_configs.get(\"data\", {})\n",
    "    dataset_path = data_config.get(\"dataset_path\", \"data/resume_tiny\")\n",
    "    DATASET_LOCAL_PATH = ROOT_DIR / dataset_path if not Path(dataset_path).is_absolute() else Path(dataset_path)\n",
    "    \n",
    "    # Run training as a module\n",
    "    training_args = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"training.train\",\n",
    "        \"--data-asset\",\n",
    "        str(DATASET_LOCAL_PATH),\n",
    "        \"--config-dir\",\n",
    "        str(CONFIG_DIR),\n",
    "        \"--backbone\",\n",
    "        final_training_config[\"backbone\"],\n",
    "        \"--learning-rate\",\n",
    "        str(final_training_config[\"learning_rate\"]),\n",
    "        \"--batch-size\",\n",
    "        str(final_training_config[\"batch_size\"]),\n",
    "        \"--dropout\",\n",
    "        str(final_training_config[\"dropout\"]),\n",
    "        \"--weight-decay\",\n",
    "        str(final_training_config[\"weight_decay\"]),\n",
    "        \"--epochs\",\n",
    "        str(final_training_config[\"epochs\"]),\n",
    "        \"--random-seed\",\n",
    "        str(final_training_config[\"random_seed\"]),\n",
    "        \"--early-stopping-enabled\",\n",
    "        str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
    "        \"--use-combined-data\",\n",
    "        str(final_training_config[\"use_combined_data\"]).lower(),\n",
    "    ]\n",
    "    \n",
    "    training_env = os.environ.copy()\n",
    "    training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
    "    \n",
    "    # Add src directory to PYTHONPATH\n",
    "    pythonpath = training_env.get(\"PYTHONPATH\", \"\")\n",
    "    if pythonpath:\n",
    "        training_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "    else:\n",
    "        training_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "    \n",
    "    mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "    if mlflow_tracking_uri:\n",
    "        training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "    training_env[\"MLFLOW_EXPERIMENT_NAME\"] = training_experiment_name\n",
    "    \n",
    "    print(\"ðŸ”„ Running final training...\")\n",
    "    result = subprocess.run(\n",
    "        training_args,\n",
    "        cwd=ROOT_DIR,\n",
    "        env=training_env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Final training failed with return code {result.returncode}\\n\"\n",
    "            f\"STDOUT: {result.stdout}\\n\"\n",
    "            f\"STDERR: {result.stderr}\"\n",
    "        )\n",
    "    else:\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "    \n",
    "    # Set final checkpoint directory\n",
    "    final_checkpoint_dir = final_output_dir / \"checkpoint\"\n",
    "    if not final_checkpoint_dir.exists():\n",
    "        # Try actual checkpoint location\n",
    "        actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "        if actual_checkpoint.exists():\n",
    "            final_checkpoint_dir = actual_checkpoint\n",
    "    \n",
    "    print(f\"âœ“ Final training completed. Checkpoint: {final_checkpoint_dir}\")\n",
    "    \n",
    "    # Try to set tag in MLflow run\n",
    "    try:\n",
    "        from orchestration.jobs.tracking.finder.run_finder import find_mlflow_run\n",
    "        from orchestration.jobs.tracking.trackers.training_tracker import MLflowTrainingTracker\n",
    "        \n",
    "        training_tracker = MLflowTrainingTracker(training_experiment_name)\n",
    "        report = find_mlflow_run(\n",
    "            experiment_name=training_experiment_name,\n",
    "            context=training_context,\n",
    "            output_dir=final_output_dir,\n",
    "            strict=False,\n",
    "            root_dir=ROOT_DIR,\n",
    "            config_dir=CONFIG_DIR,\n",
    "        )\n",
    "        \n",
    "        if report.found and report.run_id:\n",
    "            with mlflow.start_run(run_id=report.run_id):\n",
    "                mlflow.set_tag(\"code.trained_on_full_data\", \"true\")\n",
    "                print(f\"âœ“ Set code.trained_on_full_data tag in MLflow run {report.run_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not set MLflow tag: {e}\")\n",
    "else:\n",
    "    print(\"âœ“ Skipping final training - using selected checkpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Starting model conversion to ONNX...\")\n",
    "\n",
    "from orchestration.jobs.tracking.trackers.conversion_tracker import MLflowConversionTracker\n",
    "from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "from orchestration.fingerprints import build_parent_training_id, compute_conv_fp\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup conversion tracker\n",
    "conversion_tracker = MLflowConversionTracker(conversion_experiment_name)\n",
    "\n",
    "# Determine checkpoint path\n",
    "checkpoint_path = final_checkpoint_dir\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    raise ValueError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "\n",
    "# Extract backbone from best model\n",
    "backbone = best_model[\"backbone\"]\n",
    "\n",
    "# Conversion settings\n",
    "# Load conversion settings from config\n",
    "quantization = conversion_config[\"onnx\"][\"quantization\"]\n",
    "opset_version = conversion_config[\"onnx\"][\"opset_version\"]\n",
    "conversion_target = conversion_config[\"target\"][\"format\"]\n",
    "\n",
    "print(f\"âœ“ Checkpoint path: {checkpoint_path}\")\n",
    "print(f\"âœ“ Backbone: {backbone}\")\n",
    "print(f\"âœ“ Conversion target: {conversion_target}\")\n",
    "print(f\"âœ“ Quantization: {quantization}\")\n",
    "print(f\"âœ“ ONNX opset version: {opset_version}\")\n",
    "\n",
    "# Extract or compute fingerprints for conversion context\n",
    "# Try to get from best model tags first\n",
    "spec_fp = best_model[\"tags\"].get(\"code.spec_fp\")\n",
    "exec_fp = best_model[\"tags\"].get(\"code.exec_fp\")\n",
    "variant = int(best_model[\"tags\"].get(\"code.variant\", \"1\")) if best_model[\"tags\"].get(\"code.variant\") else 1\n",
    "\n",
    "# If not in tags, compute from config\n",
    "if not spec_fp or not exec_fp:\n",
    "    from orchestration.fingerprints import compute_spec_fp, compute_exec_fp\n",
    "    from orchestration.config_loader import load_all_configs\n",
    "    import subprocess\n",
    "    \n",
    "    all_configs = load_all_configs(experiment_config)\n",
    "    \n",
    "    spec_fp = compute_spec_fp(\n",
    "        model_config=all_configs.get(\"model\", {}),\n",
    "        data_config=all_configs.get(\"data\", {}),\n",
    "        train_config=all_configs.get(\"train\", {}),\n",
    "        seed=int(best_model[\"params\"].get(\"random_seed\", 42)),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        git_sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "            cwd=ROOT_DIR,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        git_sha = None\n",
    "    \n",
    "    exec_fp = compute_exec_fp(\n",
    "        git_sha=git_sha,\n",
    "        env_config=all_configs.get(\"env\", {}),\n",
    "    )\n",
    "\n",
    "# Build parent_training_id\n",
    "parent_training_id = build_parent_training_id(spec_fp, exec_fp, variant)\n",
    "\n",
    "# Compute conversion fingerprint\n",
    "conv_fp = compute_conv_fp(backbone, quantization, opset_version)\n",
    "\n",
    "# Create conversion context\n",
    "conversion_context = create_naming_context(\n",
    "    process_type=\"conversion\",\n",
    "    model=backbone.split(\"-\")[0] if \"-\" in backbone else backbone,\n",
    "    environment=PLATFORM,\n",
    "    parent_training_id=parent_training_id,\n",
    "    conv_fp=conv_fp,\n",
    ")\n",
    "\n",
    "# Build conversion output path using canonical structure\n",
    "conversion_output_dir = build_output_path(ROOT_DIR, conversion_context)\n",
    "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Conversion output: {conversion_output_dir}\")\n",
    "\n",
    "# Run conversion\n",
    "try:\n",
    "    # Import conversion function\n",
    "    from model_conversion.onnx_exporter import export_to_onnx\n",
    "    \n",
    "    # Start conversion run\n",
    "    conversion_run_name = f\"conversion_{backbone}_{conversion_target}\"\n",
    "    \n",
    "    with conversion_tracker.start_conversion_run(\n",
    "        run_name=conversion_run_name,\n",
    "        conversion_type=conversion_target,\n",
    "        source_training_run=best_model[\"run_id\"],\n",
    "        output_dir=conversion_output_dir,\n",
    "    ) as conversion_handle:\n",
    "        \n",
    "        # Log conversion parameters\n",
    "        conversion_tracker.log_conversion_parameters(\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            conversion_target=conversion_target,\n",
    "            quantization=quantization,\n",
    "            opset_version=opset_version,\n",
    "            backbone=backbone,\n",
    "        )\n",
    "        \n",
    "        # Perform conversion\n",
    "        onnx_model_path = export_to_onnx(\n",
    "            checkpoint_dir=checkpoint_path,\n",
    "            output_dir=conversion_output_dir,\n",
    "            quantize_int8=(quantization == \"int8\"),\n",
    "        )\n",
    "        \n",
    "        # Calculate original checkpoint size for compression ratio\n",
    "        original_size = None\n",
    "        if checkpoint_path.exists():\n",
    "            total_size = sum(f.stat().st_size for f in checkpoint_path.rglob(\"*\") if f.is_file())\n",
    "            original_size = total_size / (1024 * 1024)  # MB\n",
    "        \n",
    "        # Log conversion results\n",
    "        conversion_tracker.log_conversion_results(\n",
    "            conversion_success=True,\n",
    "            onnx_model_path=onnx_model_path,\n",
    "            original_checkpoint_size=original_size,\n",
    "            smoke_test_passed=True,  # Could add actual smoke test\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Model converted successfully!\")\n",
    "        print(f\"âœ“ ONNX model saved to: {onnx_model_path}\")\n",
    "        \n",
    "        if conversion_handle:\n",
    "            print(f\"âœ“ Conversion logged to MLflow run: {conversion_handle.run_id}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âš  Conversion failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Summary\n",
    "\n",
    "Display summary of best model selection, artifact locations, and conversion status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ“ Best Model Selected:\")\n",
    "print(f\"   Run ID: {best_model['run_id']}\")\n",
    "print(f\"   Run Name: {best_model['run_name']}\")\n",
    "print(f\"   Backbone: {best_model['backbone']}\")\n",
    "print(f\"   Macro-F1: {best_model['primary_metric']:.4f}\")\n",
    "if best_model['latency_ms']:\n",
    "    print(f\"   Latency: {best_model['latency_ms']:.2f} ms\")\n",
    "if best_model['throughput']:\n",
    "    print(f\"   Throughput: {best_model['throughput']:.2f} docs/sec\")\n",
    "print(f\"   Confidence: {best_model['confidence']}\")\n",
    "print(f\"   Source: {best_model['source_type']}\")\n",
    "\n",
    "print(f\"\\nâœ“ Artifacts:\")\n",
    "print(f\"   Checkpoint: {final_checkpoint_dir}\")\n",
    "\n",
    "if 'onnx_model_path' in locals():\n",
    "    print(f\"   ONNX Model: {onnx_model_path}\")\n",
    "\n",
    "print(f\"\\nâœ“ MLflow Tracking:\")\n",
    "print(f\"   Training Experiment: {training_experiment_name}\")\n",
    "print(f\"   Benchmark Experiment: {benchmark_experiment_name}\")\n",
    "print(f\"   Conversion Experiment: {conversion_experiment_name}\")\n",
    "\n",
    "print(f\"\\nâœ“ Final Training:\")\n",
    "print(f\"   {'Skipped (already final training on full data)' if SKIP_FINAL_TRAINING else 'Completed'}\")\n",
    "\n",
    "print(\"\\nâœ… Best model selection and conversion pipeline completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
