{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Training Orchestration (Google Colab)\n",
        "\n",
        "This notebook orchestrates all training activities for **Google Colab execution** with GPU compute support.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **Step 1**: Repository Setup & Environment Configuration\n",
        "- **Step 2**: Load Centralized Configs\n",
        "- **Step 3**: Verify Local Dataset (from data config)\n",
        "- **Step 4**: Setup Local Environment\n",
        "- **Step 5**: The Dry Run\n",
        "- **Step 6**: The Sweep (HPO) - Local with Optuna\n",
        "- **Step 7**: Best Configuration Selection (Automated)\n",
        "- **Step 8**: Final Training (Post-HPO, Single Run)\n",
        "- **Step 9**: Model Conversion & Optimization\n",
        "\n",
        "## Important\n",
        "\n",
        "- This notebook **executes training in Google Colab** (not on Azure ML)\n",
        "- All computation happens on Google Colab GPU\n",
        "- **Google Drive Integration**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
        "- The notebook must be **re-runnable end-to-end**\n",
        "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
        "- **Session Management**: Colab sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically, so you can resume from Drive if the session disconnects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Repository Setup\n",
        "\n",
        "Set up the repository in Google Colab. Choose one of the following options:\n",
        "\n",
        "### Option A: Clone from Git (Recommended)\n",
        "\n",
        "If your repository is on GitHub/GitLab, clone it:\n",
        "\n",
        "```python\n",
        "!git clone <your-repository-url> /content/resume-ner-azureml\n",
        "```\n",
        "\n",
        "### Option B: Upload Files\n",
        "\n",
        "If you prefer to upload files manually:\n",
        "1. Use the Colab file browser (folder icon on left sidebar)\n",
        "2. Upload your project files to `/content/resume-ner-azureml/`\n",
        "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
        "\n",
        "### Verify Repository Setup\n",
        "\n",
        "After cloning or uploading, verify the repository structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Repository not found at /content/resume-ner-azureml\nPlease run Step 1 to clone or upload the repository.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-428683789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Verify repository structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mROOT_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34mf\"Repository not found at {ROOT_DIR}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34mf\"Please run Step 1 to clone or upload the repository.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Repository not found at /content/resume-ner-azureml\nPlease run Step 1 to clone or upload the repository."
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set repository root directory\n",
        "# Change this if you used a different path in Step 1\n",
        "ROOT_DIR = Path(\"/content/resume-ner-azureml\")\n",
        "\n",
        "# Verify repository structure\n",
        "if not ROOT_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Repository not found at {ROOT_DIR}\\n\"\n",
        "        f\"Please run Step 1 to clone or upload the repository.\"\n",
        "    )\n",
        "\n",
        "required_dirs = [\"src\", \"config\", \"notebooks\"]\n",
        "missing_dirs = [d for d in required_dirs if not (ROOT_DIR / d).exists()]\n",
        "\n",
        "if missing_dirs:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing required directories: {missing_dirs}\\n\"\n",
        "        f\"Please ensure the repository structure is correct.\"\n",
        "    )\n",
        "\n",
        "print(f\"✓ Repository found at: {ROOT_DIR}\")\n",
        "print(f\"✓ Required directories found: {required_dirs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies\n",
        "\n",
        "Install all required Python packages. PyTorch is usually pre-installed in Colab, but we'll verify the version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check PyTorch version (usually pre-installed in Colab)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Verify PyTorch version meets requirements (>=2.6.0)\n",
        "torch_version = tuple(map(int, torch.__version__.split('.')[:2]))\n",
        "if torch_version < (2, 6):\n",
        "    print(f\"⚠ Warning: PyTorch {torch.__version__} may not meet requirements (>=2.6.0)\")\n",
        "    print(\"Consider upgrading: !pip install torch>=2.6.0 --upgrade\")\n",
        "else:\n",
        "    print(\"✓ PyTorch version meets requirements\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Core ML libraries\n",
        "%pip install transformers>=4.35.0,<5.0.0 --quiet\n",
        "%pip install safetensors>=0.4.0 --quiet\n",
        "%pip install datasets>=2.12.0 --quiet\n",
        "\n",
        "# ML utilities\n",
        "%pip install numpy>=1.24.0,<2.0.0 --quiet\n",
        "%pip install pandas>=2.0.0 --quiet\n",
        "%pip install scikit-learn>=1.3.0 --quiet\n",
        "\n",
        "# Utilities\n",
        "%pip install pyyaml>=6.0 --quiet\n",
        "%pip install tqdm>=4.65.0 --quiet\n",
        "%pip install seqeval>=1.2.2 --quiet\n",
        "%pip install sentencepiece>=0.1.99 --quiet\n",
        "\n",
        "# Experiment tracking\n",
        "%pip install mlflow --quiet\n",
        "%pip install optuna --quiet\n",
        "\n",
        "# ONNX support\n",
        "%pip install onnxruntime --quiet\n",
        "%pip install onnx>=1.16.0 --quiet\n",
        "%pip install onnxscript>=0.1.0 --quiet\n",
        "\n",
        "print(\"✓ All dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Setup Paths and Import Paths\n",
        "\n",
        "Configure Python paths and verify Colab environment detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify Colab environment\n",
        "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
        "\n",
        "# Setup paths (ROOT_DIR should be set in Cell 2)\n",
        "# If not, set it here\n",
        "if 'ROOT_DIR' not in globals():\n",
        "    ROOT_DIR = Path(\"/content/resume-ner-azureml\")\n",
        "\n",
        "SRC_DIR = ROOT_DIR / \"src\"\n",
        "CONFIG_DIR = ROOT_DIR / \"config\"\n",
        "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.append(str(ROOT_DIR))\n",
        "sys.path.append(str(SRC_DIR))\n",
        "\n",
        "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
        "print(\"Project root:\", ROOT_DIR)\n",
        "print(\"Source directory:\", SRC_DIR)\n",
        "print(\"Config directory:\", CONFIG_DIR)\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"⚠ Warning: Not detected as Colab environment. Some features may not work correctly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Mount Google Drive\n",
        "\n",
        "Mount Google Drive to enable checkpoint persistence across Colab sessions. Checkpoints will be automatically saved to Drive after training completes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3096143528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define backup directory in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define backup directory in Google Drive\n",
        "DRIVE_BACKUP_DIR = Path(\"/content/drive/MyDrive/resume-ner-checkpoints\")\n",
        "DRIVE_BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"✓ Google Drive mounted\")\n",
        "print(f\"✓ Checkpoint backup directory: {DRIVE_BACKUP_DIR}\")\n",
        "print(f\"\\nNote: Checkpoints will be automatically saved to this directory after training completes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.1: Load Centralized Configs\n",
        "\n",
        "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "from orchestration import EXPERIMENT_NAME\n",
        "from orchestration.config_loader import (\n",
        "    ExperimentConfig,\n",
        "    compute_config_hashes,\n",
        "    create_config_metadata,\n",
        "    load_all_configs,\n",
        "    load_experiment_config,\n",
        "    snapshot_configs,\n",
        "    validate_config_immutability,\n",
        ")\n",
        "\n",
        "# P1-3.1: Load Centralized Configs (local-only)\n",
        "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
        "\n",
        "if not CONFIG_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
        "\n",
        "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
        "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
        "config_hashes = compute_config_hashes(configs)\n",
        "config_metadata = create_config_metadata(configs, config_hashes)\n",
        "\n",
        "# Immutable snapshots for runtime mutation checks\n",
        "original_configs = snapshot_configs(configs)\n",
        "validate_config_immutability(configs, original_configs)\n",
        "\n",
        "print(f\"Loaded experiment: {experiment_config.name}\")\n",
        "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
        "print(\"Config hashes:\", config_hashes)\n",
        "print(\"Config metadata:\", config_metadata)\n",
        "\n",
        "# Get dataset path from data config (centralized configuration)\n",
        "# The local_path in the data config is relative to the config directory\n",
        "data_config = configs[\"data\"]\n",
        "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
        "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
        "\n",
        "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.2: Verify Local Dataset\n",
        "\n",
        "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# P1-3.2: Verify Local Dataset\n",
        "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
        "# This ensures the dataset location is controlled by centralized configuration.\n",
        "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
        "\n",
        "REQUIRED_FILE = \"train.json\"\n",
        "OPTIONAL_FILE = \"validation.json\"\n",
        "\n",
        "if not DATASET_LOCAL_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
        "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
        "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
        "    )\n",
        "\n",
        "# Check required file\n",
        "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
        "if not train_file.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Required dataset file not found: {train_file}\\n\"\n",
        "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
        "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
        "    )\n",
        "\n",
        "# Check optional file\n",
        "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
        "has_validation = val_file.exists()\n",
        "\n",
        "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
        "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
        "\n",
        "train_size = train_file.stat().st_size\n",
        "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
        "\n",
        "if has_validation:\n",
        "    val_size = val_file.stat().st_size\n",
        "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
        "else:\n",
        "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.3: Setup Local Environment\n",
        "\n",
        "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "DEFAULT_DEVICE = \"cuda\"\n",
        "\n",
        "env_config = configs[\"env\"]\n",
        "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
        "\n",
        "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
        "    raise RuntimeError(\n",
        "        \"CUDA device requested but not available. \"\n",
        "        \"In Colab, ensure you've selected a GPU runtime: Runtime > Change runtime type > GPU\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "MLFLOW_DIR = \"mlruns\"\n",
        "mlflow_tracking_path = ROOT_DIR / MLFLOW_DIR\n",
        "mlflow_tracking_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Convert path to file:// URI format for MLflow\n",
        "mlflow_tracking_uri = mlflow_tracking_path.as_uri()\n",
        "mlflow.set_tracking_uri(mlflow_tracking_uri)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import transformers\n",
        "    import optuna\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Required package not installed: {e}\")\n",
        "\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"torch\": torch,\n",
        "    \"transformers\": transformers,\n",
        "    \"mlflow\": mlflow,\n",
        "    \"optuna\": optuna,\n",
        "}\n",
        "\n",
        "for name, module in REQUIRED_PACKAGES.items():\n",
        "    if not hasattr(module, \"__version__\"):\n",
        "        raise ImportError(f\"Required package '{name}' is not properly installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.4: The Dry Run\n",
        "\n",
        "Run a minimal HPO sweep to validate the training pipeline works correctly before launching the full HPO sweep. Uses the smoke HPO configuration with reduced trials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib.util\n",
        "from orchestration import STAGE_SMOKE\n",
        "\n",
        "# Import local_sweeps directly to avoid triggering Azure ML imports in __init__.py\n",
        "local_sweeps_spec = importlib.util.spec_from_file_location(\n",
        "    \"local_sweeps\", SRC_DIR / \"orchestration\" / \"jobs\" / \"local_sweeps.py\"\n",
        ")\n",
        "local_sweeps = importlib.util.module_from_spec(local_sweeps_spec)\n",
        "local_sweeps_spec.loader.exec_module(local_sweeps)\n",
        "run_local_hpo_sweep = local_sweeps.run_local_hpo_sweep\n",
        "\n",
        "TRAINING_SCRIPT_PATH = SRC_DIR / \"train.py\"\n",
        "DRY_RUN_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"dry_run\"\n",
        "\n",
        "if not TRAINING_SCRIPT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Training script not found: {TRAINING_SCRIPT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hpo_config = configs[\"hpo\"]\n",
        "train_config = configs[\"train\"]\n",
        "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
        "\n",
        "dry_run_studies = {}\n",
        "\n",
        "for backbone in backbone_values:\n",
        "    mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_SMOKE}-{backbone}\"\n",
        "    backbone_output_dir = DRY_RUN_OUTPUT_DIR / backbone\n",
        "    \n",
        "    study = run_local_hpo_sweep(\n",
        "        dataset_path=str(DATASET_LOCAL_PATH),\n",
        "        config_dir=CONFIG_DIR,\n",
        "        backbone=backbone,\n",
        "        hpo_config=hpo_config,\n",
        "        train_config=train_config,\n",
        "        output_dir=backbone_output_dir,\n",
        "        mlflow_experiment_name=mlflow_experiment_name,\n",
        "    )\n",
        "    \n",
        "    dry_run_studies[backbone] = study\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for backbone, study in dry_run_studies.items():\n",
        "    if study.trials:\n",
        "        best_trial = study.best_trial\n",
        "        print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
        "        print(\n",
        "            f\"  Best {hpo_config['objective']['metric']}: {best_trial.value:.4f}\")\n",
        "        print(f\"  Best params: {best_trial.params}\")\n",
        "    else:\n",
        "        print(f\"{backbone}: No trials completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.5: The Sweep (HPO) - Local with Optuna\n",
        "\n",
        "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n",
        "\n",
        "**Note on K-Fold Cross-Validation:**\n",
        "- When k-fold CV is enabled (`k_fold.enabled: true`), each trial trains **k models** (one per fold) and returns the **average metric** across folds\n",
        "- The number of **trials** is controlled by `sampling.max_trials` (e.g., 2 trials in smoke.yaml)\n",
        "- With k=5 folds and 2 trials: **2 trials × 5 folds = 10 model trainings total**\n",
        "- K-fold CV provides more robust hyperparameter evaluation but increases compute time (k× per trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib.util\n",
        "from orchestration import STAGE_HPO\n",
        "from shared.yaml_utils import load_yaml\n",
        "\n",
        "# Import local_sweeps directly to avoid triggering Azure ML imports in __init__.py\n",
        "local_sweeps_spec = importlib.util.spec_from_file_location(\n",
        "    \"local_sweeps\", SRC_DIR / \"orchestration\" / \"jobs\" / \"local_sweeps.py\"\n",
        ")\n",
        "local_sweeps = importlib.util.module_from_spec(local_sweeps_spec)\n",
        "local_sweeps_spec.loader.exec_module(local_sweeps)\n",
        "run_local_hpo_sweep = local_sweeps.run_local_hpo_sweep\n",
        "\n",
        "DEFAULT_K_FOLDS = 5\n",
        "DEFAULT_RANDOM_SEED = 42\n",
        "HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
        "\n",
        "hpo_stage_config = experiment_config.stages.get(STAGE_HPO, {})\n",
        "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
        "hpo_config_path = CONFIG_DIR / hpo_config_override if hpo_config_override else experiment_config.hpo_config\n",
        "\n",
        "if not hpo_config_path.exists():\n",
        "    raise FileNotFoundError(f\"HPO config not found: {hpo_config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hpo_config = load_yaml(hpo_config_path)\n",
        "train_config = configs[\"train\"]\n",
        "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training.cv_utils import create_kfold_splits, save_fold_splits\n",
        "from training.data import load_dataset\n",
        "\n",
        "k_fold_config = hpo_config.get(\"k_fold\", {})\n",
        "k_folds_enabled = k_fold_config.get(\"enabled\", False)\n",
        "fold_splits_file = None\n",
        "\n",
        "if k_folds_enabled:\n",
        "    n_splits = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS)\n",
        "    random_seed = k_fold_config.get(\"random_seed\", DEFAULT_RANDOM_SEED)\n",
        "    shuffle = k_fold_config.get(\"shuffle\", True)\n",
        "    \n",
        "    full_dataset = load_dataset(str(DATASET_LOCAL_PATH))\n",
        "    train_data = full_dataset.get(\"train\", [])\n",
        "    \n",
        "    fold_splits = create_kfold_splits(\n",
        "        dataset=train_data,\n",
        "        k=n_splits,\n",
        "        random_seed=random_seed,\n",
        "        shuffle=shuffle,\n",
        "    )\n",
        "    \n",
        "    fold_splits_file = HPO_OUTPUT_DIR / \"fold_splits.json\"\n",
        "    save_fold_splits(\n",
        "        fold_splits,\n",
        "        fold_splits_file,\n",
        "        metadata={\n",
        "            \"k\": n_splits,\n",
        "            \"random_seed\": random_seed,\n",
        "            \"shuffle\": shuffle,\n",
        "            \"dataset_path\": str(DATASET_LOCAL_PATH),\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mlflow_experiment_name(experiment_name: str, stage: str, backbone: str) -> str:\n",
        "    return f\"{experiment_name}-{stage}-{backbone}\"\n",
        "\n",
        "hpo_studies = {}\n",
        "k_folds_param = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS) if k_folds_enabled else None\n",
        "\n",
        "for backbone in backbone_values:\n",
        "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
        "        experiment_config.name, STAGE_HPO, backbone\n",
        "    )\n",
        "    backbone_output_dir = HPO_OUTPUT_DIR / backbone\n",
        "    \n",
        "    study = run_local_hpo_sweep(\n",
        "        dataset_path=str(DATASET_LOCAL_PATH),\n",
        "        config_dir=CONFIG_DIR,\n",
        "        backbone=backbone,\n",
        "        hpo_config=hpo_config,\n",
        "        train_config=train_config,\n",
        "        output_dir=backbone_output_dir,\n",
        "        mlflow_experiment_name=mlflow_experiment_name,\n",
        "        k_folds=k_folds_param,\n",
        "        fold_splits_file=fold_splits_file,\n",
        "    )\n",
        "    \n",
        "    hpo_studies[backbone] = study\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_cv_statistics(best_trial):\n",
        "    if not hasattr(best_trial, \"user_attrs\"):\n",
        "        return None\n",
        "    cv_mean = best_trial.user_attrs.get(\"cv_mean\")\n",
        "    cv_std = best_trial.user_attrs.get(\"cv_std\")\n",
        "    return (cv_mean, cv_std) if cv_mean is not None else None\n",
        "\n",
        "objective_metric = hpo_config['objective']['metric']\n",
        "\n",
        "for backbone, study in hpo_studies.items():\n",
        "    if not study.trials:\n",
        "        continue\n",
        "    \n",
        "    best_trial = study.best_trial\n",
        "    cv_stats = extract_cv_statistics(best_trial)\n",
        "    \n",
        "    print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
        "    print(f\"  Best {objective_metric}: {best_trial.value:.4f}\")\n",
        "    print(f\"  Best params: {best_trial.params}\")\n",
        "    \n",
        "    if cv_stats:\n",
        "        cv_mean, cv_std = cv_stats\n",
        "        print(f\"  CV Statistics: Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.6: Best Configuration Selection (Automated)\n",
        "\n",
        "Programmatically select the best configuration from all HPO sweep runs across all backbone models. The best configuration is determined by the objective metric specified in the HPO config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib.util\n",
        "from shared.json_cache import save_json\n",
        "\n",
        "# Import local_selection directly to avoid triggering Azure ML imports in __init__.py\n",
        "local_selection_spec = importlib.util.spec_from_file_location(\n",
        "    \"local_selection\", SRC_DIR / \"orchestration\" / \"jobs\" / \"local_selection.py\"\n",
        ")\n",
        "local_selection = importlib.util.module_from_spec(local_selection_spec)\n",
        "local_selection_spec.loader.exec_module(local_selection)\n",
        "select_best_configuration_across_studies = local_selection.select_best_configuration_across_studies\n",
        "\n",
        "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_version = data_config.get(\"version\", \"unknown\")\n",
        "\n",
        "best_configuration = select_best_configuration_across_studies(\n",
        "    studies=hpo_studies,\n",
        "    hpo_config=hpo_config,\n",
        "    dataset_version=dataset_version,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_json(BEST_CONFIG_CACHE_FILE, best_configuration)\n",
        "\n",
        "print(f\"Best configuration selected:\")\n",
        "print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
        "print(f\"  Trial: {best_configuration.get('trial_name')}\")\n",
        "print(f\"  Best {hpo_config['objective']['metric']}: {best_configuration.get('selection_criteria', {}).get('best_value'):.4f}\")\n",
        "print(f\"  Hyperparameters: {best_configuration.get('hyperparameters')}\")\n",
        "print(f\"\\nSaved to: {BEST_CONFIG_CACHE_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
        "\n",
        "Train the final production model using the best configuration from HPO with stable, controlled conditions. This uses the full training epochs (no early stopping) and the best hyperparameters found during HPO.\n",
        "\n",
        "**Note**: After training completes, the checkpoint will be automatically backed up to Google Drive for persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import mlflow\n",
        "from shared.json_cache import load_json, save_json\n",
        "from orchestration import STAGE_TRAINING\n",
        "\n",
        "# Define build_final_training_config locally to avoid importing Azure ML dependencies\n",
        "# This function doesn't use Azure ML, so we can define it here\n",
        "def build_final_training_config(\n",
        "    best_config: dict,\n",
        "    train_config: dict,\n",
        "    random_seed: int = 42,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Build final training configuration by merging best HPO config with train.yaml defaults.\n",
        "    \"\"\"\n",
        "    hyperparameters = best_config.get(\"hyperparameters\", {})\n",
        "    training_defaults = train_config.get(\"training\", {})\n",
        "    \n",
        "    return {\n",
        "        \"backbone\": best_config[\"backbone\"],\n",
        "        \"learning_rate\": hyperparameters.get(\"learning_rate\", training_defaults.get(\"learning_rate\", 2e-5)),\n",
        "        \"dropout\": hyperparameters.get(\"dropout\", training_defaults.get(\"dropout\", 0.1)),\n",
        "        \"weight_decay\": hyperparameters.get(\"weight_decay\", training_defaults.get(\"weight_decay\", 0.01)),\n",
        "        \"batch_size\": training_defaults.get(\"batch_size\", 16),\n",
        "        \"epochs\": training_defaults.get(\"epochs\", 5),\n",
        "        \"random_seed\": random_seed,\n",
        "        \"early_stopping_enabled\": False,\n",
        "        \"use_combined_data\": True,\n",
        "        \"use_all_data\": True,\n",
        "    }\n",
        "\n",
        "DEFAULT_RANDOM_SEED = 42\n",
        "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
        "FINAL_TRAINING_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"final_training\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_configuration = load_json(BEST_CONFIG_CACHE_FILE, default=None)\n",
        "\n",
        "if best_configuration is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Best configuration cache not found: {BEST_CONFIG_CACHE_FILE}\\n\"\n",
        "        f\"Please run Step P1-3.6: Best Configuration Selection first.\"\n",
        "    )\n",
        "\n",
        "final_training_config = build_final_training_config(\n",
        "    best_config=best_configuration,\n",
        "    train_config=configs[\"train\"],\n",
        "    random_seed=DEFAULT_RANDOM_SEED,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
        "final_output_dir = FINAL_TRAINING_OUTPUT_DIR / final_training_config['backbone']\n",
        "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "mlflow.set_experiment(mlflow_experiment_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_script_path = SRC_DIR / \"train.py\"\n",
        "training_args = [\n",
        "    sys.executable,\n",
        "    str(training_script_path),\n",
        "    \"--data-asset\",\n",
        "    str(DATASET_LOCAL_PATH),\n",
        "    \"--config-dir\",\n",
        "    str(CONFIG_DIR),\n",
        "    \"--backbone\",\n",
        "    final_training_config[\"backbone\"],\n",
        "    \"--learning-rate\",\n",
        "    str(final_training_config[\"learning_rate\"]),\n",
        "    \"--batch-size\",\n",
        "    str(final_training_config[\"batch_size\"]),\n",
        "    \"--dropout\",\n",
        "    str(final_training_config[\"dropout\"]),\n",
        "    \"--weight-decay\",\n",
        "    str(final_training_config[\"weight_decay\"]),\n",
        "    \"--epochs\",\n",
        "    str(final_training_config[\"epochs\"]),\n",
        "    \"--random-seed\",\n",
        "    str(final_training_config[\"random_seed\"]),\n",
        "    \"--early-stopping-enabled\",\n",
        "    str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
        "    \"--use-combined-data\",\n",
        "    str(final_training_config[\"use_combined_data\"]).lower(),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_env = os.environ.copy()\n",
        "training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
        "\n",
        "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
        "if mlflow_tracking_uri:\n",
        "    training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
        "training_env[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = subprocess.run(\n",
        "    training_args,\n",
        "    cwd=ROOT_DIR,\n",
        "    env=training_env,\n",
        "    capture_output=False,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"Final training failed with return code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "METRICS_FILENAME = \"metrics.json\"\n",
        "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
        "\n",
        "# Check actual checkpoint location\n",
        "# The training script may save to outputs/checkpoint instead of final_output_dir/checkpoint\n",
        "actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
        "actual_metrics = ROOT_DIR / \"outputs\" / METRICS_FILENAME\n",
        "expected_checkpoint = final_output_dir / \"checkpoint\"\n",
        "expected_metrics = final_output_dir / METRICS_FILENAME\n",
        "\n",
        "print(\"Checking training completion...\")\n",
        "print(f\"  Expected checkpoint: {expected_checkpoint} (exists: {expected_checkpoint.exists()})\")\n",
        "print(f\"  Actual checkpoint: {actual_checkpoint} (exists: {actual_checkpoint.exists()})\")\n",
        "print(f\"  Expected metrics: {expected_metrics} (exists: {expected_metrics.exists()})\")\n",
        "print(f\"  Actual metrics: {actual_metrics} (exists: {actual_metrics.exists()})\")\n",
        "\n",
        "# Determine which checkpoint and metrics to use\n",
        "checkpoint_source = None\n",
        "metrics_file = None\n",
        "\n",
        "if expected_checkpoint.exists() and any(expected_checkpoint.iterdir()):\n",
        "    checkpoint_source = expected_checkpoint\n",
        "    print(f\"✓ Using expected checkpoint location: {checkpoint_source}\")\n",
        "elif actual_checkpoint.exists() and any(actual_checkpoint.iterdir()):\n",
        "    checkpoint_source = actual_checkpoint\n",
        "    print(f\"✓ Using actual checkpoint location: {checkpoint_source}\")\n",
        "    # Update final_output_dir to match actual location\n",
        "    final_output_dir = actual_checkpoint.parent\n",
        "\n",
        "if expected_metrics.exists():\n",
        "    metrics_file = expected_metrics\n",
        "elif actual_metrics.exists():\n",
        "    metrics_file = actual_metrics\n",
        "\n",
        "# Load metrics if available\n",
        "metrics = None\n",
        "if metrics_file and metrics_file.exists():\n",
        "    with open(metrics_file, \"r\") as f:\n",
        "        metrics = json.load(f)\n",
        "    print(f\"✓ Metrics loaded from: {metrics_file}\")\n",
        "    print(f\"  Metrics: {metrics}\")\n",
        "elif checkpoint_source:\n",
        "    print(f\"⚠ Warning: Metrics file not found, but checkpoint exists.\")\n",
        "    metrics = {\"status\": \"completed\", \"checkpoint_found\": True}\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Training completed but no checkpoint found.\\n\"\n",
        "        f\"  Expected: {expected_checkpoint}\\n\"\n",
        "        f\"  Actual: {actual_checkpoint}\\n\"\n",
        "        f\"  Please check training logs for errors.\"\n",
        "    )\n",
        "\n",
        "# Save cache file with actual paths\n",
        "save_json(FINAL_TRAINING_CACHE_FILE, {\n",
        "    \"output_dir\": str(final_output_dir),\n",
        "    \"backbone\": final_training_config[\"backbone\"],\n",
        "    \"config\": final_training_config,\n",
        "})\n",
        "\n",
        "# Backup checkpoint to Google Drive\n",
        "if checkpoint_source and checkpoint_source.exists() and any(checkpoint_source.iterdir()):\n",
        "    checkpoint_backup = DRIVE_BACKUP_DIR / f\"{final_training_config['backbone']}_checkpoint\"\n",
        "    \n",
        "    # Remove existing backup if it exists\n",
        "    if checkpoint_backup.exists():\n",
        "        shutil.rmtree(checkpoint_backup)\n",
        "    \n",
        "    # Copy checkpoint to Drive\n",
        "    shutil.copytree(checkpoint_source, checkpoint_backup)\n",
        "    print(f\"\\n✓ Checkpoint backed up to Google Drive: {checkpoint_backup}\")\n",
        "    \n",
        "    # Backup cache file to Drive\n",
        "    cache_backup = DRIVE_BACKUP_DIR / \"final_training_cache.json\"\n",
        "    shutil.copy2(FINAL_TRAINING_CACHE_FILE, cache_backup)\n",
        "    print(f\"✓ Cache file backed up to Google Drive: {cache_backup}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Checkpoint directory not found or empty.\\n\"\n",
        "        f\"  Expected: {expected_checkpoint}\\n\"\n",
        "        f\"  Actual: {actual_checkpoint}\\n\"\n",
        "        f\"Training may have failed. Please check the training output above.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-4: Model Conversion & Optimization\n",
        "\n",
        "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
        "\n",
        "**Platform Adapter Note**: The conversion script (`src/convert_to_onnx.py`) uses the platform adapter to automatically handle output paths and logging appropriately for local execution.\n",
        "\n",
        "**Checkpoint Restoration**: If the checkpoint is not found locally (e.g., after a Colab session disconnect), it will be automatically restored from Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import mlflow\n",
        "import shutil\n",
        "from shared.json_cache import load_json\n",
        "\n",
        "CONVERSION_SCRIPT_PATH = SRC_DIR / \"convert_to_onnx.py\"\n",
        "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
        "CONVERSION_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"conversion\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_cache = load_json(FINAL_TRAINING_CACHE_FILE, default=None)\n",
        "\n",
        "if training_cache is None:\n",
        "    # Try to restore from Google Drive\n",
        "    cache_backup = DRIVE_BACKUP_DIR / \"final_training_cache.json\"\n",
        "    if cache_backup.exists():\n",
        "        print(f\"Restoring training cache from Google Drive...\")\n",
        "        shutil.copy2(cache_backup, FINAL_TRAINING_CACHE_FILE)\n",
        "        training_cache = load_json(FINAL_TRAINING_CACHE_FILE, default=None)\n",
        "        print(f\"✓ Training cache restored from Google Drive\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Final training cache not found locally or in Google Drive.\\n\"\n",
        "            f\"Please run Step P1-3.7: Final Training first.\"\n",
        "        )\n",
        "\n",
        "# Try to find checkpoint in expected location or actual location\n",
        "backbone = training_cache[\"backbone\"]\n",
        "expected_checkpoint_dir = Path(training_cache[\"output_dir\"]) / \"checkpoint\"\n",
        "actual_checkpoint_dir = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
        "\n",
        "print(f\"Looking for checkpoint...\")\n",
        "print(f\"  Expected: {expected_checkpoint_dir} (exists: {expected_checkpoint_dir.exists()})\")\n",
        "print(f\"  Actual: {actual_checkpoint_dir} (exists: {actual_checkpoint_dir.exists()})\")\n",
        "\n",
        "# Determine which checkpoint to use\n",
        "checkpoint_dir = None\n",
        "if expected_checkpoint_dir.exists() and any(expected_checkpoint_dir.iterdir()):\n",
        "    checkpoint_dir = expected_checkpoint_dir\n",
        "    print(f\"✓ Using expected checkpoint location: {checkpoint_dir}\")\n",
        "elif actual_checkpoint_dir.exists() and any(actual_checkpoint_dir.iterdir()):\n",
        "    checkpoint_dir = actual_checkpoint_dir\n",
        "    print(f\"✓ Using actual checkpoint location: {checkpoint_dir}\")\n",
        "else:\n",
        "    # Try to restore from Google Drive\n",
        "    checkpoint_backup = DRIVE_BACKUP_DIR / f\"{backbone}_checkpoint\"\n",
        "    \n",
        "    if checkpoint_backup.exists():\n",
        "        print(f\"Checkpoint not found locally. Restoring from Google Drive...\")\n",
        "        print(f\"  From: {checkpoint_backup}\")\n",
        "        # Use expected location for restoration\n",
        "        checkpoint_dir = expected_checkpoint_dir\n",
        "        print(f\"  To: {checkpoint_dir}\")\n",
        "        \n",
        "        # Create parent directory if needed\n",
        "        checkpoint_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Copy checkpoint from Drive\n",
        "        shutil.copytree(checkpoint_backup, checkpoint_dir)\n",
        "        print(f\"✓ Checkpoint restored from Google Drive: {checkpoint_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Checkpoint directory not found locally or in Google Drive.\\n\"\n",
        "            f\"  Expected: {expected_checkpoint_dir}\\n\"\n",
        "            f\"  Actual: {actual_checkpoint_dir}\\n\"\n",
        "            f\"  Drive backup: {checkpoint_backup}\\n\"\n",
        "            f\"Please ensure training completed successfully and checkpoint was backed up.\"\n",
        "        )\n",
        "\n",
        "conversion_output_dir = CONVERSION_OUTPUT_DIR / backbone\n",
        "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_args = [\n",
        "    sys.executable,\n",
        "    str(CONVERSION_SCRIPT_PATH),\n",
        "    \"--checkpoint-path\",\n",
        "    str(checkpoint_dir),\n",
        "    \"--config-dir\",\n",
        "    str(CONFIG_DIR),\n",
        "    \"--backbone\",\n",
        "    backbone,\n",
        "    \"--output-dir\",\n",
        "    str(conversion_output_dir),\n",
        "    \"--quantize-int8\",\n",
        "    \"--run-smoke-test\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_env = os.environ.copy()\n",
        "conversion_env[\"AZURE_ML_OUTPUT_onnx_model\"] = str(conversion_output_dir)\n",
        "\n",
        "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
        "if mlflow_tracking_uri:\n",
        "    conversion_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = subprocess.run(\n",
        "    conversion_args,\n",
        "    cwd=ROOT_DIR,\n",
        "    env=conversion_env,\n",
        "    capture_output=False,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"Model conversion failed with return code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared.json_cache import save_json\n",
        "import shutil\n",
        "\n",
        "ONNX_MODEL_FILENAME = \"model_int8.onnx\"\n",
        "FALLBACK_ONNX_MODEL_FILENAME = \"model.onnx\"\n",
        "CONVERSION_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"conversion_cache.json\"\n",
        "\n",
        "onnx_model_path = conversion_output_dir / ONNX_MODEL_FILENAME\n",
        "if not onnx_model_path.exists():\n",
        "    onnx_model_path = conversion_output_dir / FALLBACK_ONNX_MODEL_FILENAME\n",
        "\n",
        "if not onnx_model_path.exists():\n",
        "    raise FileNotFoundError(f\"ONNX model not found in {conversion_output_dir}\")\n",
        "\n",
        "print(f\"✓ Conversion completed. ONNX model: {onnx_model_path}\")\n",
        "\n",
        "save_json(CONVERSION_CACHE_FILE, {\n",
        "    \"onnx_model_path\": str(onnx_model_path),\n",
        "    \"backbone\": backbone,\n",
        "    \"checkpoint_dir\": str(checkpoint_dir),\n",
        "})\n",
        "\n",
        "# Backup ONNX model to Google Drive\n",
        "onnx_backup = DRIVE_BACKUP_DIR / f\"{backbone}_model.onnx\"\n",
        "if onnx_model_path.exists():\n",
        "    shutil.copy2(onnx_model_path, onnx_backup)\n",
        "    print(f\"✓ ONNX model backed up to Google Drive: {onnx_backup}\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: ONNX model not found for backup: {onnx_model_path}\")\n",
        "\n",
        "# Backup conversion cache file to Drive\n",
        "conversion_cache_backup = DRIVE_BACKUP_DIR / \"conversion_cache.json\"\n",
        "shutil.copy2(CONVERSION_CACHE_FILE, conversion_cache_backup)\n",
        "print(f\"✓ Conversion cache backed up to Google Drive: {conversion_cache_backup}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
