{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration (Google Colab & Kaggle)\n",
    "\n",
    "This notebook orchestrates all training activities for **Google Colab or Kaggle execution** with GPU compute support.\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **executes training in Google Colab or Kaggle** (not on Azure ML)\n",
    "- All computation happens on the platform's GPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Will use current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None  # Will use Path.cwd() instead\n",
    "    BACKUP_ENABLED = False\n",
    "\n",
    "print(f\"✓ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "if BASE_DIR:\n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "else:\n",
    "    print(f\"Base directory: Will use current working directory\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n",
    "\n",
    "### For Colab/Kaggle: Clone from Git or Upload Files\n",
    "\n",
    "Choose one of the following options:\n",
    "\n",
    "**Option A: Clone from Git (Recommended)**\n",
    "\n",
    "If your repository is on GitHub/GitLab, clone it:\n",
    "\n",
    "**For Google Colab:**\n",
    "```python\n",
    "!git clone -b gg https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**For Kaggle:**\n",
    "```python\n",
    "!git clone -b gg https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**Option B: Upload Files**\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Use the Colab file browser (folder icon on left sidebar)\n",
    "2. Upload your project files to `/content/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "**For Kaggle:**\n",
    "1. Use the Kaggle file browser (Data tab)\n",
    "2. Upload your project files to `/kaggle/working/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "### For Local: Repository Already Exists\n",
    "\n",
    "Local environments should have the repository already cloned. The notebook will automatically detect the repository location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected - assuming repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b gg https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b gg https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"✓ Local environment detected - assuming repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Repository Setup\n",
    "\n",
    "Verify the repository structure exists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repository found at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "✓ Required directories found: ['src', 'config', 'notebooks']\n",
      "Notebook directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\n",
      "Project root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Unified path setup for all environments\n",
    "if IS_LOCAL:\n",
    "    # Local: assume notebook is in notebooks/ directory\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    # Colab/Kaggle: use fixed paths\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Verify repository structure\n",
    "if not ROOT_DIR.exists():\n",
    "    if IS_LOCAL:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please ensure you're running this notebook from the notebooks/ directory of the repository.\"\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please run Step 2 to clone or upload the repository.\"\n",
    "        )\n",
    "\n",
    "required_dirs = [\"src\", \"config\", \"notebooks\"]\n",
    "missing_dirs = [d for d in required_dirs if not (ROOT_DIR / d).exists()]\n",
    "\n",
    "if missing_dirs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required directories: {missing_dirs}\\n\"\n",
    "        f\"Please ensure the repository structure is correct.\"\n",
    "    )\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"✓ Repository found at: {ROOT_DIR}\")\n",
    "print(f\"✓ Required directories found: {required_dirs}\")\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "print(f\"Source directory: {SRC_DIR}\")\n",
    "print(f\"Config directory: {CONFIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "**For Local**: Use conda environment (instructions below).  \n",
    "**For Colab/Kaggle**: Install packages via pip (automated below).\n",
    "\n",
    "### Local Environment Setup\n",
    "\n",
    "For local execution, create and activate a conda environment:\n",
    "\n",
    "1. Open a terminal in the project root\n",
    "2. Create the conda environment: `conda env create -f config/environment/conda.yaml`\n",
    "3. Activate: `conda activate resume-ner-training`\n",
    "4. Restart the kernel after activation\n",
    "\n",
    "### Colab/Kaggle: Automated Installation\n",
    "\n",
    "PyTorch is usually pre-installed in Colab/Kaggle, but we'll verify and install other required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: True\n",
      "Visible GPUs: 1\n",
      "  GPU 0: Quadro T1000\n",
      "✓ PyTorch version meets requirements\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version and GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Visible GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Verify PyTorch version meets requirements (>=2.6.0)\n",
    "torch_version = tuple(map(int, torch.__version__.split('.')[:2]))\n",
    "if torch_version < (2, 6):\n",
    "    print(f\"⚠ Warning: PyTorch {torch.__version__} may not meet requirements (>=2.6.0)\")\n",
    "    if not IS_LOCAL:\n",
    "        print(\"Consider upgrading: !pip install torch>=2.6.0 --upgrade\")\n",
    "else:\n",
    "    print(\"✓ PyTorch version meets requirements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install azure-ai-ml>=1.0.0 --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install transformers>=4.35.0,<5.0.0 --quiet\n",
    "    %pip install safetensors>=0.4.0 --quiet\n",
    "    %pip install datasets>=2.12.0 --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install numpy>=1.24.0,<2.0.0 --quiet\n",
    "    %pip install pandas>=2.0.0 --quiet\n",
    "    %pip install scikit-learn>=1.3.0 --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install pyyaml>=6.0 --quiet\n",
    "    %pip install tqdm>=4.65.0 --quiet\n",
    "    %pip install seqeval>=1.2.2 --quiet\n",
    "    %pip install sentencepiece>=0.1.99 --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install azure-ai-ml>=1.0.0 --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install onnx>=1.16.0 --quiet\n",
    "    %pip install onnxscript>=0.1.0 --quiet\n",
    "\n",
    "    print(\"✓ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Paths and Import Paths\n",
    "\n",
    "Python paths are already configured in Step 2. This section verifies the setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: local\n",
      "Base directory: Will use current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Environment detection and platform configuration\n",
    "# Note: This cell is a duplicate of Cell 2. If Cell 2 was already executed, these variables are already set.\n",
    "# This cell ensures they're set even if Cell 2 was skipped.\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants (only if not already set)\n",
    "if 'PLATFORM' not in globals():\n",
    "    if IN_COLAB:\n",
    "        PLATFORM = \"colab\"\n",
    "        BASE_DIR = Path(\"/content\")\n",
    "        BACKUP_ENABLED = True\n",
    "        print(\"✓ Detected: Google Colab environment\")\n",
    "    elif IN_KAGGLE:\n",
    "        PLATFORM = \"kaggle\"\n",
    "        BASE_DIR = Path(\"/kaggle/working\")\n",
    "        BACKUP_ENABLED = False  # Kaggle outputs are automatically persisted\n",
    "        print(\"✓ Detected: Kaggle environment\")\n",
    "    else:\n",
    "        PLATFORM = \"local\"\n",
    "        BASE_DIR = None  # Will use Path.cwd() instead\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"✓ Detected: Local environment\")\n",
    "\n",
    "if 'PLATFORM' in globals():\n",
    "    print(f\"Platform: {PLATFORM}\")\n",
    "    if BASE_DIR:\n",
    "        print(f\"Base directory: {BASE_DIR}\")\n",
    "    else:\n",
    "        print(f\"Base directory: Will use current working directory\")\n",
    "    print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\n",
      "Project root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "Platform: local\n",
      "In Colab: False\n",
      "In Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths (ROOT_DIR should be set in Cell 2)\n",
    "# If not, set it here\n",
    "if 'ROOT_DIR' not in globals():\n",
    "    if IN_COLAB:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")\n",
    "    elif IN_KAGGLE:\n",
    "        ROOT_DIR = Path(\"/kaggle/working/resume-ner-azureml\")\n",
    "    else:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")  # Default to Colab path\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
    "print(\"Project root:\", ROOT_DIR)\n",
    "print(\"Source directory:\", SRC_DIR)\n",
    "print(\"Config directory:\", CONFIG_DIR)\n",
    "print(\"Platform:\", PLATFORM if 'PLATFORM' in globals() else \"unknown\")\n",
    "print(\"In Colab:\", IN_COLAB if 'IN_COLAB' in globals() else False)\n",
    "print(\"In Kaggle:\", IN_KAGGLE if 'IN_KAGGLE' in globals() else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to enable checkpoint persistence across Colab sessions. Checkpoints will be automatically saved to Drive after training completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Backup/restore wrapper functions defined (using DriveBackupStore)\n"
     ]
    }
   ],
   "source": [
    "# Google Drive backup/restore functionality\n",
    "# Uses the DriveBackupStore from orchestration.drive_backup module\n",
    "# The drive_store is created in Cell 15 (after mounting)\n",
    "\n",
    "# Backward-compatible wrapper functions (delegate to drive_store)\n",
    "# These maintain the old API for gradual migration\n",
    "from pathlib import Path\n",
    "\n",
    "# Note: drive_store is created in Cell 15 (Mount Google Drive)\n",
    "# If drive_store is None, backup/restore operations are disabled\n",
    "\n",
    "def backup_to_drive(source_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for drive_store.backup().\n",
    "    \n",
    "    Note: Prefer using drive_store.backup() directly for better error handling.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"⚠ Warning: Source path does not exist: {source_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.backup(source_path, expect=expect)\n",
    "    \n",
    "    if result.ok:\n",
    "        print(result)\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Backup failed: {result.reason}\")\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for drive_store.restore().\n",
    "    \n",
    "    Note: Prefer using drive_store.restore() directly for better error handling.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.restore(local_path, expect=expect)\n",
    "    \n",
    "    if result.ok:\n",
    "        print(result)\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Restore failed: {result.reason}\")\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "def check_drive_backup_exists(local_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for drive_store.backup_exists().\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    return drive_store.backup_exists(local_path)\n",
    "\n",
    "def restore_if_missing(local_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Restore from Drive only if local file/directory is missing.\n",
    "    \"\"\"\n",
    "    if local_path.exists():\n",
    "        return False\n",
    "    \n",
    "    return restore_from_drive(local_path, is_directory=is_directory)\n",
    "\n",
    "def ensure_restored_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Ensure file/directory exists locally, restoring from Drive if missing.\n",
    "    \n",
    "    This is the primary entry point for most use cases.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.ensure_local(local_path)\n",
    "    \n",
    "    if result.ok and result.action.value == \"copied\":\n",
    "        print(result)\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "print(\"✓ Backup/restore wrapper functions defined (using DriveBackupStore)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from orchestration.drive_backup import create_colab_store\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "# Uses centralized config from config/paths.yaml\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        print(f\"✓ Google Drive mounted\")\n",
    "        print(f\"✓ Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"⚠ Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"✓ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    print(\"⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n",
    "\n",
    "**Note**: \n",
    "- **Local**: Config files should already exist in the repository\n",
    "- **Colab/Kaggle**: Config files will be auto-created if missing (useful for fresh environments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Update repository from git (only for Colab/Kaggle if needed)\n",
    "# Uncomment and run if you need to pull latest changes\n",
    "# if not IS_LOCAL:\n",
    "#     !cd {ROOT_DIR} && git fetch origin gg\n",
    "#     !cd {ROOT_DIR} && git reset --hard origin/gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write config files only if they don't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming config files already exist in repository\")\n",
    "else:\n",
    "    # Create the experiment config directory if it doesn't exist\n",
    "    experiment_config_dir = CONFIG_DIR / \"experiment\"\n",
    "    experiment_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = experiment_config_dir / \"resume_ner_baseline.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "experiment_name: \"resume_ner_baseline\"\n",
    "\n",
    "# Relative to the top-level config directory\n",
    "data_config: \"data/resume_v1.yaml\"\n",
    "model_config: \"model/distilbert.yaml\"\n",
    "train_config: \"train.yaml\"\n",
    "hpo_config: \"hpo/prod.yaml\"      # default HPO config; stages can override if needed\n",
    "env_config: \"env/azure.yaml\"\n",
    "benchmark_config: \"benchmark.yaml\"\n",
    "\n",
    "# High-level orchestration design:\n",
    "# - Stages: smoke → hpo → training\n",
    "# - Smoke and HPO stage backbones are controlled by the HPO config file (search_space.backbone.values)\n",
    "# - Training stage can target specific backbones via stage config\n",
    "# - AML experiment names are per-stage, optionally per-backbone\n",
    "\n",
    "stages:\n",
    "  smoke:\n",
    "    # AML experiment base name for smoke tests\n",
    "    aml_experiment: \"resume-ner-smoke\"\n",
    "    # HPO config for smoke/dry run tests (uses smoke.yaml with reduced trials)\n",
    "    hpo_config: \"hpo/smoke.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  hpo:\n",
    "    # AML experiment base name for HPO sweeps\n",
    "    aml_experiment: \"resume-ner-hpo\"\n",
    "    # HPO config override for production HPO sweep (uses prod.yaml instead of default smoke.yaml)\n",
    "    hpo_config: \"hpo/prod.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  training:\n",
    "    # AML experiment base name for final single-run training\n",
    "    aml_experiment: \"resume-ner-train\"\n",
    "    # Final production backbone(s); typically one chosen after HPO\n",
    "    backbones:\n",
    "      - \"distilbert\"\n",
    "\n",
    "# Optional naming policy for how AML experiments are derived per backbone.\n",
    "# If true, the orchestrator should build experiment_name as:\n",
    "#   \"<aml_experiment>-<backbone>\"\n",
    "# otherwise it should use \"<aml_experiment>\" directly and rely on tags\n",
    "# (stage/backbone) for grouping in AML.\n",
    "naming:\n",
    "  include_backbone_in_experiment: true\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ Config file written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming HPO config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write HPO config file only if it doesn't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming HPO config files already exist in repository\")\n",
    "else:\n",
    "    # Create the HPO config directory if it doesn't exist\n",
    "    hpo_config_dir = CONFIG_DIR / \"hpo\"\n",
    "    hpo_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = hpo_config_dir / \"prod.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "search_space:\n",
    "  backbone:\n",
    "    type: \"choice\"\n",
    "    values: [\"distilroberta\"]\n",
    "  \n",
    "  learning_rate:\n",
    "    type: \"loguniform\"\n",
    "    min: 1e-5\n",
    "    max: 5e-5\n",
    "  \n",
    "  batch_size:\n",
    "    type: \"choice\"\n",
    "    values: [8, 16]\n",
    "  \n",
    "  dropout:\n",
    "    type: \"uniform\"\n",
    "    min: 0.1\n",
    "    max: 0.3\n",
    "  \n",
    "  weight_decay:\n",
    "    type: \"loguniform\"\n",
    "    min: 0.001\n",
    "    max: 0.1\n",
    "\n",
    "sampling:\n",
    "  algorithm: \"random\"\n",
    "  max_trials: 20\n",
    "  timeout_minutes: 960\n",
    "\n",
    "early_termination:\n",
    "  policy: \"bandit\"\n",
    "  evaluation_interval: 1\n",
    "  slack_factor: 0.2\n",
    "  delay_evaluation: 2\n",
    "\n",
    "objective:\n",
    "  metric: \"macro-f1\"\n",
    "  goal: \"maximize\"\n",
    "\n",
    "# Selection strategy configuration for accuracy-speed tradeoff\n",
    "selection:\n",
    "  # Accuracy threshold for speed tradeoff (0.015 = 1.5% relative)\n",
    "  # If two models are within this accuracy difference, prefer faster model\n",
    "  # Set to null for accuracy-only selection (default behavior)\n",
    "  accuracy_threshold: 0.015\n",
    "  \n",
    "  # Use relative threshold (percentage of best accuracy) vs absolute difference\n",
    "  # Relative thresholds are more robust across different accuracy ranges\n",
    "  # Default: true (recommended)\n",
    "  use_relative_threshold: true\n",
    "  \n",
    "  # Minimum relative accuracy gain to justify slower model (optional)\n",
    "  # If DeBERTa is < 2% better than DistilBERT, prefer DistilBERT\n",
    "  # Set to null to disable this check\n",
    "  min_accuracy_gain: 0.02\n",
    "\n",
    "k_fold:\n",
    "  enabled: true\n",
    "  n_splits: 5\n",
    "  random_seed: 42\n",
    "  shuffle: true\n",
    "  stratified: true\n",
    "\n",
    "# Checkpoint configuration for HPO resume support\n",
    "# Enables saving study state to SQLite database for resuming interrupted runs\n",
    "checkpoint:\n",
    "  enabled: true  # Set to true to enable checkpointing (useful for Colab/Kaggle)\n",
    "  storage_path: \"{backbone}/study.db\"  # Relative to output_dir, {backbone} placeholder\n",
    "  auto_resume: true  # Automatically resume if checkpoint exists (only if enabled=true)\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ HPO config written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ HPO config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming training config file already exists in repository\n"
     ]
    }
   ],
   "source": [
    "# Write training config file only if it doesn't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming training config file already exists in repository\")\n",
    "else:\n",
    "    # Ensure config directory exists\n",
    "    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = CONFIG_DIR / \"train.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "# Global Training Defaults\n",
    "# Applied to all training runs\n",
    "\n",
    "training:\n",
    "  epochs: 5\n",
    "  batch_size: 12 \n",
    "  gradient_accumulation_steps: 2\n",
    "  learning_rate: 2e-5\n",
    "  weight_decay: 0.01\n",
    "  warmup_steps: 500\n",
    "  max_grad_norm: 1.0\n",
    "  # Data splitting and model-specific settings\n",
    "  val_split_divisor: 10  # Divide train set by this to create validation split if none exists\n",
    "  deberta_max_batch_size: 16  # Maximum batch size for DeBERTa models (memory constraints)\n",
    "  warmup_steps_divisor: 10  # Divide total steps by this to cap warmup steps\n",
    "  \n",
    "  # EDA-based metric selection\n",
    "  metric: \"macro-f1\"  # Class imbalance requires macro-f1\n",
    "  metric_mode: \"max\"  # Maximize macro-f1\n",
    "  \n",
    "  early_stopping:\n",
    "    enabled: true\n",
    "    patience: 3\n",
    "    min_delta: 0.001\n",
    "\n",
    "logging:\n",
    "  log_interval: 100\n",
    "  eval_interval: 500\n",
    "  save_interval: 1000\n",
    "\n",
    "checkpointing:\n",
    "  save_strategy: \"steps\"\n",
    "  save_total_limit: 3\n",
    "  load_best_model_at_end: true\n",
    "\n",
    "# NOTE: Multi-GPU / DDP is optional and currently experimental. When enabled,\n",
    "# the training code will use this section together with hardware detection to\n",
    "# decide whether to run single-GPU vs multi-GPU. If no multiple GPUs or DDP\n",
    "# backend are available, it will safely fall back to single-GPU.\n",
    "distributed:\n",
    "  enabled: true         # Set true to enable multi-GPU / DDP\n",
    "  backend: \"nccl\"        # Typically 'nccl' for GPUs\n",
    "  world_size: \"auto\"     # 'auto' = use all visible GPUs; or set an int\n",
    "  init_method: \"env://\"  # Default init method; can be overridden if needed\n",
    "  timeout_seconds: 1800  # Process group init timeout (in seconds)\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ Training config written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Training config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "\n",
    "Define constants for file and directory names used throughout the notebook. Benchmark settings come from centralized config, not hard-coded here. These constants work across all environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import constants from centralized module\n",
    "from orchestration import (\n",
    "    STAGE_HPO,\n",
    "    STAGE_TRAINING,\n",
    "    METRICS_FILENAME,\n",
    "    BENCHMARK_FILENAME,\n",
    "    CHECKPOINT_DIRNAME,\n",
    "    DEFAULT_RANDOM_SEED,\n",
    "    DEFAULT_K_FOLDS,\n",
    ")\n",
    "\n",
    "from orchestration.jobs.tracking.mlflow_tracker import (\n",
    "    MLflowSweepTracker,\n",
    "    MLflowBenchmarkTracker,\n",
    "    MLflowTrainingTracker,\n",
    "    MLflowConversionTracker,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "\n",
    "Reusable helper functions following DRY principle for common operations. These functions work across all environments (local, Colab, Kaggle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from consolidated modules (DRY principle)\n",
    "from typing import List, Optional, Any\n",
    "from orchestration import (\n",
    "    build_mlflow_experiment_name,\n",
    "    setup_mlflow_for_stage,\n",
    "    run_benchmarking,\n",
    ")\n",
    "from shared import verify_output_file\n",
    "\n",
    "# Wrapper function for run_benchmarking that uses notebook-specific paths\n",
    "def run_benchmarking_local(\n",
    "    checkpoint_dir: Path,\n",
    "    test_data_path: Path,\n",
    "    output_path: Path,\n",
    "    batch_sizes: List[int],\n",
    "    iterations: int,\n",
    "    warmup_iterations: int,\n",
    "    max_length: int = 512,\n",
    "    device: Optional[str] = None,\n",
    "    tracker: Optional[Any] = None,\n",
    "    backbone: Optional[str] = None,\n",
    "    benchmark_source: str = \"final_training\",\n",
    "    study_key_hash: Optional[str] = None,\n",
    "    trial_key_hash: Optional[str] = None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Run benchmarking on a model checkpoint (local notebook wrapper).\n",
    "    \n",
    "    This is a thin wrapper around orchestration.benchmark_utils.run_benchmarking\n",
    "    that automatically uses the notebook's SRC_DIR and ROOT_DIR.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Path to checkpoint directory.\n",
    "        test_data_path: Path to test data JSON file.\n",
    "        output_path: Path to output benchmark.json file.\n",
    "        batch_sizes: List of batch sizes to test.\n",
    "        iterations: Number of iterations per batch size.\n",
    "        warmup_iterations: Number of warmup iterations.\n",
    "        max_length: Maximum sequence length.\n",
    "        device: Device to use (None = auto-detect).\n",
    "        tracker: Optional MLflowBenchmarkTracker instance.\n",
    "        backbone: Optional model backbone name.\n",
    "        benchmark_source: Source of benchmark (\"hpo_trial\" or \"final_training\").\n",
    "        study_key_hash: Optional study key hash for grouping tags.\n",
    "        trial_key_hash: Optional trial key hash for grouping tags.\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    return run_benchmarking(\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        test_data_path=test_data_path,\n",
    "        output_path=output_path,\n",
    "        batch_sizes=batch_sizes,\n",
    "        iterations=iterations,\n",
    "        warmup_iterations=warmup_iterations,\n",
    "        max_length=max_length,\n",
    "        device=device,\n",
    "        tracker=tracker,\n",
    "        backbone=backbone,\n",
    "        benchmark_source=benchmark_source,\n",
    "        project_root=ROOT_DIR,\n",
    "        study_key_hash=study_key_hash,\n",
    "        trial_key_hash=trial_key_hash,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment: resume_ner_baseline\n",
      "Loaded config domains: ['benchmark', 'data', 'env', 'hpo', 'model', 'train']\n",
      "Config hashes: {'data': 'e87b126b961fa20d', 'model': '5f90a66353401b44', 'train': '1f54e404fd78a76f', 'hpo': '1c47c1143764f7b2', 'env': '3e54b931c7640cf2', 'benchmark': '33da3b0fc59ff812'}\n",
      "Config metadata: {'data_config_hash': 'e87b126b961fa20d', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': '1f54e404fd78a76f', 'hpo_config_hash': '1c47c1143764f7b2', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v3', 'model_backbone': 'distilbert-base-uncased'}\n",
      "Dataset path (from data config): C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\n",
      "Using seed: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from orchestration.config_loader import (\n",
    "    ExperimentConfig,\n",
    "    compute_config_hashes,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "# P1-3.1: Load Centralized Configs (local-only)\n",
    "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
    "\n",
    "if not CONFIG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
    "\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n",
    "validate_config_immutability(configs, original_configs)\n",
    "\n",
    "print(f\"Loaded experiment: {experiment_config.name}\")\n",
    "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
    "print(\"Config hashes:\", config_hashes)\n",
    "print(\"Config metadata:\", config_metadata)\n",
    "\n",
    "# Get dataset path from data config (centralized configuration)\n",
    "# The local_path in the data config is relative to the config directory\n",
    "data_config = configs[\"data\"]\n",
    "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
    "\n",
    "# Check if seed-based dataset structure (for dataset_tiny with seed subdirectories)\n",
    "seed = data_config.get(\"seed\")\n",
    "if seed is not None and \"dataset_tiny\" in str(DATASET_LOCAL_PATH):\n",
    "    DATASET_LOCAL_PATH = DATASET_LOCAL_PATH / f\"seed{seed}\"\n",
    "\n",
    "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")\n",
    "if seed is not None:\n",
    "    print(f\"Using seed: {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Verify Local Dataset\n",
    "\n",
    "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset directory found: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\n",
      "  (from data config: resume-ner-data-tiny-short vv3)\n",
      "  ✓ train.json (28,721 bytes)\n",
      "  ⚠ validation.json not found (optional - training will proceed without validation set)\n"
     ]
    }
   ],
   "source": [
    "# P1-3.2: Verify Local Dataset\n",
    "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
    "# This ensures the dataset location is controlled by centralized configuration.\n",
    "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
    "\n",
    "REQUIRED_FILE = \"train.json\"\n",
    "OPTIONAL_FILE = \"validation.json\"\n",
    "\n",
    "if not DATASET_LOCAL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check required file\n",
    "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
    "if not train_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required dataset file not found: {train_file}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check optional file\n",
    "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
    "has_validation = val_file.exists()\n",
    "\n",
    "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
    "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
    "\n",
    "train_size = train_file.stat().st_size\n",
    "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
    "\n",
    "if has_validation:\n",
    "    val_size = val_file.stat().st_size\n",
    "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2.1: Optional Train/Test Split\n",
    "\n",
    "**Optional step**: Create a train/test split if `test.json` is missing. This is useful when you only have `train.json` and `validation.json` and want to create a separate test set.\n",
    "\n",
    "**⚠ WARNING**: This will overwrite `train.json` with the split version. Only enable if you want to create a permanent train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing test.json at C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\\test.json\n"
     ]
    }
   ],
   "source": [
    "# Optional: create train/test split if test.json is missing\n",
    "# WARNING: This will overwrite train.json with the split version\n",
    "# Only enable if you want to create a permanent train/test split\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "from training.data import split_train_test, save_split_files\n",
    "\n",
    "CREATE_TEST_SPLIT = False  # Set True to create test.json when absent (WARNING: overwrites train.json)\n",
    "\n",
    "train_file = DATASET_LOCAL_PATH / \"train.json\"\n",
    "val_file = DATASET_LOCAL_PATH / \"validation.json\"\n",
    "test_file = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if CREATE_TEST_SPLIT and not test_file.exists():\n",
    "    # Backup original train.json before overwriting\n",
    "    backup_file = DATASET_LOCAL_PATH / \"train.json.backup\"\n",
    "    if train_file.exists() and not backup_file.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(train_file, backup_file)\n",
    "        print(f\"⚠ Backed up original train.json to {backup_file}\")\n",
    "    \n",
    "    full_dataset = []\n",
    "    # Start with train data; optionally include validation to maximize coverage\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_dataset.extend(json.load(f))\n",
    "    if val_file.exists():\n",
    "        with open(val_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_dataset.extend(json.load(f))\n",
    "\n",
    "    split_cfg = configs.get(\"data\", {}).get(\"splitting\", {})\n",
    "    train_ratio = split_cfg.get(\"train_test_ratio\", 0.8)\n",
    "    stratified = split_cfg.get(\"stratified\", False)\n",
    "    random_seed = split_cfg.get(\"random_seed\", 42)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "\n",
    "    print(f\"Creating train/test split (train_ratio={train_ratio}, stratified={stratified})...\")\n",
    "    print(f\"⚠ WARNING: This will overwrite train.json with {int(len(full_dataset) * train_ratio)} samples\")\n",
    "    \n",
    "    new_train, new_test = split_train_test(\n",
    "        dataset=full_dataset,\n",
    "        train_ratio=train_ratio,\n",
    "        stratified=stratified,\n",
    "        random_seed=random_seed,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "\n",
    "    save_split_files(DATASET_LOCAL_PATH, new_train, new_test)\n",
    "    print(f\"✓ Wrote train.json ({len(new_train)}) and test.json ({len(new_test)})\")\n",
    "elif test_file.exists():\n",
    "    print(f\"✓ Found existing test.json at {test_file}\")\n",
    "else:\n",
    "    print(\"⚠ test.json not found. Set CREATE_TEST_SPLIT=True to generate a split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Setup Local Environment\n",
    "\n",
    "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\"\n",
    "\n",
    "env_config = configs[\"env\"]\n",
    "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
    "\n",
    "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"CUDA device requested but not available. \"\n",
    "        \"In Colab, ensure you've selected a GPU runtime: Runtime > Change runtime type > GPU\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:23:55,089 - shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-02 17:23:55,091 - shared.mlflow_setup - INFO - Environment variables not set, loading from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2026-01-02 17:23:55,092 - shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-02 17:23:55,093 - shared.mlflow_setup - INFO - Using DefaultAzureCredential (trying multiple auth methods)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: sqlite:///mlflow.db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-02 17:23:57,845 - shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: MLflow tracking URI not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Setup MLflow from config (automatically uses Azure ML if enabled in config/mlflow.yaml)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# To enable Azure ML Workspace tracking:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 1. Edit config/mlflow.yaml and set azure_ml.enabled: true\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 2. Set environment variables: AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43msetup_mlflow_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplaceholder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Will be set per HPO run\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG_DIR\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\\shared\\mlflow_setup.py:440\u001b[0m, in \u001b[0;36msetup_mlflow_from_config\u001b[1;34m(experiment_name, config_dir, fallback_to_local)\u001b[0m\n\u001b[0;32m    437\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzure ML disabled in config, using local tracking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# Setup MLflow with or without Azure ML\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msetup_mlflow_cross_platform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mml_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_to_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback_to_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\\shared\\mlflow_setup.py:104\u001b[0m, in \u001b[0;36msetup_mlflow_cross_platform\u001b[1;34m(experiment_name, ml_client, fallback_to_local)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ml_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         tracking_uri \u001b[38;5;241m=\u001b[39m \u001b[43m_get_azure_ml_tracking_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(tracking_uri)\n\u001b[0;32m    106\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment_name)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\\shared\\mlflow_setup.py:151\u001b[0m, in \u001b[0;36m_get_azure_ml_tracking_uri\u001b[1;34m(ml_client)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Get workspace tracking URI\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     workspace \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m workspace\u001b[38;5;241m.\u001b[39mmlflow_tracking_uri\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:288\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(ACTIVITY_SPAN):\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[0;32m    286\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[0;32m    287\u001b[0m         ):\n\u001b[1;32m--> 288\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\ai\\ml\\operations\\_workspace_operations.py:141\u001b[0m, in \u001b[0;36mWorkspaceOperations.get\u001b[1;34m(self, name, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;129m@monitor_with_activity\u001b[39m(ops_logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkspace.Get\u001b[39m\u001b[38;5;124m\"\u001b[39m, ActivityType\u001b[38;5;241m.\u001b[39mPUBLICAPI)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# pylint: disable=arguments-renamed\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Workspace]:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a Workspace by name.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    :param name: Name of the workspace.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m            :caption: Get the workspace with the given name.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget(workspace_name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\ai\\ml\\operations\\_workspace_operations_base.py:89\u001b[0m, in \u001b[0;36mWorkspaceOperationsBase.get\u001b[1;34m(self, workspace_name, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m workspace_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_workspace_name(workspace_name)\n\u001b[0;32m     88\u001b[0m resource_group \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource_group\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_group_name\n\u001b[1;32m---> 89\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m v2_service_context \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     92\u001b[0m v2_service_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscription_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription_id\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\ai\\ml\\_restclient\\v2024_10_01_preview\\operations\\_workspaces_operations.py:947\u001b[0m, in \u001b[0;36mWorkspacesOperations.get\u001b[1;34m(self, resource_group_name, workspace_name, **kwargs)\u001b[0m\n\u001b[0;32m    944\u001b[0m request \u001b[38;5;241m=\u001b[39m _convert_request(request)\n\u001b[0;32m    945\u001b[0m request\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mformat_url(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m--> 947\u001b[0m pipeline_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mrun(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    948\u001b[0m     request,\n\u001b[0;32m    949\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    951\u001b[0m )\n\u001b[0;32m    952\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:242\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[0;32m    241\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\mgmt\\core\\policies\\_base.py:95\u001b[0m, in \u001b[0;36mARMAutoResourceProviderRegistrationPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[0;32m     94\u001b[0m     http_request \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mhttp_request\n\u001b[1;32m---> 95\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mhttp_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m     97\u001b[0m         rp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_rp_not_registered_err(response)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_redirect.py:205\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    203\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[1;32m--> 205\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_retry.py:545\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[0;32m    544\u001b[0m request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 545\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[0;32m    547\u001b[0m     retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:159\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    :param request: The pipeline request object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:134\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.on_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_https(request)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_new_token:\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m bearer_token \u001b[38;5;241m=\u001b[39m cast(Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessToken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessTokenInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token)\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_headers(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders, bearer_token)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:110\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._request_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mscopes: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Request a new token from the credential.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    This will call the credential's appropriate method to get a token and store it in the policy.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    :param str scopes: The type of access needed.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_token(\u001b[38;5;241m*\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:101\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._get_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m             options[key] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(key)  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(SupportsTokenInfo, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential)\u001b[38;5;241m.\u001b[39mget_token_info(\u001b[38;5;241m*\u001b[39mscopes, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(TokenCredential, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential)\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_credentials\\default.py:225\u001b[0m, in \u001b[0;36mDefaultAzureCredential.get_token\u001b[1;34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[0;32m    224\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, claims\u001b[38;5;241m=\u001b[39mclaims, tenant_id\u001b[38;5;241m=\u001b[39mtenant_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    226\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_credentials\\chained.py:94\u001b[0m, in \u001b[0;36mChainedTokenCredential.get_token\u001b[1;34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m credential \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m         token \u001b[38;5;241m=\u001b[39m credential\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, claims\u001b[38;5;241m=\u001b[39mclaims, tenant_id\u001b[38;5;241m=\u001b[39mtenant_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m acquired a token from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, credential\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_successful_credential \u001b[38;5;241m=\u001b[39m credential\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_internal\\decorators.py:33\u001b[0m, in \u001b[0;36mlog_get_token.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         token \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m     35\u001b[0m             logging\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;28;01mif\u001b[39;00m within_credential_chain\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01melse\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m, qualified_name\n\u001b[0;32m     36\u001b[0m         )\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_credentials\\managed_identity.py:139\u001b[0m, in \u001b[0;36mManagedIdentityCredential.get_token\u001b[1;34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialUnavailableError(\n\u001b[0;32m    134\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo managed identity endpoint found. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Target Azure platform could not be determined from environment variables. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit https://aka.ms/azsdk/python/identity/managedidentitycredential/troubleshoot to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtroubleshoot this issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m     )\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, claims\u001b[38;5;241m=\u001b[39mclaims, tenant_id\u001b[38;5;241m=\u001b[39mtenant_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_internal\\get_token_mixin.py:93\u001b[0m, in \u001b[0;36mGetTokenMixin.get_token\u001b[1;34m(self, claims, tenant_id, enable_cae, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m---> 93\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_token(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;241m*\u001b[39mscopes, claims\u001b[38;5;241m=\u001b[39mclaims, tenant_id\u001b[38;5;241m=\u001b[39mtenant_id, enable_cae\u001b[38;5;241m=\u001b[39menable_cae, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_refresh(token):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_credentials\\imds.py:88\u001b[0m, in \u001b[0;36mImdsCredential._request_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m within_credential_chain\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint_available:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# If within a chain (e.g. DefaultAzureCredential), we do a quick check to see if the IMDS endpoint\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# is available to avoid hanging for a long time if the endpoint isn't available.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m CredentialUnavailableError:\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;66;03m# Response is not json, skip the IMDS credential\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\identity\\_internal\\managed_identity_client.py:131\u001b[0m, in \u001b[0;36mManagedIdentityClient.request_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m request_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m--> 131\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mrun(request, retry_on_methods\u001b[38;5;241m=\u001b[39m[request\u001b[38;5;241m.\u001b[39mmethod], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    132\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response, request_time)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:242\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[0;32m    241\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_retry.py:545\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[0;32m    544\u001b[0m request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 545\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[0;32m    547\u001b[0m     retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:130\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m cleanup_kwargs_for_transport(request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[0;32m    129\u001b[0m     request\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sender\u001b[38;5;241m.\u001b[39msend(request\u001b[38;5;241m.\u001b[39mhttp_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions),\n\u001b[0;32m    131\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[0;32m    132\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py:375\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[1;34m(self, request, proxies, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m         read_timeout \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mread_timeout)\n\u001b[0;32m    374\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m (connection_timeout, read_timeout)\n\u001b[1;32m--> 375\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    376\u001b[0m         request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    377\u001b[0m         request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    378\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    379\u001b[0m         data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    380\u001b[0m         files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    381\u001b[0m         verify\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection_verify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mverify),\n\u001b[0;32m    382\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    383\u001b[0m         cert\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection_cert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mcert),\n\u001b[0;32m    384\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    385\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    387\u001b[0m     )\n\u001b[0;32m    388\u001b[0m     response\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39menforce_content_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\opentelemetry\\instrumentation\\requests\\__init__.py:228\u001b[0m, in \u001b[0;36m_instrument.<locals>.instrumented_send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request\u001b[38;5;241m.\u001b[39mheaders\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_http_instrumentation_enabled():\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_send(\u001b[38;5;28mself\u001b[39m, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# See\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# https://github.com/open-telemetry/semantic-conventions/blob/main/docs/http/http-spans.md#http-client\u001b[39;00m\n\u001b[0;32m    232\u001b[0m method \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mmethod\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\opentelemetry\\instrumentation\\urllib3\\__init__.py:325\u001b[0m, in \u001b[0;36m_instrument.<locals>.instrumented_urlopen\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minstrumented_urlopen\u001b[39m(wrapped, instance, args, kwargs):\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_http_instrumentation_enabled():\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    327\u001b[0m     url \u001b[38;5;241m=\u001b[39m _get_url(instance, args, kwargs, url_filter)\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m excluded_urls \u001b[38;5;129;01mand\u001b[39;00m excluded_urls\u001b[38;5;241m.\u001b[39murl_disabled(url):\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\connection.py:500\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\connection.py:331\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\connection.py:204\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "# Check and install Azure ML packages if needed (for Azure ML tracking)\n",
    "try:\n",
    "    import azure.ai.ml\n",
    "    import azure.identity\n",
    "except ImportError:\n",
    "    print(\"Azure ML packages not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"azure-ai-ml>=1.0.0\", \"azure-identity>=1.12.0\", \"azureml-defaults\", \"azureml-mlflow\", \"--quiet\"])\n",
    "    print(\"[OK] Azure ML packages installed\")\n",
    "from shared.mlflow_setup import setup_mlflow_from_config\n",
    "\n",
    "# Get MLflow tracking URI for later use\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    print(f\"MLflow tracking URI: {mlflow_tracking_uri[:80]}...\")\n",
    "else:\n",
    "    print(\"Warning: MLflow tracking URI not set\")\n",
    "\n",
    "# Setup MLflow from config (automatically uses Azure ML if enabled in config/mlflow.yaml)\n",
    "# To enable Azure ML Workspace tracking:\n",
    "# 1. Edit config/mlflow.yaml and set azure_ml.enabled: true\n",
    "# 2. Set environment variables: AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP\n",
    "setup_mlflow_from_config(\n",
    "    experiment_name=\"placeholder\",  # Will be set per HPO run\n",
    "    config_dir=CONFIG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kaggle-specific package installation (not running on Kaggle)\n"
     ]
    }
   ],
   "source": [
    "# For Kaggle only - install specific package versions required for Optuna checkpointing\n",
    "if IN_KAGGLE:\n",
    "    %pip install \"SQLAlchemy<2.0.0\" \"alembic<1.13.0\" \"optuna<4.0.0\" --quiet\n",
    "else:\n",
    "    print(\"Skipping Kaggle-specific package installation (not running on Kaggle)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow\n",
    "    import transformers\n",
    "    import optuna\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Required package not installed: {e}\")\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"torch\": torch,\n",
    "    \"transformers\": transformers,\n",
    "    \"mlflow\": mlflow,\n",
    "    \"optuna\": optuna,\n",
    "}\n",
    "\n",
    "for name, module in REQUIRED_PACKAGES.items():\n",
    "    if not hasattr(module, \"__version__\"):\n",
    "        raise ImportError(\n",
    "            f\"Required package '{name}' is not properly installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Sweep (HPO) - Local with Optuna\n",
    "\n",
    "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n",
    "\n",
    "**Note on K-Fold Cross-Validation:**\n",
    "- When k-fold CV is enabled (`k_fold.enabled: true`), each trial trains **k models** (one per fold) and returns the **average metric** across folds\n",
    "- The number of **trials** is controlled by `sampling.max_trials` (e.g., 2 trials in smoke.yaml)\n",
    "- With k=5 folds and 2 trials: **2 trials × 5 folds = 10 model trainings total**\n",
    "- K-fold CV provides more robust hyperparameter evaluation but increases compute time (k× per trial)\n",
    "\n",
    "**Note on Checkpoint and Resume:**\n",
    "- When `checkpoint.enabled: true` is set in the HPO config, the system automatically saves the Optuna study state to a SQLite database\n",
    "- This allows interrupted HPO runs to be resumed from the last checkpoint\n",
    "- The checkpoint is automatically detected and loaded on the next run if `auto_resume: true` (default)\n",
    "- Platform-specific paths are handled automatically (local, Colab, Kaggle)\n",
    "- **Selective Checkpoint Saving**: When `checkpoint.save_only_best: true` is set, only best trial checkpoints are saved locally (reduces storage from ~30 GB to ~300 MB for 100 trials)\n",
    "- **MLflow Checkpoint Logging**: When `mlflow.log_best_checkpoint: true` is set, the best trial checkpoint is automatically logged to MLflow after HPO completes (artifact path: `best_trial_checkpoint`)\n",
    "- **Refit Training**: When `refit.enabled: true` is set (default), after HPO completes, the best trial is automatically retrained on the full training dataset. This produces a canonical checkpoint in `trial_<n>_<ts>/refit/checkpoint/` that is preferred over fold checkpoints for benchmarking and production use.\n",
    "- See `docs/HPO_CHECKPOINT_RESUME.md` for detailed documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from orchestration import STAGE_HPO\n",
    "from orchestration.jobs import run_local_hpo_sweep\n",
    "\n",
    "# Constants are imported from orchestration module\n",
    "HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "HPO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using stage-specific HPO config for hpo: hpo/smoke.yaml\n"
     ]
    }
   ],
   "source": [
    "# Use HPO config already loaded in configs (from Step P1-3.1)\n",
    "# Following DRY principle - don't reload configs that are already available\n",
    "# Check for stage-specific hpo_config override\n",
    "from orchestration.naming import get_stage_config\n",
    "from shared.yaml_utils import load_yaml\n",
    "\n",
    "hpo_stage_config = get_stage_config(experiment_config, STAGE_HPO)\n",
    "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
    "\n",
    "if hpo_config_override:\n",
    "    # Load stage-specific HPO config override\n",
    "    hpo_config_path = CONFIG_DIR / hpo_config_override\n",
    "    hpo_config = load_yaml(hpo_config_path)\n",
    "    print(f\"✓ Using stage-specific HPO config for hpo: {hpo_config_override}\")\n",
    "else:\n",
    "    # Use default HPO config from top-level experiment config\n",
    "    hpo_config = configs[\"hpo\"]\n",
    "    print(f\"✓ Using default HPO config: {experiment_config.hpo_config.name}\")\n",
    "train_config = configs[\"train\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup K-Fold Splits and Google Drive Backup for HPO Trials\n",
    "\n",
    "**K-Fold Cross-Validation Setup**: If k-fold CV is enabled in the HPO config, create and save fold splits before starting the sweep.\n",
    "\n",
    "**Colab-specific feature**: Configure automatic backup of each HPO trial to Google Drive immediately after completion. This prevents data loss if the Colab session disconnects during long-running hyperparameter optimization sweeps.\n",
    "\n",
    "**Note on Checkpoint Backup:**\n",
    "- If `checkpoint.save_only_best: true` is enabled, only best trial checkpoints are saved locally and backed up to Drive\n",
    "- Each trial's `metrics.json` is always saved and backed up\n",
    "- The best trial checkpoint is also automatically logged to MLflow (if `mlflow.log_best_checkpoint: true`)\n",
    "- This reduces storage usage while ensuring the best model is always available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 2-fold cross-validation splits...\n",
      "[CV] Fold 0: {'SKILL': 154} | Missing: ['EDUCATION', 'DESIGNATION', 'EXPERIENCE', 'NAME', 'EMAIL', 'PHONE', 'LOCATION']\n",
      "[CV] Fold 1: {'SKILL': 107, 'LOCATION': 4, 'DESIGNATION': 1, 'EXPERIENCE': 1, 'EDUCATION': 1} | Missing: ['NAME', 'EMAIL', 'PHONE']\n",
      "✓ K-fold splits saved to: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\hpo\\fold_splits.json\n"
     ]
    }
   ],
   "source": [
    "from training.cv_utils import create_kfold_splits, save_fold_splits, validate_splits\n",
    "from training.data import load_dataset\n",
    "\n",
    "# Setup k-fold splits if enabled\n",
    "k_fold_config = hpo_config.get(\"k_fold\", {})\n",
    "k_folds_enabled = k_fold_config.get(\"enabled\", False)\n",
    "fold_splits_file = None\n",
    "\n",
    "if k_folds_enabled:\n",
    "    n_splits = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS)\n",
    "    random_seed = k_fold_config.get(\"random_seed\", DEFAULT_RANDOM_SEED)\n",
    "    shuffle = k_fold_config.get(\"shuffle\", True)\n",
    "    stratified = k_fold_config.get(\"stratified\", False)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "    \n",
    "    print(f\"Setting up {n_splits}-fold cross-validation splits...\")\n",
    "    full_dataset = load_dataset(str(DATASET_LOCAL_PATH))\n",
    "    train_data = full_dataset.get(\"train\", [])\n",
    "    \n",
    "    fold_splits = create_kfold_splits(\n",
    "        dataset=train_data,\n",
    "        k=n_splits,\n",
    "        random_seed=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratified=stratified,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "    \n",
    "    # Optional validation to ensure rare entities appear across folds\n",
    "    validate_splits(train_data, fold_splits, entity_types=entity_types)\n",
    "    \n",
    "    HPO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    fold_splits_file = HPO_OUTPUT_DIR / \"fold_splits.json\"\n",
    "    save_fold_splits(\n",
    "        fold_splits,\n",
    "        fold_splits_file,\n",
    "        metadata={\n",
    "            \"k\": n_splits,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"shuffle\": shuffle,\n",
    "            \"stratified\": stratified,\n",
    "            \"dataset_path\": str(DATASET_LOCAL_PATH),\n",
    "        }\n",
    "    )\n",
    "    print(f\"✓ K-fold splits saved to: {fold_splits_file}\")\n",
    "else:\n",
    "    print(\"K-fold CV disabled - using single train/validation split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No manual backup callbacks are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No wrapper functions are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In a Kaggle notebook cell\n",
    "# !cd /kaggle/working/resume-ner-azureml && git fetch origin gg && git checkout origin/gg -- src/train.py src/training/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 14:24:07,609 - orchestration.jobs.hpo.local.study.manager - INFO - [HPO] Resuming optimization for distilbert from checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "✓ HPO output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\hpo\\local\\distilbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 14:24:08,022 - orchestration.jobs.hpo.local.study.manager - INFO - ✓ HPO already completed and checkpoint uploaded (best trial: 0, completed: 2026-01-02T13:58:25.628118). Skipping HPO execution.\n"
     ]
    }
   ],
   "source": [
    "# Extract checkpoint configuration from HPO config\n",
    "checkpoint_config = hpo_config.get(\"checkpoint\", {})\n",
    "\n",
    "hpo_studies = {}\n",
    "k_folds_param = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS) if k_folds_enabled else None\n",
    "\n",
    "# Use new centralized naming system for HPO\n",
    "# Build base output directory: outputs/hpo/<env>/<model>/\n",
    "# Trial-specific paths will be created by run_local_hpo_sweep as subdirectories\n",
    "\n",
    "\n",
    "# Ensure environment is defined\n",
    "from shared.platform_detection import detect_platform\n",
    "environment = detect_platform()\n",
    "print(f\"Detected environment: {environment}\")\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
    "        experiment_config.name, STAGE_HPO, backbone\n",
    "    )\n",
    "    \n",
    "    backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "    \n",
    "    # Build base HPO directory using new structure: outputs/hpo/<env>/<model>/\n",
    "    backbone_output_dir = ROOT_DIR / \"outputs\" / \"hpo\" / environment / backbone_name\n",
    "    backbone_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ HPO output directory: {backbone_output_dir}\")\n",
    "    \n",
    "    # Create restore function for HPO checkpoint if checkpointing enabled and BACKUP_ENABLED\n",
    "    restore_fn = None\n",
    "    if checkpoint_config.get(\"enabled\", False) and BACKUP_ENABLED:\n",
    "        storage_path_template = checkpoint_config.get(\"storage_path\", \"{backbone}/study.db\")\n",
    "        storage_path_str = storage_path_template.replace(\"{backbone}\", backbone)\n",
    "        expected_checkpoint = backbone_output_dir / storage_path_str\n",
    "        \n",
    "        def make_restore_fn(checkpoint_path):\n",
    "            def restore_fn_inner(path: Path) -> bool:\n",
    "                # Only restore if path matches expected checkpoint\n",
    "                if path == checkpoint_path:\n",
    "                    return ensure_restored_from_drive(checkpoint_path, is_directory=False)\n",
    "                return False\n",
    "            return restore_fn_inner\n",
    "        \n",
    "        restore_fn = make_restore_fn(expected_checkpoint)\n",
    "    \n",
    "    # Use standard run_local_hpo_sweep with checkpoint_config\n",
    "    # Checkpoint.enabled handles persistence via SQLite (better than manual Drive backup)\n",
    "    study = run_local_hpo_sweep(\n",
    "        dataset_path=str(DATASET_LOCAL_PATH),\n",
    "        config_dir=CONFIG_DIR,\n",
    "        backbone=backbone,\n",
    "        hpo_config=hpo_config,\n",
    "        train_config=train_config,\n",
    "        output_dir=backbone_output_dir,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        k_folds=k_folds_param,\n",
    "        fold_splits_file=fold_splits_file,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "        restore_from_drive=restore_fn,\n",
    "        data_config=configs.get(\"data\"),\n",
    "        benchmark_config=configs.get(\"benchmark\"),\n",
    "    )\n",
    "    \n",
    "    # Backup checkpoint to Drive after HPO completion\n",
    "    if checkpoint_config.get(\"enabled\", False) and BACKUP_ENABLED:\n",
    "        # Backup study.db file (checkpoint database)\n",
    "        storage_path_template = checkpoint_config.get(\"storage_path\", \"{backbone}/study.db\")\n",
    "        storage_path_str = storage_path_template.replace(\"{backbone}\", backbone)\n",
    "        checkpoint_path = backbone_output_dir / storage_path_str\n",
    "        if checkpoint_path.exists():\n",
    "            backup_to_drive(checkpoint_path, is_directory=False)\n",
    "            print(f\"✓ Backed up HPO checkpoint database to Drive: {checkpoint_path}\")\n",
    "        \n",
    "        # Backup all trial directories (new structure: outputs/hpo/{env}/{model}/trial_*/...)\n",
    "        # Find all directories that start with \"trial_\"\n",
    "        trial_dirs = [d for d in backbone_output_dir.iterdir() \n",
    "                     if d.is_dir() and d.name.startswith(\"trial_\")]\n",
    "        \n",
    "        if trial_dirs:\n",
    "            print(f\"Found {len(trial_dirs)} trial directory(ies) to backup...\")\n",
    "            for trial_dir in trial_dirs:\n",
    "                result = backup_to_drive(trial_dir, is_directory=True)\n",
    "                if result:\n",
    "                    print(f\"✓ Backed up trial directory to Drive: {trial_dir.name}\")\n",
    "                else:\n",
    "                    print(f\"⚠ Failed to backup trial directory: {trial_dir.name}\")\n",
    "        else:\n",
    "            print(\"No trial directories found to backup\")\n",
    "    \n",
    "    hpo_studies[backbone] = study\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert: 1 trials completed\n",
      "  Best macro-f1: 0.4851\n",
      "  Best params: {'learning_rate': 4.143783964517761e-05, 'batch_size': 4, 'dropout': 0.18800769496265501, 'weight_decay': 0.007134973962158688}\n",
      "  CV Statistics: Mean: 0.4851 ± 0.0333\n"
     ]
    }
   ],
   "source": [
    "def extract_cv_statistics(best_trial):\n",
    "    if not hasattr(best_trial, \"user_attrs\"):\n",
    "        return None\n",
    "    cv_mean = best_trial.user_attrs.get(\"cv_mean\")\n",
    "    cv_std = best_trial.user_attrs.get(\"cv_std\")\n",
    "    return (cv_mean, cv_std) if cv_mean is not None else None\n",
    "\n",
    "objective_metric = hpo_config['objective']['metric']\n",
    "\n",
    "for backbone, study in hpo_studies.items():\n",
    "    if not study.trials:\n",
    "        continue\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    cv_stats = extract_cv_statistics(best_trial)\n",
    "    \n",
    "    print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
    "    print(f\"  Best {objective_metric}: {best_trial.value:.4f}\")\n",
    "    print(f\"  Best params: {best_trial.params}\")\n",
    "    \n",
    "    if cv_stats:\n",
    "        cv_mean, cv_std = cv_stats\n",
    "        print(f\"  CV Statistics: Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: Benchmarking Best Trials\n",
    "\n",
    "Benchmark the best trial from each backbone to measure actual inference performance. This provides real latency data that replaces parameter-count proxies in model selection, enabling more accurate speed comparisons.\n",
    "\n",
    "**Workflow:**\n",
    "1. Identify best trial per backbone (from HPO results)\n",
    "2. Select checkpoint: prefers `refit/checkpoint/` (if refit training completed), otherwise uses best fold from `cv/foldN/checkpoint/`\n",
    "3. Run benchmarking on each best trial checkpoint\n",
    "4. Save benchmark results as `benchmark.json` in trial directories\n",
    "5. Model selection will automatically use this data when available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert: Best trial from current HPO run is trial_0_20260101_163854 (macro-f1=0.3431)\n"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs.local_selection import load_best_trial_from_disk, extract_best_config_from_study\n",
    "from pathlib import Path\n",
    "from shared.platform_detection import detect_platform\n",
    "\n",
    "best_trials = {}\n",
    "objective_metric = hpo_config['objective']['metric']\n",
    "dataset_version = data_config.get(\"version\", \"unknown\")  # Get dataset version from config\n",
    "\n",
    "environment = detect_platform()\n",
    "\n",
    "HPO_OUTPUT_DIR_NEW = ROOT_DIR / \"outputs\" / \"hpo\" / environment\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "    \n",
    "    # First, try to use the study from the current HPO run if available\n",
    "    best_trial_info = None\n",
    "    if 'hpo_studies' in locals() and backbone_name in hpo_studies:\n",
    "        study = hpo_studies[backbone_name]\n",
    "        if study and study.best_trial is not None:\n",
    "            try:\n",
    "                # Extract best trial from current study\n",
    "                best_trial_config = extract_best_config_from_study(\n",
    "                    study, backbone_name, dataset_version, objective_metric\n",
    "                )\n",
    "                \n",
    "                # Get trial number and find the trial directory\n",
    "                trial_number = study.best_trial.number\n",
    "                \n",
    "                # Use load_best_trial_from_disk which now handles refit checkpoints\n",
    "                # It prefers refit/checkpoint/ over cv/foldN/checkpoint/\n",
    "                hpo_backbone_dir = HPO_OUTPUT_DIR_NEW / backbone_name\n",
    "                best_trial_from_disk = None\n",
    "                if hpo_backbone_dir.exists():\n",
    "                    best_trial_from_disk = load_best_trial_from_disk(\n",
    "                        HPO_OUTPUT_DIR_NEW.parent,  # Pass parent to maintain compatibility\n",
    "                        f\"{environment}/{backbone_name}\",\n",
    "                        objective_metric\n",
    "                    )\n",
    "                \n",
    "                # Build trial_info dict matching load_best_trial_from_disk format\n",
    "                if best_trial_from_disk:\n",
    "                    # Use checkpoint_dir from load_best_trial_from_disk (prefers refit)\n",
    "                    best_trial_info = {\n",
    "                        'backbone': backbone_name,\n",
    "                        'trial_name': best_trial_from_disk['trial_name'],\n",
    "                        'trial_dir': best_trial_from_disk['trial_dir'],\n",
    "                        'checkpoint_dir': best_trial_from_disk.get('checkpoint_dir', str(Path(best_trial_from_disk['trial_dir']) / 'checkpoint')),\n",
    "                        'checkpoint_type': best_trial_from_disk.get('checkpoint_type', 'unknown'),\n",
    "                        'accuracy': best_trial_from_disk['accuracy'],\n",
    "                        'metrics': best_trial_from_disk['metrics'],\n",
    "                        'hyperparameters': best_trial_config.get('hyperparameters', {}),\n",
    "                    }\n",
    "                    print(f\"{backbone}: Best trial from current HPO run is {best_trial_info['trial_name']} \"\n",
    "                          f\"({objective_metric}={best_trial_info['accuracy']:.4f})\")\n",
    "            except Exception as e:\n",
    "                import logging\n",
    "                logger = logging.getLogger(__name__)\n",
    "                logger.warning(f\"Could not extract best trial from study for {backbone}: {e}\")\n",
    "                # Fall through to disk search\n",
    "    \n",
    "    # Fallback to disk search if study not available\n",
    "    if best_trial_info is None:\n",
    "        # Try new path structure first\n",
    "        hpo_backbone_dir = HPO_OUTPUT_DIR_NEW / backbone_name\n",
    "        if hpo_backbone_dir.exists():\n",
    "            best_trial_info = load_best_trial_from_disk(\n",
    "                HPO_OUTPUT_DIR_NEW.parent,  # Pass parent to maintain compatibility\n",
    "                f\"{environment}/{backbone_name}\",  # Modified backbone path\n",
    "                objective_metric\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to old structure for backward compatibility\n",
    "            HPO_OUTPUT_DIR_OLD = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "            best_trial_info = load_best_trial_from_disk(\n",
    "                HPO_OUTPUT_DIR_OLD,\n",
    "                backbone,\n",
    "                objective_metric\n",
    "            )\n",
    "    \n",
    "    if best_trial_info:\n",
    "        best_trials[backbone] = best_trial_info\n",
    "        if 'hpo_studies' not in locals() or backbone_name not in hpo_studies:\n",
    "            # Only print if we used disk search (not already printed above)\n",
    "            print(f\"{backbone}: Best trial is {best_trial_info['trial_name']} \"\n",
    "                  f\"({objective_metric}={best_trial_info['accuracy']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computed grouping tags: study_key_hash=350a79aa1e425060..., trial_key_hash=db84e525ec11af04...\n",
      "\n",
      "Benchmarking distilbert (trial_0_20260101_163854)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 14:25:31,167 - orchestration.benchmark_utils - INFO - [Benchmark Run Name] Extracted trial_id from path (fallback): trial_0_20260101_163854\n",
      "2026-01-02 14:25:31,168 - orchestration.benchmark_utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial_0_20260101_163854, root_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml, config_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "2026-01-02 14:25:31,177 - orchestration.jobs.tracking.config.loader - INFO - [MLflow Config] Loaded config from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\\mlflow.yaml\n",
      "2026-01-02 14:25:31,178 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-02 14:25:31,179 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-02 14:25:31,180 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:625ec9f1465c9951548f48079229959fd464..., root_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml, config_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config, counter_path=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\run_name_counter.json\n",
      "2026-01-02 14:25:31,180 - orchestration.jobs.tracking.index.version_counter - WARNING - [Reserve Version] Could not acquire lock for c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\run_name_counter.json, proceeding with non-atomic write\n",
      "2026-01-02 14:25:31,182 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 18 existing allocations from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\run_name_counter.json\n",
      "2026-01-02 14:25:31,182 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 2 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2], expired=[], max_committed_version=0\n",
      "2026-01-02 14:25:31,183 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026')]\n",
      "2026-01-02 14:25:31,184 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 3 (incremented from max_committed=0, skipped 2 reserved/expired versions)\n",
      "2026-01-02 14:25:31,187 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 3 for counter_key resume-ner:benchmarking:625ec9f1465c9951548f480792... (run_id: pending_2026...)\n",
      "2026-01-02 14:25:31,188 - orchestration.benchmark_utils - INFO - [Benchmark Run Name] Generated run name: benchmark_distilbert_trial_0_20260101_163854_3\n",
      "2026-01-02 14:25:33,722 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilbert', environment='local', spec_fp=None, exec_fp=None, variant=1, trial_id='trial_0_20260101_163854', parent_training_id=None, conv_fp=None), study_key_hash=350a79aa1e425060..., trial_key_hash=db84e525ec11af04..., context.model=distilbert, context.process_type=benchmarking\n",
      "2026-01-02 14:25:33,723 - orchestration.jobs.tracking.config.loader - INFO - [MLflow Config] Config file not found at c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\benchmarking\\config\\mlflow.yaml, using defaults\n",
      "2026-01-02 14:25:33,723 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilbert, code.stage=benchmarking\n",
      "2026-01-02 14:25:35,447 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 3 in run name 'benchmark_distilbert_trial_0_20260101_163854_3'\n",
      "2026-01-02 14:25:35,448 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\benchmarking\\config, raw_auto_inc_config={}\n",
      "2026-01-02 14:25:35,449 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-02 14:25:35,449 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Auto-increment not enabled for benchmarking, skipping commit\n",
      "2026-01-02 14:25:35,452 - orchestration.jobs.tracking.index.run_index - WARNING - Could not acquire lock for c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\benchmarking\\local\\outputs\\cache\\mlflow_index.json, proceeding with non-atomic write\n",
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\azureml\\core\\__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run benchmark_distilbert_trial_0_20260101_163854_3 at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/05e58c57-c15e-4023-b2a5-ccc206638c27/runs/07a3b8e1-18b8-4891-8d5a-b503a20a7cd3\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/05e58c57-c15e-4023-b2a5-ccc206638c27\n",
      "✓ Benchmark completed: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\benchmarking\\local\\distilbert\\trial_0_20260101_163854\\benchmark.json\n",
      "\n",
      "✓ Benchmarking complete. 1/1 trials benchmarked.\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarking on best trials\n",
    "test_data_path = test_file  # Use test_file as test_data_path\n",
    "\n",
    "# Load benchmark config (if available)\n",
    "benchmark_config = configs.get(\"benchmark\", {})\n",
    "benchmark_settings = benchmark_config.get(\"benchmarking\", {})\n",
    "\n",
    "# Get benchmark parameters from config or use defaults\n",
    "benchmark_batch_sizes = benchmark_settings.get(\"batch_sizes\", [1, 8, 16])\n",
    "benchmark_iterations = benchmark_settings.get(\"iterations\", 100)\n",
    "benchmark_warmup = benchmark_settings.get(\"warmup_iterations\", 10)\n",
    "benchmark_max_length = benchmark_settings.get(\"max_length\", 512)\n",
    "benchmark_device = benchmark_settings.get(\"device\")\n",
    "\n",
    "# Create MLflow tracker for benchmarking\n",
    "from orchestration.jobs.tracking.mlflow_tracker import MLflowBenchmarkTracker\n",
    "# Use benchmark experiment name (typically same as HPO experiment with -benchmark suffix)\n",
    "benchmark_experiment_name = f\"{experiment_config.name}-benchmark\" if 'experiment_config' in locals() else \"resume_ner_baseline-benchmark\"\n",
    "benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "\n",
    "\n",
    "if test_data_path and test_data_path.exists():\n",
    "    benchmark_results = {}\n",
    "\n",
    "    for backbone, trial_info in best_trials.items():\n",
    "        # Use checkpoint_dir from trial_info if available (from load_best_trial_from_disk)\n",
    "        # This handles new structure: refit/checkpoint/ or cv/foldN/checkpoint/\n",
    "        if \"checkpoint_dir\" in trial_info:\n",
    "            checkpoint_dir = Path(trial_info[\"checkpoint_dir\"])\n",
    "        else:\n",
    "            # Fallback: construct from trial_dir (backward compatibility)\n",
    "            trial_dir = Path(trial_info[\"trial_dir\"])\n",
    "            checkpoint_dir = trial_dir / CHECKPOINT_DIRNAME\n",
    "\n",
    "        backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "\n",
    "        trial_id_raw = trial_info.get(\n",
    "            \"trial_id\") or trial_info.get(\"trial_name\", \"unknown\")\n",
    "        if trial_id_raw.startswith(\"trial_\"):\n",
    "            trial_id = trial_id_raw[6:]\n",
    "        else:\n",
    "            trial_id = trial_id_raw\n",
    "\n",
    "        if 'environment' not in locals():\n",
    "            from shared.platform_detection import detect_platform\n",
    "            environment = detect_platform()\n",
    "\n",
    "        from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "        benchmarking_context = create_naming_context(\n",
    "            process_type=\"benchmarking\",\n",
    "            model=backbone_name,\n",
    "            trial_id=trial_id,\n",
    "            environment=environment,\n",
    "        )\n",
    "        benchmarking_path = build_output_path(ROOT_DIR, benchmarking_context)\n",
    "        benchmarking_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        benchmark_output = benchmarking_path / BENCHMARK_FILENAME\n",
    "\n",
    "        if not checkpoint_dir.exists():\n",
    "            print(\n",
    "                f\"Warning: Checkpoint not found for {backbone} \"\n",
    "                f\"{trial_info['trial_name']}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if ensure_restored_from_drive(benchmark_output, is_directory=False):\n",
    "            print(\n",
    "                f\"✓ Restored benchmark results from Drive - \"\n",
    "                f\"skipping benchmarking for {backbone}\"\n",
    "            )\n",
    "            benchmark_results[backbone] = benchmark_output\n",
    "            continue\n",
    "\n",
    "        # Compute grouping tags locally (no MLflow lookup needed)\n",
    "        # We have all the data needed: data_config, hpo_config, benchmark_config, backbone, hyperparameters\n",
    "        study_key_hash = None\n",
    "        trial_key_hash = None\n",
    "        study_family_hash = None\n",
    "        \n",
    "        try:\n",
    "            from orchestration.jobs.tracking.naming.hpo_keys import (\n",
    "                build_hpo_study_key,\n",
    "                build_hpo_study_key_hash,\n",
    "                build_hpo_study_family_key,\n",
    "                build_hpo_study_family_hash,\n",
    "                build_hpo_trial_key,\n",
    "                build_hpo_trial_key_hash,\n",
    "            )\n",
    "            \n",
    "            # Get hyperparameters from trial_info\n",
    "            hyperparameters = trial_info.get('hyperparameters', {})\n",
    "            \n",
    "            if hyperparameters and data_config and hpo_config:\n",
    "                # Compute study_key_hash\n",
    "                study_key = build_hpo_study_key(\n",
    "                    data_config=data_config,\n",
    "                    hpo_config=hpo_config,\n",
    "                    model=backbone_name,\n",
    "                    benchmark_config=benchmark_config if 'benchmark_config' in locals() else None,\n",
    "                )\n",
    "                study_key_hash = build_hpo_study_key_hash(study_key)\n",
    "                \n",
    "                # Compute study_family_hash (optional, for cross-model comparison)\n",
    "                study_family_key = build_hpo_study_family_key(\n",
    "                    data_config=data_config,\n",
    "                    hpo_config=hpo_config,\n",
    "                    benchmark_config=benchmark_config if 'benchmark_config' in locals() else None,\n",
    "                )\n",
    "                study_family_hash = build_hpo_study_family_hash(study_family_key)\n",
    "                \n",
    "                # Compute trial_key_hash\n",
    "                trial_key = build_hpo_trial_key(\n",
    "                    study_key_hash=study_key_hash,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                )\n",
    "                trial_key_hash = build_hpo_trial_key_hash(trial_key)\n",
    "                \n",
    "                print(f\"  Computed grouping tags: study_key_hash={study_key_hash[:16]}..., trial_key_hash={trial_key_hash[:16]}...\")\n",
    "            else:\n",
    "                missing = []\n",
    "                if not hyperparameters:\n",
    "                    missing.append(\"hyperparameters\")\n",
    "                if not data_config:\n",
    "                    missing.append(\"data_config\")\n",
    "                if not hpo_config:\n",
    "                    missing.append(\"hpo_config\")\n",
    "                print(f\"  Warning: Cannot compute grouping tags (missing: {', '.join(missing)})\")\n",
    "        except Exception as e:\n",
    "            import logging\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logger.warning(f\"Could not compute grouping tags locally: {e}\", exc_info=True)\n",
    "            # Continue with None values (backward compatible)\n",
    "\n",
    "        print(f\"\\nBenchmarking {backbone} ({trial_info['trial_name']})...\")\n",
    "\n",
    "        success = run_benchmarking_local(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            test_data_path=test_data_path,\n",
    "            output_path=benchmark_output,\n",
    "            batch_sizes=benchmark_batch_sizes,\n",
    "            iterations=benchmark_iterations,\n",
    "            warmup_iterations=benchmark_warmup,\n",
    "            max_length=benchmark_max_length,\n",
    "            device=benchmark_device,\n",
    "            tracker=benchmark_tracker,\n",
    "            backbone=backbone,\n",
    "            benchmark_source=\"hpo_trial\",\n",
    "            study_key_hash=study_key_hash,\n",
    "            trial_key_hash=trial_key_hash,\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            benchmark_results[backbone] = benchmark_output\n",
    "            print(f\"✓ Benchmark completed: {benchmark_output}\")\n",
    "\n",
    "            if BACKUP_ENABLED:\n",
    "                backup_to_drive(benchmark_output, is_directory=False)\n",
    "                print(\"✓ Backed up benchmark results to Drive\")\n",
    "        else:\n",
    "            print(f\"✗ Benchmark failed for {backbone}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n✓ Benchmarking complete. \"\n",
    "        f\"{len(benchmark_results)}/{len(best_trials)} trials benchmarked.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping benchmarking (test data not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models. The best configuration is determined by the objective metric specified in the HPO config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "from shared.json_cache import save_json\n",
    "\n",
    "# Import local_selection directly to avoid triggering Azure ML imports in __init__.py\n",
    "local_selection_spec = importlib.util.spec_from_file_location(\n",
    "    \"local_selection\", SRC_DIR / \"orchestration\" / \"jobs\" / \"local_selection.py\"\n",
    ")\n",
    "local_selection = importlib.util.module_from_spec(local_selection_spec)\n",
    "local_selection_spec.loader.exec_module(local_selection)\n",
    "select_best_configuration_across_studies = local_selection.select_best_configuration_across_studies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = data_config.get(\"version\", \"unknown\")\n",
    "\n",
    "# Select best configuration with accuracy-speed tradeoff\n",
    "# Supports both in-memory studies and disk-based selection\n",
    "# Uses threshold from hpo_config[\"selection\"] if configured\n",
    "\n",
    "# Option 1: Use in-memory studies (if notebook still running)\n",
    "if 'hpo_studies' in locals() and hpo_studies:\n",
    "    best_configuration = select_best_configuration_across_studies(\n",
    "        studies=hpo_studies,\n",
    "        hpo_config=hpo_config,\n",
    "        dataset_version=dataset_version,\n",
    "        # Uses accuracy_threshold from hpo_config[\"selection\"] if set\n",
    "    )\n",
    "else:\n",
    "    # Option 2: Load from disk (works after notebook restart)\n",
    "    HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "    best_configuration = select_best_configuration_across_studies(\n",
    "        studies=None,  # No in-memory studies\n",
    "        hpo_config=hpo_config,\n",
    "        dataset_version=dataset_version,\n",
    "        hpo_output_dir=HPO_OUTPUT_DIR,  # Load from saved metrics.json files\n",
    "        # Uses accuracy_threshold from hpo_config[\"selection\"] if set\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration selected:\n",
      "  Backbone: distilbert\n",
      "  Trial: trial_5\n",
      "  Best macro-f1: None\n",
      "  Selection reason: unknown\n",
      "\n",
      "All candidates considered:\n",
      "\n",
      "✓ Saved to: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\best_configurations\\distilbert\\spec_81710c3324325ad0\n"
     ]
    }
   ],
   "source": [
    "from orchestration.paths import (\n",
    "    resolve_output_path,\n",
    "    save_cache_with_dual_strategy,\n",
    ")\n",
    "from datetime import datetime\n",
    "from shared.json_cache import save_json\n",
    "\n",
    "# Extract backbone name first (needed for spec_fp computation)\n",
    "backbone = best_configuration.get('backbone', 'unknown')\n",
    "backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "\n",
    "# Use new centralized naming system with fingerprints\n",
    "from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "from orchestration.fingerprints import compute_spec_fp\n",
    "from orchestration.config_loader import load_all_configs\n",
    "\n",
    "# Compute spec_fp for best configuration\n",
    "all_configs = load_all_configs(experiment_config)\n",
    "spec_fp = compute_spec_fp(\n",
    "    model_config=all_configs.get(\"model\", {}),\n",
    "    data_config=all_configs.get(\"data\", {}),\n",
    "    train_config=all_configs.get(\"train\", {}),\n",
    "    seed=configs.get(\"train\", {}).get(\"training\", {}).get(\"random_seed\", 42)\n",
    ")\n",
    "\n",
    "# Create naming context for best configuration\n",
    "best_config_context = create_naming_context(\n",
    "    process_type=\"best_configurations\",\n",
    "    model=backbone_name,\n",
    "    spec_fp=spec_fp\n",
    ")\n",
    "\n",
    "# Build path using new structure: outputs/cache/best_configurations/<model>/spec_<spec_fp>/\n",
    "BEST_CONFIG_CACHE_DIR = build_output_path(ROOT_DIR, best_config_context)\n",
    "BEST_CONFIG_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Generate timestamp and identifiers\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "trial_name = best_configuration.get('trial_name', 'unknown')\n",
    "trial_id = best_configuration.get('trial_id', 'unknown')\n",
    "\n",
    "# Generate stable training name from best configuration\n",
    "stable_training_name = f\"{backbone_name}_{trial_name}\"\n",
    "\n",
    "timestamped_file, latest_file, index_file = save_cache_with_dual_strategy(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    cache_type=\"best_configurations\",\n",
    "    data=best_configuration,\n",
    "    backbone=backbone,\n",
    "    identifier=trial_name,\n",
    "    timestamp=timestamp,\n",
    "    additional_metadata={\n",
    "        \"experiment_name\": experiment_config.name if 'experiment_config' in locals() else \"unknown\",\n",
    "        \"hpo_study_name\": hpo_config.get('study_name', 'unknown') if 'hpo_config' in locals() else \"unknown\",\n",
    "        \"spec_fp\": spec_fp\n",
    "    }\n",
    ")\n",
    "\n",
    "# Also save directly to new fingerprint-based directory\n",
    "new_timestamped_file = BEST_CONFIG_CACHE_DIR / f\"best_config_{backbone_name}_{trial_name}_{timestamp}.json\"\n",
    "save_json(new_timestamped_file, best_configuration)\n",
    "\n",
    "# Save latest and index to new directory\n",
    "new_latest_file = BEST_CONFIG_CACHE_DIR / \"latest_best_configuration.json\"\n",
    "save_json(new_latest_file, best_configuration)\n",
    "\n",
    "# Update index in new directory\n",
    "from orchestration.paths import get_cache_file_path, load_json\n",
    "new_index_file = BEST_CONFIG_CACHE_DIR / \"index.json\"\n",
    "index_data = load_json(new_index_file, default={\"entries\": []})\n",
    "index_entry = {\n",
    "    \"backbone\": backbone,\n",
    "    \"trial_name\": trial_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"file\": str(new_timestamped_file),\n",
    "    \"spec_fp\": spec_fp\n",
    "}\n",
    "index_data.setdefault(\"entries\", []).append(index_entry)\n",
    "save_json(new_index_file, index_data)\n",
    "\n",
    "\n",
    "print(f\"Best configuration selected:\")\n",
    "print(f\"  Backbone: {best_configuration.get('backbone', 'unknown')}\")\n",
    "print(f\"  Trial: {trial_name}\")\n",
    "best_value = best_configuration.get('best_value', None)\n",
    "if best_value is not None and isinstance(best_value, (int, float)):\n",
    "    print(f\"  Best macro-f1: {best_value:.4f}\")\n",
    "else:\n",
    "    print(f\"  Best macro-f1: {best_value}\")\n",
    "print(f\"  Selection reason: {best_configuration.get('selection_reason', 'unknown')}\")\n",
    "print(f\"\\nAll candidates considered:\")\n",
    "for candidate in best_configuration.get('candidates', []):\n",
    "    candidate_value = candidate.get('best_value', None)\n",
    "    if candidate_value is not None and isinstance(candidate_value, (int, float)):\n",
    "        print(f\"  ✓ {candidate.get('backbone', 'unknown')}: acc={candidate_value:.4f}, speed={candidate.get('speed_factor', 1.0):.2f}x\")\n",
    "    else:\n",
    "        print(f\"  ✓ {candidate.get('backbone', 'unknown')}: acc={candidate_value}, speed={candidate.get('speed_factor', 1.0):.2f}x\")\n",
    "\n",
    "print(f\"\\n✓ Saved to: {BEST_CONFIG_CACHE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions. This uses the full training epochs (no early stopping) and the best hyperparameters found during HPO.\n",
    "\n",
    "**Note**: After training completes, the checkpoint will be automatically backed up to Google Drive for persistence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "from shared.json_cache import load_json, save_json\n",
    "from orchestration import STAGE_TRAINING\n",
    "\n",
    "# Define build_final_training_config locally to avoid importing Azure ML dependencies\n",
    "# This function doesn't use Azure ML, so we can define it here\n",
    "def build_final_training_config(\n",
    "    best_config: dict,\n",
    "    train_config: dict,\n",
    "    random_seed: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Build final training configuration by merging best HPO config with train.yaml defaults.\n",
    "    \"\"\"\n",
    "    hyperparameters = best_config.get(\"hyperparameters\", {})\n",
    "    training_defaults = train_config.get(\"training\", {})\n",
    "    \n",
    "    return {\n",
    "        \"backbone\": best_config[\"backbone\"],\n",
    "        \"learning_rate\": hyperparameters.get(\"learning_rate\", training_defaults.get(\"learning_rate\", 2e-5)),\n",
    "        \"dropout\": hyperparameters.get(\"dropout\", training_defaults.get(\"dropout\", 0.1)),\n",
    "        \"weight_decay\": hyperparameters.get(\"weight_decay\", training_defaults.get(\"weight_decay\", 0.01)),\n",
    "        \"batch_size\": training_defaults.get(\"batch_size\", 16),\n",
    "        \"epochs\": training_defaults.get(\"epochs\", 5),\n",
    "        \"random_seed\": random_seed,\n",
    "        \"early_stopping_enabled\": False,\n",
    "        \"use_combined_data\": True,\n",
    "        \"use_all_data\": True,\n",
    "    }\n",
    "\n",
    "DEFAULT_RANDOM_SEED = 42\n",
    "FINAL_TRAINING_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"final_training\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.paths import load_cache_file\n",
    "from orchestration.metadata_manager import (\n",
    "    load_training_metadata,\n",
    "    is_training_complete,\n",
    "    are_training_artifacts_uploaded,\n",
    "    save_training_metadata,\n",
    "    get_training_checkpoint_path,\n",
    ")\n",
    "\n",
    "# Try loading from centralized cache first\n",
    "best_configuration = load_cache_file(\n",
    "    ROOT_DIR, CONFIG_DIR, \"best_configurations\", use_latest=True\n",
    ")\n",
    "\n",
    "\n",
    "if best_configuration is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Best configuration cache not found.\\n\"\n",
    "        f\"Please run Step P1-3.6: Best Configuration Selection first.\\n\"\n",
    "        f\"Cache directory: {resolve_output_path(ROOT_DIR, CONFIG_DIR, 'cache', subcategory='best_configurations')}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training configuration:\n",
      "  Backbone: distilbert\n",
      "  Learning rate: 4.033407641328295e-05\n",
      "  Batch size: 2\n",
      "  Dropout: 0.2962144733204924\n",
      "  Weight decay: 0.005813977793367053\n",
      "  Epochs: 1\n",
      "  Random seed: 42\n",
      "  Early stopping: False\n"
     ]
    }
   ],
   "source": [
    "# Build final training configuration from best HPO configuration\n",
    "# Use train_config from configs if available, otherwise load it\n",
    "if 'train_config' not in locals():\n",
    "    train_config = configs.get(\"train\", {})\n",
    "\n",
    "# Read seed from train_config (with fallback to 42)\n",
    "random_seed = train_config.get(\"training\", {}).get(\"random_seed\", 42)\n",
    "\n",
    "final_training_config = build_final_training_config(\n",
    "    best_config=best_configuration,\n",
    "    train_config=train_config,\n",
    "    random_seed=random_seed,\n",
    ")\n",
    "\n",
    "print(\"Final training configuration:\")\n",
    "print(f\"  Backbone: {final_training_config['backbone']}\")\n",
    "print(f\"  Learning rate: {final_training_config['learning_rate']}\")\n",
    "print(f\"  Batch size: {final_training_config['batch_size']}\")\n",
    "print(f\"  Dropout: {final_training_config['dropout']}\")\n",
    "print(f\"  Weight decay: {final_training_config['weight_decay']}\")\n",
    "print(f\"  Epochs: {final_training_config['epochs']}\")\n",
    "print(f\"  Random seed: {final_training_config['random_seed']}\")\n",
    "print(f\"  Early stopping: {final_training_config['early_stopping_enabled']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Computed fingerprints:\n",
      "  spec_fp: 81710c3324325ad0\n",
      "  exec_fp: 8d244347b2eff67e\n",
      "Using new variant 1\n",
      "✓ Final training output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\n",
      "Training not yet completed - will proceed with training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='', creation_time=1766263257813, experiment_id='a5897b88-fd66-448c-ae65-1ef21bfc11dd', last_update_time=None, lifecycle_stage='active', name='resume_ner_baseline-training-distilbert', tags={}>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from orchestration.naming_centralized import (\n",
    "    create_naming_context,\n",
    "    build_output_path,\n",
    ")\n",
    "from orchestration.config_loader import load_all_configs\n",
    "from orchestration.fingerprints import compute_spec_fp, compute_exec_fp\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "mlflow_experiment_name = (\n",
    "    f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Use new centralized naming system with fingerprints and variants\n",
    "backbone = best_configuration.get(\"backbone\", \"unknown\")\n",
    "backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "\n",
    "# Get git SHA for exec_fp computation\n",
    "try:\n",
    "    git_sha = subprocess.check_output(\n",
    "        [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "        cwd=ROOT_DIR,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    ).decode().strip()\n",
    "except Exception:\n",
    "    git_sha = \"unknown\"\n",
    "\n",
    "# Compute fingerprints for final training\n",
    "\n",
    "all_configs = load_all_configs(experiment_config)\n",
    "spec_fp = compute_spec_fp(\n",
    "    model_config=all_configs.get(\"model\", {}),\n",
    "    data_config=all_configs.get(\"data\", {}),\n",
    "    train_config=all_configs.get(\"train\", {}),\n",
    "    seed=final_training_config.get(\"random_seed\", 42),\n",
    ")\n",
    "exec_fp = compute_exec_fp(\n",
    "    git_sha=git_sha,\n",
    "    env_config=all_configs.get(\"env\", {}),\n",
    ")\n",
    "\n",
    "print(\"✓ Computed fingerprints:\")\n",
    "print(f\"  spec_fp: {spec_fp}\")\n",
    "print(f\"  exec_fp: {exec_fp}\")\n",
    "\n",
    "# Create naming context for final training\n",
    "\n",
    "# Ensure environment is defined\n",
    "if \"environment\" not in locals():\n",
    "    from shared.platform_detection import detect_platform\n",
    "    environment = detect_platform()\n",
    "\n",
    "# Check for existing training with same fingerprints (check variants)\n",
    "# Use variant resolution to find next available variant or reuse existing\n",
    "from orchestration.final_training_config import _compute_next_variant, _find_existing_variant, _is_variant_complete\n",
    "\n",
    "# Check for existing variants\n",
    "existing_variant = _find_existing_variant(\n",
    "    ROOT_DIR, CONFIG_DIR, spec_fp, exec_fp, backbone_name\n",
    ")\n",
    "\n",
    "# Reuse existing variant if complete, otherwise use next available\n",
    "if existing_variant and _is_variant_complete(ROOT_DIR, CONFIG_DIR, spec_fp, exec_fp, backbone_name, existing_variant):\n",
    "    variant = existing_variant\n",
    "    print(f\"Reusing existing variant {variant}\")\n",
    "else:\n",
    "    variant = _compute_next_variant(ROOT_DIR, CONFIG_DIR, spec_fp, exec_fp, backbone_name)\n",
    "    print(f\"Using new variant {variant}\")\n",
    "training_context = create_naming_context(\n",
    "    process_type=\"final_training\",\n",
    "    model=backbone_name,\n",
    "    spec_fp=spec_fp,\n",
    "    exec_fp=exec_fp,\n",
    "    environment=environment,\n",
    "    variant=variant,\n",
    ")\n",
    "\n",
    "final_output_dir = build_output_path(ROOT_DIR, training_context)\n",
    "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Final training output directory: {final_output_dir}\")\n",
    "\n",
    "# Check if training already exists at this path\n",
    "checkpoint_path = final_output_dir / \"checkpoint\"\n",
    "training_complete = (\n",
    "    checkpoint_path.exists() and any(checkpoint_path.iterdir())\n",
    ")\n",
    "\n",
    "stable_training_name = (\n",
    "    f\"{backbone_name}_{best_configuration.get('trial_name', 'unknown')}\"\n",
    ")\n",
    "\n",
    "if training_complete:\n",
    "    print(f\"✓ Training already completed at: {final_output_dir}\")\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"  Checkpoint: {checkpoint_path}\")\n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    print(\"Training not yet completed - will proceed with training\")\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training as a module (python -m training.train) to allow relative imports to work\n",
    "# This requires src/ to be in PYTHONPATH (set in env below)\n",
    "training_args = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"training.train\",\n",
    "    \"--data-asset\",\n",
    "    str(DATASET_LOCAL_PATH),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    final_training_config[\"backbone\"],\n",
    "    \"--learning-rate\",\n",
    "    str(final_training_config[\"learning_rate\"]),\n",
    "    \"--batch-size\",\n",
    "    str(final_training_config[\"batch_size\"]),\n",
    "    \"--dropout\",\n",
    "    str(final_training_config[\"dropout\"]),\n",
    "    \"--weight-decay\",\n",
    "    str(final_training_config[\"weight_decay\"]),\n",
    "    \"--epochs\",\n",
    "    str(final_training_config[\"epochs\"]),\n",
    "    \"--random-seed\",\n",
    "    str(final_training_config[\"random_seed\"]),\n",
    "    \"--early-stopping-enabled\",\n",
    "    str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
    "    \"--use-combined-data\",\n",
    "    str(final_training_config[\"use_combined_data\"]).lower(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = os.environ.copy()\n",
    "training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
    "\n",
    "# Add src directory to PYTHONPATH to allow relative imports in training.train\n",
    "pythonpath = training_env.get(\"PYTHONPATH\", \"\")\n",
    "if pythonpath:\n",
    "    training_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "else:\n",
    "    training_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "training_env[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n",
    "\n",
    "            # Note: Run name is now set automatically by trainer.py using build_mlflow_run_name()\n",
    "# Extract backbone name (e.g., \"distilbert\" from \"distilbert-base-uncased\" or use as-is if already short)\n",
    "backbone_value = final_training_config[\"backbone\"]\n",
    "# If backbone contains hyphens, extract the first part (e.g., \"distilbert\" from \"distilbert-base-uncased\")\n",
    "# Otherwise use as-is (e.g., \"distilbert\" stays \"distilbert\")\n",
    "backbone_name = backbone_value.split(\"-\")[0] if \"-\" in backbone_value else backbone_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check if we should skip training\n",
    "if 'SKIP_TRAINING' in locals() and SKIP_TRAINING:\n",
    "    print(\"Skipping training - already completed\")\n",
    "\n",
    "    checkpoint_path = None\n",
    "\n",
    "    # First, try new fingerprint-based location\n",
    "    if 'final_output_dir' in locals() and final_output_dir:\n",
    "        new_checkpoint = final_output_dir / \"checkpoint\"\n",
    "        if new_checkpoint.exists() and any(new_checkpoint.iterdir()):\n",
    "            checkpoint_path = new_checkpoint\n",
    "        else:\n",
    "            checkpoint_path = None\n",
    "\n",
    "    # Third, try actual checkpoint location\n",
    "    if not checkpoint_path:\n",
    "        actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "        if actual_checkpoint.exists() and any(actual_checkpoint.iterdir()):\n",
    "            checkpoint_path = actual_checkpoint\n",
    "            final_output_dir = actual_checkpoint.parent\n",
    "\n",
    "    if checkpoint_path and checkpoint_path.exists():\n",
    "        if 'artifacts_uploaded' not in locals():\n",
    "            try:\n",
    "                artifacts_uploaded = are_training_artifacts_uploaded(\n",
    "                    ROOT_DIR, CONFIG_DIR, stable_training_name\n",
    "                )\n",
    "            except Exception:\n",
    "                artifacts_uploaded = False\n",
    "\n",
    "        if not artifacts_uploaded:\n",
    "            metrics_file = checkpoint_path.parent / \"metrics.json\"\n",
    "            if not metrics_file.exists():\n",
    "                metrics_file = checkpoint_path.parent.parent / \"metrics.json\"\n",
    "\n",
    "            metrics = None\n",
    "            if metrics_file.exists():\n",
    "                import json\n",
    "                with open(metrics_file, \"r\") as f:\n",
    "                    metrics = json.load(f)\n",
    "\n",
    "            UPLOAD_ARTIFACTS = True\n",
    "        else:\n",
    "            UPLOAD_ARTIFACTS = False\n",
    "    else:\n",
    "        print(\"⚠ Warning: Checkpoint not found - proceeding without checkpoint\")\n",
    "\n",
    "        if 'final_output_dir' not in locals() or not final_output_dir:\n",
    "            if 'training_context' in locals():\n",
    "                from orchestration.naming_centralized import build_output_path\n",
    "                final_output_dir = build_output_path(\n",
    "                    ROOT_DIR, training_context\n",
    "                )\n",
    "            else:\n",
    "                from orchestration.naming_centralized import (\n",
    "                    create_naming_context,\n",
    "                    build_output_path,\n",
    "                )\n",
    "                from orchestration.fingerprints import (\n",
    "                    compute_spec_fp,\n",
    "                    compute_exec_fp,\n",
    "                )\n",
    "                from orchestration.final_training_config import (\n",
    "                    _compute_next_variant,\n",
    "                )\n",
    "                from orchestration.config_loader import load_all_configs\n",
    "                from shared.platform_detection import detect_platform\n",
    "\n",
    "                environment = detect_platform()\n",
    "                backbone_name = final_training_config.get(\n",
    "                    \"backbone\", \"unknown\"\n",
    "                )\n",
    "                if \"-\" in backbone_name:\n",
    "                    backbone_name = backbone_name.split(\"-\")[0]\n",
    "\n",
    "                try:\n",
    "                    from orchestration.metadata_manager import (\n",
    "                        load_training_metadata,\n",
    "                    )\n",
    "                    old_meta = load_training_metadata(\n",
    "                        ROOT_DIR, CONFIG_DIR, stable_training_name\n",
    "                    )\n",
    "                    spec_fp = old_meta.get(\"fingerprints\", {}).get(\"spec_fp\")\n",
    "                    exec_fp = old_meta.get(\"fingerprints\", {}).get(\"exec_fp\")\n",
    "                except Exception:\n",
    "                    spec_fp = None\n",
    "                    exec_fp = None\n",
    "\n",
    "                if not spec_fp or not exec_fp:\n",
    "                    all_configs = load_all_configs(experiment_config)\n",
    "                    spec_fp = compute_spec_fp(\n",
    "                        model_config=all_configs.get(\"model\", {}),\n",
    "                        data_config=all_configs.get(\"data\", {}),\n",
    "                        train_config=all_configs.get(\"train\", {}),\n",
    "                        seed=final_training_config.get(\"random_seed\", 42),\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        git_sha = subprocess.check_output(\n",
    "                            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "                            cwd=ROOT_DIR,\n",
    "                            stderr=subprocess.DEVNULL,\n",
    "                        ).decode().strip()\n",
    "                    except Exception:\n",
    "                        git_sha = None\n",
    "\n",
    "                    exec_fp = compute_exec_fp(\n",
    "                        git_sha=git_sha,\n",
    "                        env_config=all_configs.get(\"env\", {}),\n",
    "                    )\n",
    "\n",
    "                training_context = create_naming_context(\n",
    "                    process_type=\"final_training\",\n",
    "                    model=backbone_name,\n",
    "                    spec_fp=spec_fp,\n",
    "                    exec_fp=exec_fp,\n",
    "                    environment=environment,\n",
    "                    variant=_compute_next_variant(\n",
    "                        ROOT_DIR,\n",
    "                        CONFIG_DIR,\n",
    "                        spec_fp,\n",
    "                        exec_fp,\n",
    "                        backbone_name,\n",
    "                    ),\n",
    "                )\n",
    "                final_output_dir = build_output_path(\n",
    "                    ROOT_DIR, training_context\n",
    "                )\n",
    "\n",
    "        UPLOAD_ARTIFACTS = False\n",
    "\n",
    "    result = type(\n",
    "        \"obj\",\n",
    "        (object,),\n",
    "        {\"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"},\n",
    "    )()\n",
    "\n",
    "else:\n",
    "    result = subprocess.run(\n",
    "        training_args,\n",
    "        cwd=ROOT_DIR,\n",
    "        env=training_env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\n",
    "        f\"Final training failed with return code {result.returncode}\"\n",
    "    )\n",
    "else:\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "\n",
    "    if BACKUP_ENABLED:\n",
    "        checkpoint_dir = final_output_dir / \"checkpoint\"\n",
    "        if not checkpoint_dir.exists():\n",
    "            actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "            if actual_checkpoint.exists():\n",
    "                checkpoint_dir = actual_checkpoint\n",
    "\n",
    "        if checkpoint_dir.exists():\n",
    "            backup_to_drive(checkpoint_dir, is_directory=True)\n",
    "            print(\n",
    "                f\"Backed up final training checkpoint to Drive: {checkpoint_dir}\"\n",
    "            )\n",
    "\n",
    "        metrics_file = final_output_dir / \"metrics.json\"\n",
    "        if not metrics_file.exists():\n",
    "            actual_metrics = ROOT_DIR / \"outputs\" / \"metrics.json\"\n",
    "            if actual_metrics.exists():\n",
    "                metrics_file = actual_metrics\n",
    "\n",
    "        if metrics_file.exists():\n",
    "            backup_to_drive(metrics_file, is_directory=False)\n",
    "\n",
    "    if mlflow_tracking_uri:\n",
    "        try:\n",
    "            from orchestration.jobs.tracking.mlflow_tracker import (\n",
    "                MLflowTrainingTracker,\n",
    "            )\n",
    "            training_tracker = MLflowTrainingTracker(\n",
    "                mlflow_experiment_name\n",
    "            )\n",
    "\n",
    "            checkpoint_dir = final_output_dir / \"checkpoint\"\n",
    "            if not checkpoint_dir.exists():\n",
    "                actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "                if actual_checkpoint.exists():\n",
    "                    checkpoint_dir = actual_checkpoint\n",
    "\n",
    "            metrics_json_path = final_output_dir / \"metrics.json\"\n",
    "            if not metrics_json_path.exists():\n",
    "                actual_metrics = ROOT_DIR / \"outputs\" / \"metrics.json\"\n",
    "                if actual_metrics.exists():\n",
    "                    metrics_json_path = actual_metrics\n",
    "\n",
    "            if checkpoint_dir.exists() or metrics_json_path.exists():\n",
    "                import mlflow\n",
    "                # Use new systematic MLflow run finder\n",
    "                from orchestration.jobs.tracking.finder.run_finder import find_mlflow_run\n",
    "\n",
    "                # Find MLflow run using new finder\n",
    "                report = find_mlflow_run(\n",
    "                    experiment_name=mlflow_experiment_name,\n",
    "                    context=training_context if 'training_context' in locals() else None,\n",
    "                    output_dir=final_output_dir if 'final_output_dir' in locals() else None,\n",
    "                    strict=True,  # Default: fail loud instead of attaching to wrong run\n",
    "                    root_dir=ROOT_DIR,\n",
    "                    config_dir=CONFIG_DIR if 'CONFIG_DIR' in locals() else None,\n",
    "                )\n",
    "\n",
    "                if report.found and report.run_id:\n",
    "                    with mlflow.start_run(run_id=report.run_id):\n",
    "                        training_tracker.log_training_artifacts(\n",
    "                            checkpoint_dir=checkpoint_dir\n",
    "                            if checkpoint_dir.exists()\n",
    "                            else None,\n",
    "                            metrics_json_path=metrics_json_path\n",
    "                            if metrics_json_path.exists()\n",
    "                            else None,\n",
    "                        )\n",
    "                        print(f\"✓ Logged training artifacts to MLflow run {report.run_id}\")\n",
    "                        print(f\"  Strategy used: {report.strategy_used}\")\n",
    "                else:\n",
    "                    print(f\"⚠ Could not find MLflow run for artifact upload\")\n",
    "                    print(f\"  Experiment: {mlflow_experiment_name}\")\n",
    "                    if report.error:\n",
    "                        print(f\"  Error: {report.error}\")\n",
    "                    if report.strategies_attempted:\n",
    "                        print(f\"  Attempted strategies: {', '.join(report.strategies_attempted)}\")\n",
    "                    print(f\"  Try checking the MLflow UI for the most recent run\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"⚠ Failed to log training artifacts to MLflow: {e}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training completion...\n",
      "  Expected checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\checkpoint (exists: True)\n",
      "  Actual checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\checkpoint (exists: False)\n",
      "  Expected metrics: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\metrics.json (exists: True)\n",
      "  Actual metrics: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\metrics.json (exists: False)\n",
      "✓ Using expected checkpoint location: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\checkpoint\n",
      "✓ Metrics loaded from: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\metrics.json\n",
      "  Metrics: {'macro-f1': 0.3722755013077594, 'macro-f1-span': 0.08695652173913043, 'loss': 1.9506404399871826, 'per_entity': {'AME': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}, 'KILL': {'precision': 0.4, 'recall': 0.19047619047619047, 'f1': 0.25806451612903225, 'support': 21}, 'MAIL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}}}\n",
      "✓ Saved training completion to metadata: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\metadata.json\n",
      "✓ Updated training index\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Check actual checkpoint location\n",
    "# The training script may save to outputs/checkpoint instead of final_output_dir/checkpoint\n",
    "actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "actual_metrics = ROOT_DIR / \"outputs\" / METRICS_FILENAME\n",
    "expected_checkpoint = final_output_dir / \"checkpoint\"\n",
    "expected_metrics = final_output_dir / METRICS_FILENAME\n",
    "\n",
    "print(\"Checking training completion...\")\n",
    "print(f\"  Expected checkpoint: {expected_checkpoint} (exists: {expected_checkpoint.exists()})\")\n",
    "print(f\"  Actual checkpoint: {actual_checkpoint} (exists: {actual_checkpoint.exists()})\")\n",
    "print(f\"  Expected metrics: {expected_metrics} (exists: {expected_metrics.exists()})\")\n",
    "print(f\"  Actual metrics: {actual_metrics} (exists: {actual_metrics.exists()})\")\n",
    "\n",
    "# Determine which checkpoint and metrics to use\n",
    "checkpoint_source = None\n",
    "metrics_file = None\n",
    "\n",
    "if expected_checkpoint.exists() and any(expected_checkpoint.iterdir()):\n",
    "    checkpoint_source = expected_checkpoint\n",
    "    print(f\"✓ Using expected checkpoint location: {checkpoint_source}\")\n",
    "elif actual_checkpoint.exists() and any(actual_checkpoint.iterdir()):\n",
    "    checkpoint_source = actual_checkpoint\n",
    "    print(f\"✓ Using actual checkpoint location: {checkpoint_source}\")\n",
    "    # Update final_output_dir to match actual location\n",
    "    final_output_dir = actual_checkpoint.parent\n",
    "\n",
    "if expected_metrics.exists():\n",
    "    metrics_file = expected_metrics\n",
    "elif actual_metrics.exists():\n",
    "    metrics_file = actual_metrics\n",
    "\n",
    "# Load metrics if available\n",
    "metrics = None\n",
    "if metrics_file and metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"✓ Metrics loaded from: {metrics_file}\")\n",
    "    print(f\"  Metrics: {metrics}\")\n",
    "elif checkpoint_source:\n",
    "    print(f\"⚠ Warning: Metrics file not found, but checkpoint exists.\")\n",
    "    metrics = {\"status\": \"completed\", \"checkpoint_found\": True}\n",
    "\n",
    "# Save training completion to metadata using new system with fingerprints\n",
    "if 'training_context' in locals() and not training_complete:\n",
    "    from orchestration.metadata_manager import save_metadata_with_fingerprints\n",
    "    from orchestration.index_manager import update_index\n",
    "    \n",
    "    metadata_content = {\n",
    "        \"backbone\": best_configuration.get('backbone', 'unknown'),\n",
    "        \"trial_name\": best_configuration.get('trial_name', 'unknown'),\n",
    "        \"trial_id\": best_configuration.get('trial_id', 'unknown'),\n",
    "        \"checkpoint_path\": str(checkpoint_source) if checkpoint_source else None,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    \n",
    "    # Save metadata with fingerprints\n",
    "    metadata_file = save_metadata_with_fingerprints(\n",
    "        ROOT_DIR,\n",
    "        CONFIG_DIR,\n",
    "        training_context,\n",
    "        metadata_content,\n",
    "        status_updates={\n",
    "            \"training\": {\n",
    "                \"completed\": True,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"✓ Saved training completion to metadata: {metadata_file}\")\n",
    "    \n",
    "    # Update index for fast lookup\n",
    "    update_index(ROOT_DIR, CONFIG_DIR, training_context, metadata_content)\n",
    "    print(f\"✓ Updated training index\")\n",
    "    \n",
    "    if 'stable_training_name' in locals():\n",
    "        from datetime import datetime\n",
    "        best_config_timestamp = best_configuration.get('cache_metadata', {}).get('saved_at', datetime.now().isoformat())\n",
    "        if isinstance(best_config_timestamp, str) and 'T' in best_config_timestamp:\n",
    "            best_config_timestamp = best_config_timestamp.split('T')[0].replace('-', '') + '_' + best_config_timestamp.split('T')[1].split('.')[0].replace(':', '')\n",
    "        \n",
    "elif checkpoint_source:\n",
    "    # Checkpoint found but training_context condition not met - still valid\n",
    "    pass\n",
    "else:\n",
    "    # No checkpoint found - raise error\n",
    "    raise FileNotFoundError(\n",
    "        f\"Training completed but no checkpoint found.\\n\"\n",
    "        f\"  Expected: {expected_checkpoint}\\n\"\n",
    "        f\"  Actual: {actual_checkpoint}\\n\"\n",
    "        f\"  Please check training logs for errors.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved timestamped final training cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\final_training_distilbert_spec_81710c3324325ad0_exec_8d244347b2eff67e_v1_20251230_185958.json\n",
      "✓ Updated latest cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\latest_final_training_cache.json\n",
      "✓ Updated index: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\final_training_index.json\n"
     ]
    }
   ],
   "source": [
    "from orchestration.paths import (\n",
    "    resolve_output_path,\n",
    "    save_cache_with_dual_strategy,\n",
    ")\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "# Ensure final_output_dir is defined\n",
    "if 'final_output_dir' not in locals() or not final_output_dir:\n",
    "    # Try to get from training_context\n",
    "    if 'training_context' in locals():\n",
    "        from orchestration.naming_centralized import build_output_path\n",
    "        final_output_dir = build_output_path(ROOT_DIR, training_context)\n",
    "    else:\n",
    "        # Fallback: use expected path structure\n",
    "        from orchestration.naming_centralized import (\n",
    "            create_naming_context,\n",
    "            build_output_path,\n",
    "        )\n",
    "        from orchestration.fingerprints import (\n",
    "            compute_spec_fp,\n",
    "            compute_exec_fp,\n",
    "        )\n",
    "        from orchestration.final_training_config import _compute_next_variant\n",
    "        from orchestration.config_loader import load_all_configs\n",
    "        from shared.platform_detection import detect_platform\n",
    "\n",
    "        environment = detect_platform()\n",
    "        backbone_name = final_training_config.get(\n",
    "            \"backbone\", \"unknown\"\n",
    "        )\n",
    "        if \"-\" in backbone_name:\n",
    "            backbone_name = backbone_name.split(\"-\")[0]\n",
    "\n",
    "        # Compute fingerprints (this should match the earlier computation)\n",
    "        all_configs = load_all_configs(experiment_config)\n",
    "        spec_fp = compute_spec_fp(\n",
    "            model_config=all_configs.get(\"model\", {}),\n",
    "            data_config=all_configs.get(\"data\", {}),\n",
    "            train_config=all_configs.get(\"train\", {}),\n",
    "            seed=final_training_config.get(\"random_seed\", 42),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            git_sha = subprocess.check_output(\n",
    "                [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "                cwd=ROOT_DIR,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            ).decode().strip()\n",
    "        except Exception:\n",
    "            git_sha = None\n",
    "\n",
    "        exec_fp = compute_exec_fp(\n",
    "            git_sha=git_sha,\n",
    "            env_config=all_configs.get(\"env\", {}),\n",
    "        )\n",
    "\n",
    "        training_context = create_naming_context(\n",
    "            process_type=\"final_training\",\n",
    "            model=backbone_name,\n",
    "            spec_fp=spec_fp,\n",
    "            exec_fp=exec_fp,\n",
    "            environment=environment,\n",
    "            variant=_compute_next_variant(\n",
    "                ROOT_DIR,\n",
    "                CONFIG_DIR,\n",
    "                spec_fp,\n",
    "                exec_fp,\n",
    "                backbone_name,\n",
    "            ),\n",
    "        )\n",
    "        final_output_dir = build_output_path(ROOT_DIR, training_context)\n",
    "\n",
    "# Prepare cache data\n",
    "final_training_cache_data = {\n",
    "    \"output_dir\": str(final_output_dir),\n",
    "    \"backbone\": final_training_config[\"backbone\"],\n",
    "    \"run_id\": stable_training_name,\n",
    "    \"config\": final_training_config,\n",
    "    \"metrics\": metrics,\n",
    "}\n",
    "\n",
    "# Save using dual file strategy\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backbone = (\n",
    "    final_training_config[\"backbone\"]\n",
    "    .replace(\"-\", \"_\")\n",
    "    .replace(\"/\", \"_\")\n",
    ")\n",
    "\n",
    "# Use fingerprint-based identifier for cache naming\n",
    "if (\n",
    "    'training_context' in locals()\n",
    "    and training_context.spec_fp\n",
    "    and training_context.exec_fp\n",
    "):\n",
    "    from orchestration.naming_centralized import build_parent_training_id\n",
    "    cache_identifier = build_parent_training_id(\n",
    "        spec_fp=training_context.spec_fp,\n",
    "        exec_fp=training_context.exec_fp,\n",
    "        variant=training_context.variant,\n",
    "    ).replace(\"/\", \"_\")\n",
    "else:\n",
    "    cache_identifier = (\n",
    "        stable_training_name.replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "timestamped_file, latest_file, index_file = save_cache_with_dual_strategy(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    cache_type=\"final_training\",\n",
    "    data=final_training_cache_data,\n",
    "    backbone=backbone,\n",
    "    identifier=cache_identifier,\n",
    "    timestamp=timestamp,\n",
    "    additional_metadata={\n",
    "        \"checkpoint_path\": (\n",
    "            str(checkpoint_source) if checkpoint_source else None\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"✓ Saved timestamped final training cache: {timestamped_file}\")\n",
    "print(f\"✓ Updated latest cache: {latest_file}\")\n",
    "print(f\"✓ Updated index: {index_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
    "\n",
    "**Platform Adapter Note**: The conversion script (`src/model_conversion/convert_to_onnx.py`) uses the platform adapter to automatically handle output paths and logging appropriately for local execution.\n",
    "\n",
    "**Checkpoint Restoration**: \n",
    "- **Google Colab**: If the checkpoint is not found locally (e.g., after a session disconnect), it will be automatically restored from Google Drive.\n",
    "- **Kaggle**: Checkpoints are automatically persisted in `/kaggle/working/` - no restoration needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "import shutil\n",
    "from shared.json_cache import load_json\n",
    "\n",
    "CONVERSION_SCRIPT_PATH = SRC_DIR / \"model_conversion\" / \"convert_to_onnx.py\"\n",
    "CONVERSION_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"conversion\"\n",
    "\n",
    "from orchestration.metadata_manager import (\n",
    "    is_conversion_complete,\n",
    "    are_conversion_artifacts_uploaded,\n",
    "    save_training_metadata,\n",
    "    get_conversion_onnx_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.paths import load_cache_file, resolve_output_path\n",
    "\n",
    "# Try loading from centralized cache first\n",
    "training_cache = load_cache_file(\n",
    "    ROOT_DIR, CONFIG_DIR, \"final_training\", use_latest=True\n",
    ")\n",
    "\n",
    "# Restore cache from Drive if missing\n",
    "cache_dir = resolve_output_path(ROOT_DIR, CONFIG_DIR, \"cache\", subcategory=\"final_training\")\n",
    "latest_cache_file = cache_dir / \"latest_final_training_cache.json\"\n",
    "if latest_cache_file.exists() or ensure_restored_from_drive(latest_cache_file, is_directory=False):\n",
    "    # Cache exists or was restored - load it\n",
    "    if training_cache is None:\n",
    "        from shared.json_cache import load_json\n",
    "        training_cache = load_json(latest_cache_file, default=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\checkpoint\n",
      "✓ Computed conversion fingerprints:\n",
      "✓ Conversion output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\conversion\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\conv_7f8c58314bae731e\n",
      "Conversion not yet completed - will proceed with conversion\n"
     ]
    }
   ],
   "source": [
    "# Extract checkpoint directory, backbone, and create conversion output directory\n",
    "from datetime import datetime\n",
    "\n",
    "# Get checkpoint directory from training cache\n",
    "checkpoint_source = Path(training_cache.get(\"output_dir\", \"\")) / \"checkpoint\"\n",
    "if not checkpoint_source.exists():\n",
    "    # Try alternative location\n",
    "    checkpoint_source = Path(training_cache.get(\"output_dir\", \"\")) / CHECKPOINT_DIRNAME\n",
    "    if not checkpoint_source.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found in training cache output_dir: {training_cache.get('output_dir', '')}\"\n",
    "        )\n",
    "\n",
    "checkpoint_dir = checkpoint_source\n",
    "\n",
    "# Restore checkpoint from Drive if missing\n",
    "if not checkpoint_dir.exists():\n",
    "    # Try restoring checkpoint directory\n",
    "    if ensure_restored_from_drive(checkpoint_dir, is_directory=True):\n",
    "        print(f\"Restored checkpoint from Drive: {checkpoint_dir}\")\n",
    "    else:\n",
    "        # Try restoring entire output directory\n",
    "        output_parent = checkpoint_dir.parent.parent\n",
    "        if ensure_restored_from_drive(output_parent, is_directory=True):\n",
    "            print(f\"Restored output directory from Drive: {output_parent}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Checkpoint not found locally or in Drive: {checkpoint_dir}\\n\"\n",
    "                f\"Please ensure final training completed successfully.\"\n",
    "            )\n",
    "\n",
    "print(f\"Using checkpoint: {checkpoint_dir}\")\n",
    "\n",
    "# Extract backbone from training cache\n",
    "backbone = training_cache.get(\"backbone\", \"unknown\")\n",
    "if backbone == \"unknown\":\n",
    "    # Try to get from config\n",
    "    backbone = training_cache.get(\"config\", {}).get(\"backbone\", \"unknown\")\n",
    "    if backbone == \"unknown\":\n",
    "        raise ValueError(\"Could not determine backbone from training cache\")\n",
    "\n",
    "# Extract backbone name (e.g., \"distilbert\" from \"distilbert-base-uncased\")\n",
    "backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "\n",
    "# Use new centralized naming system for conversion\n",
    "# Build parent_training_id from training context (if available) or from cache\n",
    "if 'training_context' in locals():\n",
    "    from orchestration.naming_centralized import build_parent_training_id\n",
    "    parent_training_id = build_parent_training_id(\n",
    "        spec_fp=training_context.spec_fp,\n",
    "        exec_fp=training_context.exec_fp,\n",
    "        variant=training_context.variant\n",
    "    )\n",
    "else:\n",
    "    # Fallback: construct from training cache or use directory name\n",
    "    training_output_dir = Path(training_cache.get(\"output_dir\", \"\"))\n",
    "    # Try to extract from path: outputs/final_training/<env>/<model>/spec_<spec_fp>_exec_<exec_fp>/v<variant>\n",
    "    parent_training_id = training_output_dir.name if training_output_dir.exists() else None\n",
    "    if not parent_training_id or not parent_training_id.startswith(\"spec_\"):\n",
    "        # Fallback to old naming\n",
    "        if 'stable_training_name' in locals():\n",
    "            parent_training_id = stable_training_name\n",
    "        else:\n",
    "            parent_training_id = f\"{backbone_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Create conversion context with fingerprints\n",
    "from orchestration.naming_centralized import create_naming_context, build_output_path\n",
    "from orchestration.fingerprints import compute_conv_fp\n",
    "\n",
    "# Get conversion config (defaults if not specified)\n",
    "conversion_config = {\n",
    "    \"opset\": 18,  # Default from conversion script\n",
    "    \"quantization\": \"int8\",\n",
    "    \"dynamic_axes\": True,\n",
    "}\n",
    "\n",
    "# Compute conv_fp (requires parent spec_fp and exec_fp)\n",
    "# Try to get from training context, otherwise compute from configs\n",
    "if 'training_context' in locals():\n",
    "    parent_spec_fp = training_context.spec_fp\n",
    "    parent_exec_fp = training_context.exec_fp\n",
    "else:\n",
    "    # Compute from configs\n",
    "    from orchestration.config_loader import load_all_configs\n",
    "    all_configs = load_all_configs(experiment_config)\n",
    "    parent_spec_fp = compute_spec_fp(\n",
    "        model_config=all_configs.get(\"model\", {}),\n",
    "        data_config=all_configs.get(\"data\", {}),\n",
    "        train_config=all_configs.get(\"train\", {}),\n",
    "        seed=training_cache.get(\"config\", {}).get(\"random_seed\", 42)\n",
    "    )\n",
    "    try:\n",
    "        git_sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"], \n",
    "            cwd=ROOT_DIR, \n",
    "            stderr=subprocess.DEVNULL\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        git_sha = \"unknown\"\n",
    "    parent_exec_fp = compute_exec_fp(\n",
    "        git_sha=git_sha,\n",
    "        env_config=all_configs.get(\"env\", {})\n",
    "    )\n",
    "\n",
    "conv_fp = compute_conv_fp(\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    conversion_config=conversion_config\n",
    ")\n",
    "\n",
    "print(f\"✓ Computed conversion fingerprints:\")\n",
    "\n",
    "# Ensure environment is defined\n",
    "if 'environment' not in locals():\n",
    "    from shared.platform_detection import detect_platform\n",
    "    environment = detect_platform()\n",
    "\n",
    "# Create conversion context\n",
    "conversion_context = create_naming_context(\n",
    "    process_type=\"conversion\",\n",
    "    model=backbone_name,\n",
    "    environment=environment,\n",
    "    parent_training_id=parent_training_id,\n",
    "    conv_fp=conv_fp\n",
    ")\n",
    "\n",
    "conversion_output_dir = build_output_path(ROOT_DIR, conversion_context)\n",
    "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Conversion output directory: {conversion_output_dir}\")\n",
    "\n",
    "# Check if conversion already exists\n",
    "onnx_model_path = conversion_output_dir / \"model_int8.onnx\"\n",
    "if not onnx_model_path.exists():\n",
    "    onnx_model_path = conversion_output_dir / \"model.onnx\"\n",
    "\n",
    "conversion_complete = onnx_model_path.exists()\n",
    "\n",
    "stable_training_name = f\"{backbone_name}_{training_cache.get('config', {}).get('trial_name', 'unknown')}\"\n",
    "\n",
    "if conversion_complete:\n",
    "    print(f\"✓ Conversion already completed at: {conversion_output_dir}\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    # Check if artifacts are already uploaded\n",
    "    try:\n",
    "        from orchestration.metadata_manager import are_conversion_artifacts_uploaded\n",
    "        artifacts_uploaded = are_conversion_artifacts_uploaded(\n",
    "            ROOT_DIR, CONFIG_DIR, stable_training_name\n",
    "        )\n",
    "        if artifacts_uploaded:\n",
    "            print(f\"  ✓ Artifacts already uploaded to MLflow\")\n",
    "            SKIP_CONVERSION = True\n",
    "        else:\n",
    "            print(f\"  ⚠ Artifacts not yet uploaded - will upload after loading ONNX model\")\n",
    "            SKIP_CONVERSION = True  # Conversion done, just need to upload\n",
    "    except Exception:\n",
    "        SKIP_CONVERSION = True  # Conversion done, upload status unknown\n",
    "else:\n",
    "    print(f\"Conversion not yet completed - will proceed with conversion\")\n",
    "    SKIP_CONVERSION = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversion as a module (python -m model_conversion.convert_to_onnx) to allow relative imports to work\n",
    "# This requires src/ to be in PYTHONPATH (set in env below)\n",
    "conversion_args = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"model_conversion.convert_to_onnx\",\n",
    "    \"--checkpoint-path\",\n",
    "    str(checkpoint_dir),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    backbone,\n",
    "    \"--output-dir\",\n",
    "    str(conversion_output_dir),\n",
    "    \"--quantize-int8\",\n",
    "    \"--run-smoke-test\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_env = os.environ.copy()\n",
    "conversion_env[\"AZURE_ML_OUTPUT_onnx_model\"] = str(conversion_output_dir)\n",
    "\n",
    "# Add src directory to PYTHONPATH to allow relative imports in model_conversion.convert_to_onnx\n",
    "pythonpath = conversion_env.get(\"PYTHONPATH\", \"\")\n",
    "if pythonpath:\n",
    "    conversion_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "else:\n",
    "    conversion_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    conversion_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 19:03:57,048 - orchestration.jobs.tracking.mlflow_tracker - WARNING - Failed to upload model_int8.onnx: ('Connection aborted.', TimeoutError('The write operation timed out'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged conversion artifacts to MLflow run 2286bf78-a801-4408-a525-6647f42e7839\n",
      "🏃 View run distilbert_conversion_20251230_190044 at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/713fa6a7-15e4-4b23-bf15-6f6ca0242b78/runs/2286bf78-a801-4408-a525-6647f42e7839\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/713fa6a7-15e4-4b23-bf15-6f6ca0242b78\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(\n",
    "    conversion_args,\n",
    "    cwd=ROOT_DIR,\n",
    "    env=conversion_env,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"Model conversion failed with the following output:\")\n",
    "    print(\"=\" * 80)\n",
    "    if result.stdout:\n",
    "        print(\"STDOUT:\")\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "    print(\"=\" * 80)\n",
    "    raise RuntimeError(f\"Model conversion failed with return code {result.returncode}\")\n",
    "else:\n",
    "\n",
    "    # Log conversion artifacts to MLflow\n",
    "    if mlflow_tracking_uri:\n",
    "        try:\n",
    "            from orchestration.jobs.tracking.mlflow_tracker import MLflowConversionTracker\n",
    "            \n",
    "            # Get MLflow experiment name for conversion\n",
    "            conversion_experiment_name = f\"{experiment_config.name}-conversion\"\n",
    "            conversion_tracker = MLflowConversionTracker(conversion_experiment_name)\n",
    "            \n",
    "            # Get ONNX model path\n",
    "            onnx_model_path = conversion_output_dir / \"model_int8.onnx\"\n",
    "            if not onnx_model_path.exists():\n",
    "                onnx_model_path = conversion_output_dir / \"model.onnx\"\n",
    "            \n",
    "            # Get source training run ID (if available)\n",
    "            source_training_run_id = None\n",
    "            try:\n",
    "                # Try to get from training cache\n",
    "                training_cache = load_cache_file(\n",
    "                    ROOT_DIR, CONFIG_DIR, \"final_training\", use_latest=True\n",
    "                )\n",
    "                if training_cache and \"mlflow_run_id\" in training_cache:\n",
    "                    source_training_run_id = training_cache[\"mlflow_run_id\"]\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            import mlflow\n",
    "            from mlflow.tracking import MlflowClient\n",
    "            \n",
    "            # Create or find MLflow run for conversion\n",
    "            # Since conversion runs via subprocess, we need to create a run here\n",
    "            mlflow.set_experiment(conversion_experiment_name)\n",
    "            \n",
    "            # Create a new run for this conversion\n",
    "            \n",
    "            # Use systematic naming if conversion_context is available\n",
    "            from orchestration.jobs.tracking.naming.run_names import build_mlflow_run_name\n",
    "            \n",
    "            if 'conversion_context' in locals():\n",
    "                run_name = build_mlflow_run_name(\n",
    "                    conversion_context, \n",
    "                    config_dir=CONFIG_DIR,\n",
    "                    root_dir=ROOT_DIR,\n",
    "                    output_dir=conversion_output_dir\n",
    "                )\n",
    "            else:\n",
    "                # Fallback to manual construction if context not available\n",
    "                from datetime import datetime\n",
    "                conversion_run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                run_name = f\"{backbone}_conversion_{conversion_run_id}\"\n",
    "            with mlflow.start_run(run_name=run_name) as conversion_run:\n",
    "                # Set tags\n",
    "                mlflow.set_tag(\"conversion_type\", \"onnx_int8\" if \"int8\" in str(onnx_model_path) else \"onnx_fp32\")\n",
    "                if source_training_run_id:\n",
    "                    mlflow.set_tag(\"source_training_run\", source_training_run_id)\n",
    "                \n",
    "                # Log conversion parameters\n",
    "                conversion_tracker.log_conversion_parameters(\n",
    "                    checkpoint_path=str(checkpoint_dir),\n",
    "                    conversion_target=\"onnx_int8\" if \"int8\" in str(onnx_model_path) else \"onnx_fp32\",\n",
    "                    quantization=\"int8\" if \"int8\" in str(onnx_model_path) else \"none\",\n",
    "                    opset_version=18,  # Hardcoded in conversion script\n",
    "                    backbone=backbone,\n",
    "                )\n",
    "                \n",
    "                # Log artifacts if ONNX model exists\n",
    "                if onnx_model_path.exists():\n",
    "                    # Calculate compression ratio if possible\n",
    "                    original_checkpoint_size_mb = None\n",
    "                    try:\n",
    "                        import os\n",
    "                        total_size = sum(f.stat().st_size for f in checkpoint_dir.rglob('*') if f.is_file())\n",
    "                        original_checkpoint_size_mb = total_size / (1024 * 1024)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    # Log conversion results\n",
    "                    conversion_tracker.log_conversion_results(\n",
    "                        conversion_success=True,\n",
    "                        onnx_model_path=onnx_model_path,\n",
    "                        original_checkpoint_size=original_checkpoint_size_mb,\n",
    "                        smoke_test_passed=None,  # Could parse from output if needed\n",
    "                        conversion_log_path=None,  # Could add if conversion script logs to file\n",
    "                    )\n",
    "                    print(f\"✓ Logged conversion artifacts to MLflow run {conversion_run.info.run_id}\")\n",
    "                else:\n",
    "                    print(f\"⚠ ONNX model not found: {onnx_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Failed to log conversion artifacts to MLflow: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Conversion completed. ONNX model: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\conversion\\local\\distilbert\\spec_81710c3324325ad0_exec_8d244347b2eff67e\\v1\\conv_7f8c58314bae731e\\model_int8.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared.json_cache import save_json\n",
    "import shutil\n",
    "\n",
    "ONNX_MODEL_FILENAME = \"model_int8.onnx\"\n",
    "FALLBACK_ONNX_MODEL_FILENAME = \"model.onnx\"\n",
    "\n",
    "onnx_model_path = conversion_output_dir / ONNX_MODEL_FILENAME\n",
    "if not onnx_model_path.exists():\n",
    "    onnx_model_path = conversion_output_dir / FALLBACK_ONNX_MODEL_FILENAME\n",
    "\n",
    "if not onnx_model_path.exists():\n",
    "    raise FileNotFoundError(f\"ONNX model not found in {conversion_output_dir}\")\n",
    "\n",
    "print(f\"✓ Conversion completed. ONNX model: {onnx_model_path}\")\n",
    "\n",
    "# Backup ONNX model to Google Drive (mirrors local structure)\n",
    "if onnx_model_path.exists():\n",
    "    backup_to_drive(onnx_model_path, is_directory=False)\n",
    "else:\n",
    "    print(f\"⚠ Warning: ONNX model not found for backup: {onnx_model_path}\")\n",
    "\n",
    "# Backup entire conversion output directory to Drive\n",
    "backup_to_drive(conversion_output_dir, is_directory=True)\n",
    "\n",
    "# Backup conversion cache file to Drive (if it's in outputs/, otherwise skip)\n",
    "# If you want to backup cache files, move them to outputs/cache/ or backup manually\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}