{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Local Training Orchestration (Google Colab Compatible)\n",
    "\n",
    "This notebook orchestrates all training activities for **local execution** with Google Colab GPU compute support.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Step 1**: Load Centralized Configs\n",
    "- **Step 2**: Verify Local Dataset (from data config)\n",
    "- **Step 3**: Setup Local Environment\n",
    "- **Step 4**: The Dry Run\n",
    "- **Step 5**: The Sweep (HPO) - Local with Optuna\n",
    "- **Step 5.5**: Benchmarking Best Trials (NEW)\n",
    "- **Step 6**: Best Configuration Selection (Automated)\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **executes training locally** (not on Azure ML)\n",
    "- All computation happens on the local machine or Google Colab GPU\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository (no external downloads needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a local Conda environment\n",
    "\n",
    "\n",
    "### 1. Open a terminal in the project root\n",
    "\n",
    "In PowerShell:\n",
    "\n",
    "```powershell\n",
    "cd \"C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\"\n",
    "```\n",
    "\n",
    "### 2. Create the Conda environment from the project’s `conda.yaml`\n",
    "\n",
    "```powershell\n",
    "conda env create -f config\\environment\\conda.yaml\n",
    "```\n",
    "\n",
    "- This will create an environment named `resume-ner-training` (from the `name:` field in the YAML).\n",
    "- It installs Python 3.10, PyTorch, transformers, Azure ML SDK, etc.\n",
    "\n",
    "If Conda says the env already exists, use:\n",
    "\n",
    "```powershell\n",
    "conda env update -f config\\environment\\conda.yaml\n",
    "```\n",
    "\n",
    "### 3. Activate the environment\n",
    "\n",
    "```powershell\n",
    "conda activate resume-ner-training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "\n",
    "# Assume this notebook lives in `notebooks/` under the project root\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
    "print(\"Project root:\", ROOT_DIR)\n",
    "print(\"Source directory:\", SRC_DIR)\n",
    "print(\"Config directory:\", CONFIG_DIR)\n",
    "print(\"In Colab:\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from orchestration.config_loader import (\n",
    "    ExperimentConfig,\n",
    "    compute_config_hashes,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "# P1-3.1: Load Centralized Configs (local-only)\n",
    "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
    "\n",
    "if not CONFIG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
    "\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n",
    "validate_config_immutability(configs, original_configs)\n",
    "\n",
    "print(f\"Loaded experiment: {experiment_config.name}\")\n",
    "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
    "print(\"Config hashes:\", config_hashes)\n",
    "print(\"Config metadata:\", config_metadata)\n",
    "\n",
    "# Get dataset path from data config (centralized configuration)\n",
    "# The local_path in the data config is relative to the config directory\n",
    "data_config = configs[\"data\"]\n",
    "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
    "\n",
    "# Check if seed-based dataset structure (for dataset_tiny with seed subdirectories)\n",
    "seed = data_config.get(\"seed\")\n",
    "if seed is not None and \"dataset_tiny\" in str(DATASET_LOCAL_PATH):\n",
    "    DATASET_LOCAL_PATH = DATASET_LOCAL_PATH / f\"seed{seed}\"\n",
    "\n",
    "\n",
    "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")\n",
    "if seed is not None:\n",
    "    print(f\"Using seed: {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1.1: Define Constants\n",
    "\n",
    "Define constants for file and directory names used throughout the notebook. Benchmark settings come from centralized config, not hard-coded here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import constants from centralized module\n",
    "from orchestration import (\n",
    "    METRICS_FILENAME,\n",
    "    BENCHMARK_FILENAME,\n",
    "    CHECKPOINT_DIRNAME,\n",
    "    OUTPUTS_DIRNAME,\n",
    "    MLRUNS_DIRNAME,\n",
    "    DEFAULT_RANDOM_SEED,\n",
    "    DEFAULT_K_FOLDS,\n",
    ")\n",
    "\n",
    "# Note: Benchmark settings (batch_sizes, iterations, etc.) come from configs[\"benchmark\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1.2: Define Helper Functions\n",
    "\n",
    "Reusable helper functions following DRY principle for common operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from consolidated modules (DRY principle)\n",
    "from typing import List, Optional\n",
    "from orchestration import (\n",
    "    build_mlflow_experiment_name,\n",
    "    setup_mlflow_for_stage,\n",
    "    run_benchmarking,\n",
    ")\n",
    "from shared import verify_output_file\n",
    "\n",
    "# Wrapper function for run_benchmarking that uses notebook-specific paths\n",
    "def run_benchmarking_local(\n",
    "    checkpoint_dir: Path,\n",
    "    test_data_path: Path,\n",
    "    output_path: Path,\n",
    "    batch_sizes: List[int],\n",
    "    iterations: int,\n",
    "    warmup_iterations: int,\n",
    "    max_length: int = 512,\n",
    "    device: Optional[str] = None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Run benchmarking on a model checkpoint (local notebook wrapper).\n",
    "    \n",
    "    This is a thin wrapper around orchestration.benchmark_utils.run_benchmarking\n",
    "    that automatically uses the notebook's SRC_DIR and ROOT_DIR.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Path to checkpoint directory.\n",
    "        test_data_path: Path to test data JSON file.\n",
    "        output_path: Path to output benchmark.json file.\n",
    "        batch_sizes: List of batch sizes to test.\n",
    "        iterations: Number of iterations per batch size.\n",
    "        warmup_iterations: Number of warmup iterations.\n",
    "        max_length: Maximum sequence length.\n",
    "        device: Device to use (None = auto-detect).\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    return run_benchmarking(\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        test_data_path=test_data_path,\n",
    "        output_path=output_path,\n",
    "        batch_sizes=batch_sizes,\n",
    "        iterations=iterations,\n",
    "        warmup_iterations=warmup_iterations,\n",
    "        max_length=max_length,\n",
    "        device=device,\n",
    "        project_root=ROOT_DIR,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Verify Local Dataset\n",
    "\n",
    "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1-3.2: Verify Local Dataset\n",
    "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
    "# This ensures the dataset location is controlled by centralized configuration.\n",
    "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
    "\n",
    "REQUIRED_FILE = \"train.json\"\n",
    "OPTIONAL_FILE = \"validation.json\"\n",
    "\n",
    "if not DATASET_LOCAL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check required file\n",
    "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
    "if not train_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required dataset file not found: {train_file}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check optional file\n",
    "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
    "has_validation = val_file.exists()\n",
    "\n",
    "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
    "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
    "\n",
    "train_size = train_file.stat().st_size\n",
    "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
    "\n",
    "if has_validation:\n",
    "    val_size = val_file.stat().st_size\n",
    "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2.1: Optional Train/Test Split\n",
    "\n",
    "**Optional step**: Create a train/test split if `test.json` is missing. This is useful when you only have `train.json` and `validation.json` and want to create a separate test set.\n",
    "\n",
    "**⚠ WARNING**: This will overwrite `train.json` with the split version. Only enable if you want to create a permanent train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: create train/test split if test.json is missing\n",
    "# WARNING: This will overwrite train.json with the split version\n",
    "# Only enable if you want to create a permanent train/test split\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Try to import from training.data, with fallback inline definitions\n",
    "try:\n",
    "    from training.data import split_train_test, save_split_files\n",
    "except ImportError:\n",
    "    # Fallback: define functions inline if import fails (e.g., missing dependencies)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    def split_train_test(\n",
    "        dataset: List[Dict[str, Any]],\n",
    "        train_ratio: float = 0.8,\n",
    "        stratified: bool = False,\n",
    "        random_seed: int = 42,\n",
    "        entity_types: Optional[List[str]] = None,\n",
    "    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "        \"\"\"Split dataset into train and test sets.\"\"\"\n",
    "        if not 0 < train_ratio < 1:\n",
    "            raise ValueError(\"train_ratio must be between 0 and 1 (exclusive).\")\n",
    "        \n",
    "        test_size = 1.0 - train_ratio\n",
    "        train_data, test_data = train_test_split(\n",
    "            dataset,\n",
    "            test_size=test_size,\n",
    "            random_state=random_seed,\n",
    "            shuffle=True,\n",
    "            stratify=None,  # Simplified version without stratification\n",
    "        )\n",
    "        return list(train_data), list(test_data)\n",
    "    \n",
    "    def save_split_files(\n",
    "        output_dir: Path,\n",
    "        train_data: List[Dict[str, Any]],\n",
    "        test_data: List[Dict[str, Any]],\n",
    "    ) -> None:\n",
    "        \"\"\"Persist train/test splits to disk.\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_dir / \"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        with open(output_dir / \"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "CREATE_TEST_SPLIT = False  # Set True to create test.json when absent (WARNING: overwrites train.json)\n",
    "\n",
    "train_file = DATASET_LOCAL_PATH / \"train.json\"\n",
    "val_file = DATASET_LOCAL_PATH / \"validation.json\"\n",
    "test_file = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if CREATE_TEST_SPLIT and not test_file.exists():\n",
    "    # Backup original train.json before overwriting\n",
    "    backup_file = DATASET_LOCAL_PATH / \"train.json.backup\"\n",
    "    if train_file.exists() and not backup_file.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(train_file, backup_file)\n",
    "        print(f\"⚠ Backed up original train.json to {backup_file}\")\n",
    "    \n",
    "    full_dataset = []\n",
    "    # Start with train data; optionally include validation to maximize coverage\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_dataset.extend(json.load(f))\n",
    "    if val_file.exists():\n",
    "        with open(val_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_dataset.extend(json.load(f))\n",
    "\n",
    "    split_cfg = configs.get(\"data\", {}).get(\"splitting\", {})\n",
    "    train_ratio = split_cfg.get(\"train_test_ratio\", 0.8)\n",
    "    stratified = split_cfg.get(\"stratified\", False)\n",
    "    random_seed = split_cfg.get(\"random_seed\", 42)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "\n",
    "    print(f\"Creating train/test split (train_ratio={train_ratio}, stratified={stratified})...\")\n",
    "    print(f\"⚠ WARNING: This will overwrite train.json with {int(len(full_dataset) * train_ratio)} samples\")\n",
    "    \n",
    "    new_train, new_test = split_train_test(\n",
    "        dataset=full_dataset,\n",
    "        train_ratio=train_ratio,\n",
    "        stratified=stratified,\n",
    "        random_seed=random_seed,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "\n",
    "    save_split_files(DATASET_LOCAL_PATH, new_train, new_test)\n",
    "    print(f\"✓ Wrote train.json ({len(new_train)}) and test.json ({len(new_test)})\")\n",
    "elif test_file.exists():\n",
    "    print(f\"✓ Found existing test.json at {test_file}\")\n",
    "else:\n",
    "    print(\"⚠ test.json not found. Set CREATE_TEST_SPLIT=True to generate a split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Setup Local Environment\n",
    "\n",
    "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\"\n",
    "\n",
    "env_config = configs[\"env\"]\n",
    "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
    "\n",
    "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"CUDA device requested but not available. \"\n",
    "        \"Set device to 'cpu' in env config or ensure CUDA is properly installed.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "MLFLOW_DIR = \"mlruns\"\n",
    "mlflow_tracking_path = ROOT_DIR / MLFLOW_DIR\n",
    "mlflow_tracking_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Convert Windows path to file:// URI format for MLflow\n",
    "mlflow_tracking_uri = mlflow_tracking_path.as_uri()\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "    import optuna\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Required package not installed: {e}\")\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"torch\": torch,\n",
    "    \"transformers\": transformers,\n",
    "    \"mlflow\": mlflow,\n",
    "    \"optuna\": optuna,\n",
    "}\n",
    "\n",
    "for name, module in REQUIRED_PACKAGES.items():\n",
    "    if not hasattr(module, \"__version__\"):\n",
    "        raise ImportError(f\"Required package '{name}' is not properly installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Dry Run\n",
    "\n",
    "Run a minimal HPO sweep to validate the training pipeline works correctly before launching the full HPO sweep. Uses the smoke HPO configuration with reduced trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from orchestration import STAGE_SMOKE\n",
    "from orchestration.jobs.local_sweeps import run_local_hpo_sweep\n",
    "\n",
    "TRAINING_SCRIPT_PATH = SRC_DIR / \"train.py\"\n",
    "DRY_RUN_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"dry_run\"\n",
    "\n",
    "if not TRAINING_SCRIPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Training script not found: {TRAINING_SCRIPT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs for dry run (using smoke HPO config from experiment stages)\n",
    "hpo_config = configs[\"hpo\"]\n",
    "train_config = configs[\"train\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "\n",
    "dry_run_studies = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
    "        experiment_config.name, STAGE_SMOKE, backbone\n",
    "    )\n",
    "    backbone_output_dir = DRY_RUN_OUTPUT_DIR / backbone\n",
    "    \n",
    "    study = run_local_hpo_sweep(\n",
    "        dataset_path=str(DATASET_LOCAL_PATH),\n",
    "        config_dir=CONFIG_DIR,\n",
    "        backbone=backbone,\n",
    "        hpo_config=hpo_config,\n",
    "        train_config=train_config,\n",
    "        output_dir=backbone_output_dir,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "    )\n",
    "    \n",
    "    dry_run_studies[backbone] = study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use hpo_config from configs (loaded in cell 17 for dry run)\n",
    "dry_run_hpo_config = configs[\"hpo\"]\n",
    "objective_metric = dry_run_hpo_config['objective']['metric']\n",
    "\n",
    "for backbone, study in dry_run_studies.items():\n",
    "    if study.trials:\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
    "        print(f\"  Best {objective_metric}: {best_trial.value:.4f}\")\n",
    "        print(f\"  Best params: {best_trial.params}\")\n",
    "    else:\n",
    "        print(f\"{backbone}: No trials completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5.1: The Sweep (HPO) - Local with Optuna\n",
    "\n",
    "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n",
    "\n",
    "**Note on K-Fold Cross-Validation:**\n",
    "- When k-fold CV is enabled (`k_fold.enabled: true`), each trial trains **k models** (one per fold) and returns the **average metric** across folds\n",
    "- The number of **trials** is controlled by `sampling.max_trials` (e.g., 2 trials in smoke.yaml)\n",
    "- With k=5 folds and 2 trials: **2 trials × 5 folds = 10 model trainings total**\n",
    "- K-fold CV provides more robust hyperparameter evaluation but increases compute time (k× per trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from orchestration import STAGE_HPO\n",
    "from shared.yaml_utils import load_yaml\n",
    "from orchestration.jobs.local_sweeps import run_local_hpo_sweep\n",
    "\n",
    "# Use constants defined in Step P1-3.1.1 (DEFAULT_K_FOLDS, DEFAULT_RANDOM_SEED)\n",
    "HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "\n",
    "hpo_stage_config = experiment_config.stages.get(STAGE_HPO, {})\n",
    "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
    "hpo_config_path = CONFIG_DIR / hpo_config_override if hpo_config_override else experiment_config.hpo_config\n",
    "\n",
    "if not hpo_config_path.exists():\n",
    "    raise FileNotFoundError(f\"HPO config not found: {hpo_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_config = load_yaml(hpo_config_path)\n",
    "train_config = configs[\"train\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.cv_utils import create_kfold_splits, save_fold_splits, validate_splits\n",
    "from training.data import load_dataset\n",
    "\n",
    "k_fold_config = hpo_config.get(\"k_fold\", {})\n",
    "k_folds_enabled = k_fold_config.get(\"enabled\", False)\n",
    "fold_splits_file = None\n",
    "\n",
    "if k_folds_enabled:\n",
    "    n_splits = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS)\n",
    "    random_seed = k_fold_config.get(\"random_seed\", DEFAULT_RANDOM_SEED)\n",
    "    shuffle = k_fold_config.get(\"shuffle\", True)\n",
    "    stratified = k_fold_config.get(\"stratified\", False)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "    \n",
    "    full_dataset = load_dataset(str(DATASET_LOCAL_PATH))\n",
    "    train_data = full_dataset.get(\"train\", [])\n",
    "    \n",
    "    fold_splits = create_kfold_splits(\n",
    "        dataset=train_data,\n",
    "        k=n_splits,\n",
    "        random_seed=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratified=stratified,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "    \n",
    "    # Optional validation to ensure rare entities appear across folds\n",
    "    validate_splits(train_data, fold_splits, entity_types=entity_types)\n",
    "    \n",
    "    fold_splits_file = HPO_OUTPUT_DIR / \"fold_splits.json\"\n",
    "    save_fold_splits(\n",
    "        fold_splits,\n",
    "        fold_splits_file,\n",
    "        metadata={\n",
    "            \"k\": n_splits,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"shuffle\": shuffle,\n",
    "            \"stratified\": stratified,\n",
    "            \"dataset_path\": str(DATASET_LOCAL_PATH),\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_studies = {}\n",
    "k_folds_param = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS) if k_folds_enabled else None\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
    "        experiment_config.name, STAGE_HPO, backbone\n",
    "    )\n",
    "    backbone_output_dir = HPO_OUTPUT_DIR / backbone\n",
    "    \n",
    "    study = run_local_hpo_sweep(\n",
    "        dataset_path=str(DATASET_LOCAL_PATH),\n",
    "        config_dir=CONFIG_DIR,\n",
    "        backbone=backbone,\n",
    "        hpo_config=hpo_config,\n",
    "        train_config=train_config,\n",
    "        output_dir=backbone_output_dir,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        k_folds=k_folds_param,\n",
    "        fold_splits_file=fold_splits_file,\n",
    "    )\n",
    "    \n",
    "    hpo_studies[backbone] = study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cv_statistics(best_trial):\n",
    "    if not hasattr(best_trial, \"user_attrs\"):\n",
    "        return None\n",
    "    cv_mean = best_trial.user_attrs.get(\"cv_mean\")\n",
    "    cv_std = best_trial.user_attrs.get(\"cv_std\")\n",
    "    return (cv_mean, cv_std) if cv_mean is not None else None\n",
    "\n",
    "objective_metric = hpo_config['objective']['metric']\n",
    "\n",
    "for backbone, study in hpo_studies.items():\n",
    "    if not study.trials:\n",
    "        continue\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    cv_stats = extract_cv_statistics(best_trial)\n",
    "    \n",
    "    print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
    "    print(f\"  Best {objective_metric}: {best_trial.value:.4f}\")\n",
    "    print(f\"  Best params: {best_trial.params}\")\n",
    "    \n",
    "    if cv_stats:\n",
    "        cv_mean, cv_std = cv_stats\n",
    "        print(f\"  CV Statistics: Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5.2: Benchmarking Best Trials\n",
    "\n",
    "Benchmark the best trial from each backbone to measure actual inference performance. This provides real latency data that replaces parameter-count proxies in model selection, enabling more accurate speed comparisons.\n",
    "\n",
    "**Workflow:**\n",
    "1. Identify best trial per backbone (from HPO results)\n",
    "2. Run benchmarking on each best trial checkpoint\n",
    "3. Save benchmark results as `benchmark.json` in trial directories\n",
    "4. Model selection will automatically use this data when available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs.local_selection import load_best_trial_from_disk\n",
    "import json\n",
    "\n",
    "# Load benchmark config (if available)\n",
    "benchmark_config = configs.get(\"benchmark\", {})\n",
    "benchmark_settings = benchmark_config.get(\"benchmarking\", {})\n",
    "\n",
    "# Get benchmark parameters from config or use defaults\n",
    "benchmark_batch_sizes = benchmark_settings.get(\"batch_sizes\", [1, 8, 16])\n",
    "benchmark_iterations = benchmark_settings.get(\"iterations\", 100)\n",
    "benchmark_warmup = benchmark_settings.get(\"warmup_iterations\", 10)\n",
    "benchmark_max_length = benchmark_settings.get(\"max_length\", 512)\n",
    "benchmark_device = benchmark_settings.get(\"device\")\n",
    "\n",
    "# Get test data path from benchmark config or data config\n",
    "test_data_path_str = benchmark_settings.get(\"test_data\")\n",
    "if test_data_path_str:\n",
    "    test_data_path = (CONFIG_DIR / test_data_path_str).resolve()\n",
    "else:\n",
    "    # Fallback to dataset directory\n",
    "    test_data_path = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if not test_data_path.exists():\n",
    "    print(f\"Warning: Test data not found at {test_data_path}\")\n",
    "    print(\"Benchmarking will be skipped. Model selection will use parameter proxy.\")\n",
    "    test_data_path = None\n",
    "\n",
    "# Identify best trials per backbone\n",
    "objective_metric = hpo_config[\"objective\"][\"metric\"]\n",
    "best_trials = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    best_trial_info = load_best_trial_from_disk(\n",
    "        HPO_OUTPUT_DIR,\n",
    "        backbone,\n",
    "        objective_metric\n",
    "    )\n",
    "    if best_trial_info:\n",
    "        best_trials[backbone] = best_trial_info\n",
    "        print(f\"{backbone}: Best trial is {best_trial_info['trial_name']} \"\n",
    "              f\"({objective_metric}={best_trial_info['accuracy']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarking on best trials\n",
    "if test_data_path and test_data_path.exists():\n",
    "    benchmark_results = {}\n",
    "    \n",
    "    for backbone, trial_info in best_trials.items():\n",
    "        trial_dir = Path(trial_info[\"trial_dir\"])\n",
    "        checkpoint_dir = trial_dir / CHECKPOINT_DIRNAME\n",
    "        benchmark_output = trial_dir / BENCHMARK_FILENAME\n",
    "        \n",
    "        if not checkpoint_dir.exists():\n",
    "            print(f\"Warning: Checkpoint not found for {backbone} {trial_info['trial_name']}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nBenchmarking {backbone} ({trial_info['trial_name']})...\")\n",
    "        \n",
    "        success = run_benchmarking(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            test_data_path=test_data_path,\n",
    "            output_path=benchmark_output,\n",
    "            batch_sizes=benchmark_batch_sizes,\n",
    "            iterations=benchmark_iterations,\n",
    "            warmup_iterations=benchmark_warmup,\n",
    "            max_length=benchmark_max_length,\n",
    "            device=benchmark_device,\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            benchmark_results[backbone] = benchmark_output\n",
    "            print(f\"✓ Benchmark completed: {benchmark_output}\")\n",
    "        else:\n",
    "            print(f\"✗ Benchmark failed for {backbone}\")\n",
    "    \n",
    "    print(f\"\\n✓ Benchmarking complete. {len(benchmark_results)}/{len(best_trials)} trials benchmarked.\")\n",
    "else:\n",
    "    print(\"Skipping benchmarking (test data not available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify benchmark files were created\n",
    "if test_data_path and test_data_path.exists():\n",
    "    for backbone, trial_info in best_trials.items():\n",
    "        trial_dir = Path(trial_info[\"trial_dir\"])\n",
    "        benchmark_file = trial_dir / BENCHMARK_FILENAME\n",
    "        \n",
    "        if benchmark_file.exists():\n",
    "            with open(benchmark_file, \"r\") as f:\n",
    "                benchmark_data = json.load(f)\n",
    "            batch_1_latency = benchmark_data.get(\"batch_1\", {}).get(\"mean_ms\", \"N/A\")\n",
    "            print(f\"{backbone}: benchmark.json exists (batch_1 latency: {batch_1_latency} ms)\")\n",
    "        else:\n",
    "            print(f\"{backbone}: benchmark.json not found (will use parameter proxy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models. The best configuration is determined by the objective metric specified in the HPO config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shared.json_cache import save_json\n",
    "from orchestration.jobs.local_selection import select_best_configuration_across_studies\n",
    "\n",
    "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = data_config.get(\"version\", \"unknown\")\n",
    "\n",
    "# Select best configuration - supports both in-memory studies and disk-based selection\n",
    "# hpo_output_dir enables loading benchmark data if available\n",
    "best_configuration = select_best_configuration_across_studies(\n",
    "    studies=hpo_studies if hpo_studies else None,\n",
    "    hpo_config=hpo_config,\n",
    "    dataset_version=dataset_version,\n",
    "    hpo_output_dir=HPO_OUTPUT_DIR,  # Enable benchmark data loading\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(BEST_CONFIG_CACHE_FILE, best_configuration)\n",
    "\n",
    "# Display comprehensive selection summary with all metrics\n",
    "selection_criteria = best_configuration.get('selection_criteria', {})\n",
    "selected_backbone = best_configuration.get('backbone')\n",
    "selected_trial = best_configuration.get('trial_name')\n",
    "best_value = selection_criteria.get('best_value', 0.0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL SELECTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSelected Model:\")\n",
    "print(f\"  Backbone: {selected_backbone}\")\n",
    "print(f\"  Trial: {selected_trial}\")\n",
    "print(f\"  Best {hpo_config['objective']['metric']}: {best_value:.4f}\")\n",
    "print(f\"  Selection Strategy: {selection_criteria.get('selection_strategy', 'N/A')}\")\n",
    "print(f\"  Speed Data Source: {selection_criteria.get('speed_data_source', 'parameter_proxy')}\")\n",
    "\n",
    "# Show all candidates with complete metrics\n",
    "if 'all_candidates' in selection_criteria:\n",
    "    print(f\"\\nAll Candidates:\")\n",
    "    for i, candidate in enumerate(selection_criteria['all_candidates'], 1):\n",
    "        marker = \"✓\" if candidate['backbone'] == selected_backbone else \" \"\n",
    "        speed_info = f\"speed_score={candidate.get('speed_score', 'N/A'):.2f}\"\n",
    "        speed_source = candidate.get('speed_data_source', 'parameter_proxy')\n",
    "        benchmark_latency = candidate.get('benchmark_latency_ms')\n",
    "        \n",
    "        if benchmark_latency:\n",
    "            speed_info += f\" (benchmark: {benchmark_latency:.1f}ms)\"\n",
    "        else:\n",
    "            speed_info += f\" ({speed_source})\"\n",
    "        \n",
    "        print(f\"  {marker} {i}. {candidate['backbone']}: \"\n",
    "              f\"accuracy={candidate['accuracy']:.4f}, {speed_info}\")\n",
    "\n",
    "# Show per-entity metrics if available\n",
    "metrics = best_configuration.get('metrics', {})\n",
    "if 'per_entity' in metrics:\n",
    "    print(f\"\\nPer-Entity Metrics (Selected Model):\")\n",
    "    per_entity = metrics['per_entity']\n",
    "    for entity, entity_metrics in sorted(per_entity.items()):\n",
    "        print(f\"  {entity}: \"\n",
    "              f\"precision={entity_metrics.get('precision', 0):.3f}, \"\n",
    "              f\"recall={entity_metrics.get('recall', 0):.3f}, \"\n",
    "              f\"f1={entity_metrics.get('f1', 0):.3f}, \"\n",
    "              f\"support={entity_metrics.get('support', 0)}\")\n",
    "\n",
    "# Show selection reason and details\n",
    "if 'reason' in selection_criteria:\n",
    "    print(f\"\\nSelection Reason: {selection_criteria['reason']}\")\n",
    "\n",
    "if 'accuracy_diff_from_best' in selection_criteria:\n",
    "    print(f\"Accuracy Difference from Best: {selection_criteria['accuracy_diff_from_best']:.4f}\")\n",
    "\n",
    "# Print complete configuration as JSON for easy copying\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Complete Configuration (JSON format for copying):\")\n",
    "print(\"=\" * 80)\n",
    "import json\n",
    "print(json.dumps(best_configuration, indent=2, default=str))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nSaved to: {BEST_CONFIG_CACHE_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions. This uses the full training epochs (no early stopping) and the best hyperparameters found during HPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "from shared.json_cache import load_json, save_json\n",
    "from orchestration import STAGE_TRAINING\n",
    "from orchestration.jobs.training import build_final_training_config\n",
    "\n",
    "# Use constants defined in Step P1-3.1.1 (DEFAULT_RANDOM_SEED, METRICS_FILENAME)\n",
    "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
    "FINAL_TRAINING_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"final_training\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best configuration from cache (for use in subsequent steps)\n",
    "best_configuration = load_json(BEST_CONFIG_CACHE_FILE, default=None)\n",
    "\n",
    "if best_configuration is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Best configuration cache not found: {BEST_CONFIG_CACHE_FILE}\\n\"\n",
    "        f\"Please run Step P1-3.6: Best Configuration Selection first.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configuration = load_json(BEST_CONFIG_CACHE_FILE, default=None)\n",
    "\n",
    "if best_configuration is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Best configuration cache not found: {BEST_CONFIG_CACHE_FILE}\\n\"\n",
    "        f\"Please run Step P1-3.6: Best Configuration Selection first.\"\n",
    "    )\n",
    "\n",
    "final_training_config = build_final_training_config(\n",
    "    best_config=best_configuration,\n",
    "    train_config=configs[\"train\"],\n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
    "final_output_dir = FINAL_TRAINING_OUTPUT_DIR / final_training_config['backbone']\n",
    "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_path = SRC_DIR / \"train.py\"\n",
    "training_args = [\n",
    "    sys.executable,\n",
    "    str(training_script_path),\n",
    "    \"--data-asset\",\n",
    "    str(DATASET_LOCAL_PATH),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    final_training_config[\"backbone\"],\n",
    "    \"--learning-rate\",\n",
    "    str(final_training_config[\"learning_rate\"]),\n",
    "    \"--batch-size\",\n",
    "    str(final_training_config[\"batch_size\"]),\n",
    "    \"--dropout\",\n",
    "    str(final_training_config[\"dropout\"]),\n",
    "    \"--weight-decay\",\n",
    "    str(final_training_config[\"weight_decay\"]),\n",
    "    \"--epochs\",\n",
    "    str(final_training_config[\"epochs\"]),\n",
    "    \"--random-seed\",\n",
    "    str(final_training_config[\"random_seed\"]),\n",
    "    \"--early-stopping-enabled\",\n",
    "    str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
    "    \"--use-combined-data\",\n",
    "    str(final_training_config[\"use_combined_data\"]).lower(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = os.environ.copy()\n",
    "training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "training_env[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    training_args,\n",
    "    cwd=ROOT_DIR,\n",
    "    env=training_env,\n",
    "    capture_output=False,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"Final training failed with return code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use METRICS_FILENAME constant defined in Step P1-3.1.1\n",
    "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
    "\n",
    "metrics_file = final_output_dir / METRICS_FILENAME\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"✓ Training completed. Checkpoint: {final_output_dir}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Training metrics not found: {metrics_file}\")\n",
    "\n",
    "save_json(FINAL_TRAINING_CACHE_FILE, {\n",
    "    \"output_dir\": str(final_output_dir),\n",
    "    \"backbone\": final_training_config[\"backbone\"],\n",
    "    \"config\": final_training_config,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
    "\n",
    "**Platform Adapter Note**: The conversion script (`src/convert_to_onnx.py`) uses the platform adapter to automatically handle output paths and logging appropriately for local execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "from shared.json_cache import load_json\n",
    "\n",
    "CONVERSION_SCRIPT_PATH = SRC_DIR / \"convert_to_onnx.py\"\n",
    "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
    "CONVERSION_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"conversion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cache = load_json(FINAL_TRAINING_CACHE_FILE, default=None)\n",
    "\n",
    "if training_cache is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Final training cache not found: {FINAL_TRAINING_CACHE_FILE}\\n\"\n",
    "        f\"Please run Step P1-3.7: Final Training first.\"\n",
    "    )\n",
    "\n",
    "checkpoint_dir = Path(training_cache[\"output_dir\"]) / \"checkpoint\"\n",
    "if not checkpoint_dir.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "\n",
    "backbone = training_cache[\"backbone\"]\n",
    "conversion_output_dir = CONVERSION_OUTPUT_DIR / backbone\n",
    "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_args = [\n",
    "    sys.executable,\n",
    "    str(CONVERSION_SCRIPT_PATH),\n",
    "    \"--checkpoint-path\",\n",
    "    str(checkpoint_dir),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    backbone,\n",
    "    \"--output-dir\",\n",
    "    str(conversion_output_dir),\n",
    "    \"--quantize-int8\",\n",
    "    \"--run-smoke-test\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_env = os.environ.copy()\n",
    "conversion_env[\"AZURE_ML_OUTPUT_onnx_model\"] = str(conversion_output_dir)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    conversion_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    conversion_args,\n",
    "    cwd=ROOT_DIR,\n",
    "    env=conversion_env,\n",
    "    capture_output=False,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"Model conversion failed with return code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.json_cache import save_json\n",
    "\n",
    "ONNX_MODEL_FILENAME = \"model_int8.onnx\"\n",
    "FALLBACK_ONNX_MODEL_FILENAME = \"model.onnx\"\n",
    "CONVERSION_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"conversion_cache.json\"\n",
    "\n",
    "onnx_model_path = conversion_output_dir / ONNX_MODEL_FILENAME\n",
    "if not onnx_model_path.exists():\n",
    "    onnx_model_path = conversion_output_dir / FALLBACK_ONNX_MODEL_FILENAME\n",
    "\n",
    "if not onnx_model_path.exists():\n",
    "    raise FileNotFoundError(f\"ONNX model not found in {conversion_output_dir}\")\n",
    "\n",
    "print(f\"✓ Conversion completed. ONNX model: {onnx_model_path}\")\n",
    "\n",
    "save_json(CONVERSION_CACHE_FILE, {\n",
    "    \"onnx_model_path\": str(onnx_model_path),\n",
    "    \"backbone\": backbone,\n",
    "    \"checkpoint_dir\": str(checkpoint_dir),\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
