{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration\n",
    "\n",
    "This notebook orchestrates all training activities without performing local computation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Step 1**: Load Centralized Configs\n",
    "- **Step 2**: Data Ingestion & Versioning (Asset Layer)\n",
    "- **Step 3**: Environment Definition\n",
    "- **Step 4**: The Dry Run\n",
    "- **Step 5**: The Sweep (HPO)\n",
    "- **Step 6**: Best Configuration Selection (Automated)\n",
    "- **Step 7**: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **only submits and monitors Azure ML jobs**\n",
    "- **No training logic** is executed locally\n",
    "- All computation happens remotely on Azure ML compute\n",
    "- The notebook must be **re-runnable end-to-end**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "CONFIG_HASH_LENGTH = 16\n",
    "DEFAULT_WORKSPACE_NAME = \"resume-ner-ws\"\n",
    "\n",
    "env_path = Path(\"../config.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = Path(\"../config\")\n",
    "CONFIG_PATHS = {\n",
    "    \"data\": CONFIG_DIR / \"data\" / \"resume_v1.yaml\",\n",
    "    \"model\": CONFIG_DIR / \"model\" / \"distilbert.yaml\",\n",
    "    \"train\": CONFIG_DIR / \"train.yaml\",\n",
    "    \"hpo\": CONFIG_DIR / \"hpo\" / \"smoke.yaml\",\n",
    "    \"env\": CONFIG_DIR / \"env\" / \"azure.yaml\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_config_file(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load a YAML config file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the YAML config file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Parsed configuration dictionary\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If config file does not exist\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def compute_config_hash(config: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA256 hash of config for reproducibility.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Hexadecimal hash string (truncated to CONFIG_HASH_LENGTH)\n",
    "    \"\"\"\n",
    "    config_str = json.dumps(config, sort_keys=True)\n",
    "    full_hash = hashlib.sha256(config_str.encode()).hexdigest()\n",
    "    return full_hash[:CONFIG_HASH_LENGTH]\n",
    "\n",
    "\n",
    "configs = {}\n",
    "config_hashes = {}\n",
    "\n",
    "for name, path in CONFIG_PATHS.items():\n",
    "    configs[name] = load_config_file(path)\n",
    "    config_hashes[name] = compute_config_hash(configs[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_configs = {k: json.dumps(v, sort_keys=True) for k, v in configs.items()}\n",
    "\n",
    "\n",
    "def validate_config_immutability():\n",
    "    \"\"\"\n",
    "    Ensure configs haven't been mutated at runtime.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any config was mutated\n",
    "    \"\"\"\n",
    "    for name in configs:\n",
    "        current = json.dumps(configs[name], sort_keys=True)\n",
    "        if current != original_configs[name]:\n",
    "            raise ValueError(f\"Config '{name}' was mutated at runtime!\")\n",
    "\n",
    "\n",
    "validate_config_immutability()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "def get_workspace_name() -> str:\n",
    "    \"\"\"\n",
    "    Get workspace name from infrastructure config or use default.\n",
    "    \n",
    "    Returns:\n",
    "        str: Workspace name\n",
    "    \"\"\"\n",
    "    infrastructure_config_path = Path(\"../config/infrastructure.yaml\")\n",
    "    if infrastructure_config_path.exists():\n",
    "        with open(infrastructure_config_path, \"r\") as f:\n",
    "            infrastructure_config = yaml.safe_load(f)\n",
    "        return infrastructure_config[\"workspace\"][\"name\"]\n",
    "    return DEFAULT_WORKSPACE_NAME\n",
    "\n",
    "\n",
    "subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "\n",
    "if not subscription_id or not resource_group:\n",
    "    raise ValueError(\"AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP must be set\")\n",
    "\n",
    "workspace_name = get_workspace_name()\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All configs and their hashes will be attached to each Azure ML job for full reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_metadata(configs: Dict[str, Any], config_hashes: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create metadata dictionary for job tagging.\n",
    "    \n",
    "    Args:\n",
    "        configs: Dictionary of loaded configs\n",
    "        config_hashes: Dictionary of config hashes\n",
    "        \n",
    "    Returns:\n",
    "        dict: Metadata dictionary for Azure ML job tags\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"data_config_hash\": config_hashes[\"data\"],\n",
    "        \"model_config_hash\": config_hashes[\"model\"],\n",
    "        \"train_config_hash\": config_hashes[\"train\"],\n",
    "        \"hpo_config_hash\": config_hashes[\"hpo\"],\n",
    "        \"env_config_hash\": config_hashes[\"env\"],\n",
    "        \"data_version\": configs[\"data\"][\"version\"],\n",
    "        \"model_backbone\": configs[\"model\"][\"backbone\"],\n",
    "    }\n",
    "\n",
    "\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Data Ingestion & Versioning (Asset Layer)\n",
    "\n",
    "Upload dataset to Blob Storage and register as an Azure ML Data Asset for versioned, immutable data access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import json\n",
    "\n",
    "DATASET_LOCAL_PATH = Path(\"../dataset\")\n",
    "DATA_ASSET_NAME = configs[\"data\"][\"name\"]\n",
    "DATA_ASSET_VERSION = configs[\"data\"][\"version\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storage_account_name() -> str:\n",
    "    \"\"\"\n",
    "    Get storage account name from infrastructure config.\n",
    "    \n",
    "    Returns:\n",
    "        str: Storage account name\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If storage account name not found\n",
    "    \"\"\"\n",
    "    infrastructure_config_path = Path(\"../config/infrastructure.yaml\")\n",
    "    if infrastructure_config_path.exists():\n",
    "        with open(infrastructure_config_path, \"r\") as f:\n",
    "            infrastructure_config = yaml.safe_load(f)\n",
    "        return infrastructure_config[\"storage\"][\"account_name\"]\n",
    "    raise ValueError(\"Storage account name not found in infrastructure config\")\n",
    "\n",
    "\n",
    "def build_connection_string(account_name: str, account_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Build storage account connection string.\n",
    "    \n",
    "    Args:\n",
    "        account_name: Storage account name\n",
    "        account_key: Storage account key\n",
    "        \n",
    "    Returns:\n",
    "        str: Connection string for blob service client\n",
    "    \"\"\"\n",
    "    return f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "\n",
    "def upload_dataset_to_blob(dataset_path: Path, container_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload dataset to blob storage (idempotent).\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Local path to dataset directory\n",
    "        container_name: Blob storage container name\n",
    "        \n",
    "    Returns:\n",
    "        str: Azure ML datastore path\n",
    "    \"\"\"\n",
    "    storage_account_name = get_storage_account_name()\n",
    "    subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "    resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "    \n",
    "    from azure.mgmt.storage import StorageManagementClient\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    \n",
    "    credential = DefaultAzureCredential()\n",
    "    storage_mgmt = StorageManagementClient(credential, subscription_id)\n",
    "    keys = storage_mgmt.storage_accounts.list_keys(resource_group, storage_account_name)\n",
    "    \n",
    "    conn_str = build_connection_string(storage_account_name, keys.keys[0].value)\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "    \n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    if not container_client.exists():\n",
    "        container_client.create_container()\n",
    "    \n",
    "    blob_path = f\"{DATA_ASSET_NAME}/v{DATA_ASSET_VERSION}\"\n",
    "    \n",
    "    for file_path in dataset_path.rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            relative_path = file_path.relative_to(dataset_path)\n",
    "            blob_name = f\"{blob_path}/{relative_path}\"\n",
    "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "            \n",
    "            if not blob_client.exists():\n",
    "                with open(file_path, \"rb\") as data:\n",
    "                    blob_client.upload_blob(data, overwrite=False)\n",
    "    \n",
    "    return f\"azureml://datastores/workspaceblobstore/paths/{blob_path}\"\n",
    "\n",
    "\n",
    "blob_uri = upload_dataset_to_blob(DATASET_LOCAL_PATH, \"datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_data_asset(name: str, version: str, uri: str, description: str) -> Data:\n",
    "    \"\"\"\n",
    "    Register or resolve Azure ML Data Asset (uri_folder type).\n",
    "    \n",
    "    Args:\n",
    "        name: Data asset name\n",
    "        version: Data asset version\n",
    "        uri: Azure ML datastore path\n",
    "        description: Asset description\n",
    "        \n",
    "    Returns:\n",
    "        Data: Registered data asset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_asset = ml_client.data.get(name=name, version=version)\n",
    "        return existing_asset\n",
    "    except Exception:\n",
    "        data_asset = Data(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            description=description,\n",
    "            path=uri,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "        )\n",
    "        return ml_client.data.create_or_update(data_asset)\n",
    "\n",
    "\n",
    "data_asset = register_data_asset(\n",
    "    name=DATA_ASSET_NAME,\n",
    "    version=DATA_ASSET_VERSION,\n",
    "    uri=blob_uri,\n",
    "    description=configs[\"data\"][\"description\"],\n",
    ")\n",
    "\n",
    "# Store both asset reference and datastore path for fallback\n",
    "default_datastore = ml_client.datastores.get_default()\n",
    "if \"/paths/\" in data_asset.path:\n",
    "    relative_path = data_asset.path.split(\"/paths/\", 1)[1].rstrip('/')\n",
    "    datastore_path = f\"azureml://datastores/{default_datastore.name}/paths/{relative_path}\"\n",
    "else:\n",
    "    datastore_path = data_asset.path.rstrip('/')\n",
    "\n",
    "asset_reference = f\"azureml:{data_asset.name}:{data_asset.version}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compute Cluster Verification ===\n",
      "✓ Compute cluster found: cpu-cluster\n",
      "  Status: Succeeded\n",
      "  VM Size: Standard_D2s_v3\n",
      "  Identity: system_assigned\n",
      "\n",
      "⚠️  IMPORTANT: Jobs must use this compute cluster, NOT 'Serverless'\n",
      "   If you see 'Target: Serverless' in job details, the compute cluster wasn't found!\n",
      "\n",
      "=== Data Asset Configuration ===\n",
      "Asset Name: resume-ner-data\n",
      "Asset Version: 1\n",
      "Using Datastore Path: azureml://datastores/workspaceblobstore/paths/resume-ner-data/v1\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Verify compute cluster and data asset configuration\n",
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "print(\"=== Compute Cluster Verification ===\")\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    print(f\"✓ Compute cluster found: {compute_cluster.name}\")\n",
    "    print(f\"  Status: {compute_cluster.provisioning_state if hasattr(compute_cluster, 'provisioning_state') else 'unknown'}\")\n",
    "    print(f\"  VM Size: {compute_cluster.size if hasattr(compute_cluster, 'size') else 'unknown'}\")\n",
    "    \n",
    "    if hasattr(compute_cluster, 'identity') and compute_cluster.identity:\n",
    "        print(f\"  Identity: {compute_cluster.identity.type if hasattr(compute_cluster.identity, 'type') else 'configured'}\")\n",
    "    \n",
    "    print(f\"\\n⚠️  IMPORTANT: Jobs must use this compute cluster, NOT 'Serverless'\")\n",
    "    print(f\"   If you see 'Target: Serverless' in job details, the compute cluster wasn't found!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR: Compute cluster '{compute_cluster_name}' not found!\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(f\"\\n⚠️  ACTION REQUIRED:\")\n",
    "    print(f\"  1. Verify the compute cluster exists in Azure Portal\")\n",
    "    print(f\"  2. Check the name in config/env/azure.yaml matches the actual cluster name\")\n",
    "    print(f\"  3. If cluster doesn't exist, run the infrastructure setup notebook\")\n",
    "\n",
    "print(f\"\\n=== Data Asset Configuration ===\")\n",
    "print(f\"Asset Name: {data_asset.name}\")\n",
    "print(f\"Asset Version: {data_asset.version}\")\n",
    "\n",
    "# Compute datastore path for display\n",
    "default_datastore = ml_client.datastores.get_default()\n",
    "if \"/paths/\" in data_asset.path:\n",
    "    relative_path = data_asset.path.split(\"/paths/\", 1)[1].rstrip('/')\n",
    "    datastore_path = f\"azureml://datastores/{default_datastore.name}/paths/{relative_path}\"\n",
    "else:\n",
    "    datastore_path = data_asset.path.rstrip('/')\n",
    "\n",
    "print(f\"Using Datastore Path: {datastore_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting: ScriptExecution.StreamAccess.NotFound\n",
    "\n",
    "If you encounter `ScriptExecution.StreamAccess.NotFound`, this typically indicates a **permissions issue**. The compute cluster's managed identity needs access to the storage account.\n",
    "\n",
    "**To fix this issue:**\n",
    "\n",
    "1. **Verify Compute Cluster has Managed Identity:**\n",
    "   - Go to Azure Portal → Your Workspace → Compute → Compute clusters\n",
    "   - Select your compute cluster (e.g., `cpu-cluster`)\n",
    "   - Check the \"Identity\" tab to ensure a managed identity is assigned\n",
    "\n",
    "2. **Grant Storage Blob Data Reader Role:**\n",
    "   - Go to Azure Portal → Storage Account (the one used by your workspace)\n",
    "   - Navigate to \"Access Control (IAM)\"\n",
    "   - Click \"Add\" → \"Add role assignment\"\n",
    "   - Select role: **\"Storage Blob Data Reader\"**\n",
    "   - Assign to: **\"Managed identity\"**\n",
    "   - Select: Your compute cluster's managed identity\n",
    "   - Click \"Save\"\n",
    "\n",
    "3. **Check Storage Account Firewall:**\n",
    "   - Go to Storage Account → Networking\n",
    "   - Ensure \"Allow Azure services on the trusted services list to access this storage account\" is enabled\n",
    "   - Or add your compute cluster's network to allowed networks\n",
    "\n",
    "4. **Verify Data Asset Registration:**\n",
    "   - Ensure the data asset is properly registered in Azure ML\n",
    "   - Check that files exist in the blob storage path\n",
    "\n",
    "**Alternative:** If you continue to have issues, you can also try using the full registered path instead of the asset reference format, though the asset reference format is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5960"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_data_asset(asset: Data, local_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Validate data asset: file presence, sample readability, token/label alignment.\n",
    "    \n",
    "    Args:\n",
    "        asset: Registered data asset\n",
    "        local_path: Local dataset path for validation\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of validated samples\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    train_file = local_path / \"train.json\"\n",
    "    if not train_file.exists():\n",
    "        raise ValueError(f\"train.json not found in {local_path}\")\n",
    "    \n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if not isinstance(data, list) or len(data) == 0:\n",
    "        raise ValueError(\"train.json must contain a non-empty list\")\n",
    "    \n",
    "    sample = data[0]\n",
    "    required_keys = [\"text\", \"annotations\"]\n",
    "    for key in required_keys:\n",
    "        if key not in sample:\n",
    "            raise ValueError(f\"Sample missing required key: {key}\")\n",
    "    \n",
    "    text = sample[\"text\"]\n",
    "    annotations = sample[\"annotations\"]\n",
    "    \n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        raise ValueError(\"Sample text must be a non-empty string\")\n",
    "    \n",
    "    if not isinstance(annotations, list):\n",
    "        raise ValueError(\"Annotations must be a list\")\n",
    "    \n",
    "    for ann in annotations:\n",
    "        if not isinstance(ann, list) or len(ann) != 3:\n",
    "            raise ValueError(\"Each annotation must be [start, end, entity_type]\")\n",
    "        start, end, entity_type = ann\n",
    "        if not (0 <= start < end <= len(text)):\n",
    "            raise ValueError(f\"Invalid annotation span: [{start}, {end}] for text length {len(text)}\")\n",
    "    \n",
    "    return len(data)\n",
    "\n",
    "\n",
    "validate_data_asset(data_asset, DATASET_LOCAL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Environment Definition\n",
    "\n",
    "Define a stable execution environment (Docker image + Conda dependencies) for consistent behavior across all training jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "ENVIRONMENT_NAME = \"resume-ner-training\"\n",
    "CONDA_ENV_PATH = Path(\"../config/environment/conda.yaml\")\n",
    "# Azure ML OpenMPI base image (lightweight, CPU-compatible)\n",
    "# This base image provides MPI support for Azure ML orchestration\n",
    "# PyTorch and other dependencies are installed via conda.yaml for reproducibility\n",
    "# For GPU compute, use: \"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.1-cuda12.1:latest\"\n",
    "DEFAULT_DOCKER_IMAGE = \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conda_environment(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load conda environment definition from YAML file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to conda environment YAML file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Parsed conda environment dictionary\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If conda environment file does not exist\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Conda environment file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def compute_environment_hash(conda_deps: Dict[str, Any], docker_image: str) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA256 hash of environment definition for versioning.\n",
    "    \n",
    "    Args:\n",
    "        conda_deps: Conda environment dependencies dictionary\n",
    "        docker_image: Docker base image name\n",
    "        \n",
    "    Returns:\n",
    "        str: Hexadecimal hash string (truncated to CONFIG_HASH_LENGTH)\n",
    "    \"\"\"\n",
    "    env_spec = {\n",
    "        \"conda_dependencies\": conda_deps,\n",
    "        \"docker_image\": docker_image,\n",
    "    }\n",
    "    env_str = json.dumps(env_spec, sort_keys=True)\n",
    "    full_hash = hashlib.sha256(env_str.encode()).hexdigest()\n",
    "    return full_hash[:CONFIG_HASH_LENGTH]\n",
    "\n",
    "\n",
    "conda_env = load_conda_environment(CONDA_ENV_PATH)\n",
    "docker_image = DEFAULT_DOCKER_IMAGE\n",
    "environment_hash = compute_environment_hash(conda_env, docker_image)\n",
    "environment_version = f\"v{environment_hash}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_environment(\n",
    "    ml_client: MLClient,\n",
    "    name: str,\n",
    "    version: str,\n",
    "    conda_dependencies: Dict[str, Any],\n",
    "    docker_image: str,\n",
    ") -> Environment:\n",
    "    \"\"\"\n",
    "    Get existing environment or create new one based on hash-based versioning.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        name: Environment name\n",
    "        version: Environment version (hash-based)\n",
    "        conda_dependencies: Conda environment dependencies\n",
    "        docker_image: Docker base image (required by Azure ML)\n",
    "        \n",
    "    Returns:\n",
    "        Environment: Azure ML Environment instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_env = ml_client.environments.get(name=name, version=version)\n",
    "        return existing_env\n",
    "    except ResourceNotFoundError:\n",
    "        # Create environment from conda file with base image\n",
    "        # Azure ML will install conda dependencies on top of the base image\n",
    "        environment = Environment(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            conda_file=conda_dependencies,\n",
    "            image=docker_image,\n",
    "            description=f\"Training environment for Resume NER (hash: {version})\",\n",
    "        )\n",
    "        return ml_client.environments.create_or_update(environment)\n",
    "\n",
    "\n",
    "training_environment = get_or_create_environment(\n",
    "    ml_client=ml_client,\n",
    "    name=ENVIRONMENT_NAME,\n",
    "    version=environment_version,\n",
    "    conda_dependencies=conda_env,\n",
    "    docker_image=docker_image,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running training jobs, we prepare the environment image by submitting a minimal warm-up job. This ensures:\n",
    "\n",
    "- **Docker image is built and cached** before sweep jobs start\n",
    "- **Prevents timeouts** in sweep jobs that would otherwise wait for image preparation\n",
    "- **Faster subsequent runs** since the image is cached after the first build\n",
    "\n",
    "This step is especially important for the first run when the environment image doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: olive_whistle_2bjw3701rx\n",
      "Web View: https://ml.azure.com/runs/olive_whistle_2bjw3701rx?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: olive_whistle_2bjw3701rx\n",
      "Web View: https://ml.azure.com/runs/olive_whistle_2bjw3701rx?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "def prepare_environment_image(\n",
    "    ml_client: MLClient,\n",
    "    environment: Environment,\n",
    "    compute_cluster: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prepare environment image by submitting a minimal warm-up job if not already built.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        environment: Environment to prepare\n",
    "        compute_cluster: Compute cluster name for warm-up job\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If warm-up job fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        jobs = ml_client.jobs.list(\n",
    "            display_name=\"environment-warmup\",\n",
    "            list_view_type=\"All\",\n",
    "        )\n",
    "        for i, job in enumerate(jobs):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            if (job.status == \"Completed\" and \n",
    "                hasattr(job, 'environment') and job.environment and\n",
    "                getattr(job.environment, 'name', None) == environment.name and\n",
    "                getattr(job.environment, 'version', None) == environment.version):\n",
    "                return\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    warmup_job = command(\n",
    "        code=\"../src\",\n",
    "        command=\"python -c 'import torch; print(f\\\"PyTorch: {torch.__version__}\\\"); print(\\\"Environment ready!\\\")'\",\n",
    "        environment=environment,\n",
    "        compute=compute_cluster,\n",
    "        display_name=\"environment-warmup\",\n",
    "        description=\"Warm-up job to prepare environment image before actual training\",\n",
    "    )\n",
    "    \n",
    "    submitted_warmup = ml_client.jobs.create_or_update(warmup_job)\n",
    "    ml_client.jobs.stream(submitted_warmup.name)\n",
    "    \n",
    "    completed_warmup = ml_client.jobs.get(submitted_warmup.name)\n",
    "    \n",
    "    if completed_warmup.status != \"Completed\":\n",
    "        raise RuntimeError(\n",
    "            f\"Environment warm-up job failed with status: {completed_warmup.status}. \"\n",
    "            f\"Check the job logs to diagnose environment build issues.\"\n",
    "        )\n",
    "\n",
    "\n",
    "prepare_environment_image(\n",
    "    ml_client=ml_client,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=configs[\"env\"][\"compute\"][\"training_cluster\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Dry Run\n",
    "\n",
    "Submit a minimal sweep job using `smoke.yaml` to validate the sweep mechanism and pipeline integrity before launching the production HPO sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.entities import Job\n",
    "from azure.ai.ml.sweep import SweepJob, Objective, SweepJobLimits, Choice, Uniform, LogUniform\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n",
    "DRY_RUN_JOB_NAME = \"dry-run-sweep\"\n",
    "SMOKE_HPO_CONFIG_PATH = Path(\"../config/hpo/smoke.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_space(hpo_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert HPO config to Azure ML search space format using proper search space objects.\n",
    "    \n",
    "    Args:\n",
    "        hpo_config: HPO configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        dict: Azure ML search space dictionary with Choice/Uniform/LogUniform objects\n",
    "    \"\"\"\n",
    "    search_space = {}\n",
    "    for param_name, param_config in hpo_config[\"search_space\"].items():\n",
    "        param_type = param_config[\"type\"]\n",
    "        if param_type == \"choice\":\n",
    "            search_space[param_name] = Choice(values=param_config[\"values\"])\n",
    "        elif param_type == \"uniform\":\n",
    "            search_space[param_name] = Uniform(\n",
    "                min_value=float(param_config[\"min\"]),\n",
    "                max_value=float(param_config[\"max\"]),\n",
    "            )\n",
    "        elif param_type == \"loguniform\":\n",
    "            search_space[param_name] = LogUniform(\n",
    "                min_value=float(param_config[\"min\"]),\n",
    "                max_value=float(param_config[\"max\"]),\n",
    "            )\n",
    "    return search_space\n",
    "\n",
    "\n",
    "def create_dry_run_sweep_job_for_backbone(\n",
    "    ml_client: MLClient,\n",
    "    script_path: Path,\n",
    "    data_asset: Data,\n",
    "    environment: Environment,\n",
    "    compute_cluster: str,\n",
    "    backbone: str,\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> SweepJob:\n",
    "    \"\"\"\n",
    "    Create a dry run Azure ML Sweep Job for a specific backbone model using smoke HPO config.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        script_path: Path to training script\n",
    "        data_asset: Registered data asset\n",
    "        environment: Training environment\n",
    "        compute_cluster: Compute cluster name\n",
    "        backbone: Backbone model name (e.g., \"distilbert\", \"deberta\")\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        SweepJob: Azure ML Sweep Job definition\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If training script or smoke HPO config does not exist\n",
    "    \"\"\"\n",
    "    if not script_path.exists():\n",
    "        raise FileNotFoundError(f\"Training script not found: {script_path}\")\n",
    "    \n",
    "    smoke_hpo_config = load_config_file(SMOKE_HPO_CONFIG_PATH)\n",
    "    \n",
    "    smoke_hpo_config_without_backbone = {\n",
    "        \"search_space\": {\n",
    "            k: v for k, v in smoke_hpo_config[\"search_space\"].items() if k != \"backbone\"\n",
    "        }\n",
    "    }\n",
    "    search_space_without_backbone = create_search_space(smoke_hpo_config_without_backbone)\n",
    "    \n",
    "    dry_run_trials_per_backbone = max(2, smoke_hpo_config[\"sampling\"][\"max_trials\"] // 2)\n",
    "    \n",
    "    command_args = (\n",
    "        f\"--data-asset ${{{{inputs.data}}}} \"\n",
    "        f\"--config-dir ../config \"\n",
    "        f\"--backbone {backbone} \"\n",
    "        f\"--learning-rate ${{{{search_space.learning_rate}}}} \"\n",
    "        f\"--batch-size ${{{{search_space.batch_size}}}} \"\n",
    "        f\"--dropout ${{{{search_space.dropout}}}} \"\n",
    "        f\"--weight-decay ${{{{search_space.weight_decay}}}}\"\n",
    "    )\n",
    "    \n",
    "    default_datastore = ml_client.datastores.get_default()\n",
    "    if \"/paths/\" in data_asset.path:\n",
    "        relative_path = data_asset.path.split(\"/paths/\", 1)[1].rstrip('/')\n",
    "        data_path = f\"azureml://datastores/{default_datastore.name}/paths/{relative_path}\"\n",
    "    else:\n",
    "        data_path = data_asset.path.rstrip('/')\n",
    "    \n",
    "    data_input = Input(type=\"uri_folder\", path=data_path)\n",
    "    \n",
    "    trial_job = command(\n",
    "        code=\"../src\",\n",
    "        command=f\"python {script_path.name} {command_args}\",\n",
    "        inputs={\n",
    "            \"data\": data_input,\n",
    "        },\n",
    "        environment=environment,\n",
    "        compute=compute_cluster,\n",
    "    )\n",
    "    \n",
    "    objective = Objective(\n",
    "        goal=smoke_hpo_config[\"objective\"][\"goal\"],\n",
    "        primary_metric=smoke_hpo_config[\"objective\"][\"metric\"],\n",
    "    )\n",
    "    \n",
    "    timeout_seconds = smoke_hpo_config[\"sampling\"][\"timeout_minutes\"] * 60\n",
    "    limits = SweepJobLimits(\n",
    "        max_total_trials=dry_run_trials_per_backbone,\n",
    "        timeout=timeout_seconds,\n",
    "    )\n",
    "    \n",
    "    sweep_job = SweepJob(\n",
    "        trial=trial_job,\n",
    "        search_space=search_space_without_backbone,\n",
    "        sampling_algorithm=smoke_hpo_config[\"sampling\"][\"algorithm\"],\n",
    "        objective=objective,\n",
    "        limits=limits,\n",
    "        inputs={\n",
    "            \"data\": data_input,\n",
    "        },\n",
    "        experiment_name=f\"{configs['env']['logging']['experiment_name']}-{backbone}\",\n",
    "        tags={**config_metadata, \"job_type\": \"dry_run_sweep\", \"backbone\": backbone},\n",
    "        display_name=f\"{DRY_RUN_JOB_NAME}-{backbone}\",\n",
    "        description=f\"Dry run sweep to validate sweep mechanism for {backbone} before production HPO\",\n",
    "    )\n",
    "    \n",
    "    return sweep_job\n",
    "\n",
    "\n",
    "def submit_and_wait_for_job(ml_client: MLClient, job: Any) -> Job:\n",
    "    \"\"\"\n",
    "    Submit job and wait for completion.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        job: Job definition (command or sweep)\n",
    "        \n",
    "    Returns:\n",
    "        Job: Completed job instance\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If job fails\n",
    "    \"\"\"\n",
    "    submitted_job = ml_client.jobs.create_or_update(job)\n",
    "    ml_client.jobs.stream(submitted_job.name)\n",
    "    \n",
    "    completed_job = ml_client.jobs.get(submitted_job.name)\n",
    "    \n",
    "    if completed_job.status != \"Completed\":\n",
    "        raise RuntimeError(f\"Job {completed_job.name} failed with status: {completed_job.status}\")\n",
    "    \n",
    "    return completed_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "smoke_hpo_config = load_config_file(SMOKE_HPO_CONFIG_PATH)\n",
    "backbone_values = smoke_hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "dry_run_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    dry_run_sweep_jobs[backbone] = create_dry_run_sweep_job_for_backbone(\n",
    "        ml_client=ml_client,\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        backbone=backbone,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: lemon_atemoya_pz74vbmw03\n",
      "Web View: https://ml.azure.com/runs/lemon_atemoya_pz74vbmw03?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2025-12-15T00:28:47.1508867Z][GENERATOR][DEBUG]Sampled 2 jobs from search space \n",
      "[2025-12-15T00:28:47.5856234Z][SCHEDULER][INFO]Scheduling job, id='lemon_atemoya_pz74vbmw03_0' \n",
      "[2025-12-15T00:28:47.6528396Z][SCHEDULER][INFO]Scheduling job, id='lemon_atemoya_pz74vbmw03_1' \n",
      "[2025-12-15T00:28:48.3405786Z][SCHEDULER][INFO]Successfully scheduled a job. Id='lemon_atemoya_pz74vbmw03_0' \n",
      "[2025-12-15T00:28:48.2924345Z][SCHEDULER][INFO]Successfully scheduled a job. Id='lemon_atemoya_pz74vbmw03_1' \n",
      "[2025-12-15T00:29:17.7873008Z][GENERATOR][DEBUG]Setting all jobs generated as True, reason : Max number of jobs reached \n",
      "[2025-12-15T00:33:48.0404978Z][CONTROLLER][INFO]Changing Run Status from Running to Failed \n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: lemon_atemoya_pz74vbmw03\n",
      "Web View: https://ml.azure.com/runs/lemon_atemoya_pz74vbmw03?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"\\nError Code: ScriptExecution.StreamAccess.NotFound\\nNative Error: error in streaming from input data sources\\n\\tStreamError(NotFound)\\n=> stream not found\\n\\tNotFound\\nError Message: The requested stream was not found. Please make sure the request uri is correct.| session_id=c16c4644-1d93-409f-a029-f58354278591 User errors were found in at least one of the child runs.\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJobException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m dry_run_completed_jobs = {}\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m backbone, sweep_job \u001b[38;5;129;01min\u001b[39;00m dry_run_sweep_jobs.items():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     completed_job = \u001b[43msubmit_and_wait_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msweep_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     validate_dry_run_sweep_job(completed_job, backbone)\n\u001b[32m     31\u001b[39m     dry_run_completed_jobs[backbone] = completed_job\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36msubmit_and_wait_for_job\u001b[39m\u001b[34m(ml_client, job)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03mSubmit job and wait for completion.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m \u001b[33;03m    RuntimeError: If job fails\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m submitted_job = ml_client.jobs.create_or_update(job)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmitted_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m completed_job = ml_client.jobs.get(submitted_job.name)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completed_job.status != \u001b[33m\"\u001b[39m\u001b[33mCompleted\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:858\u001b[39m, in \u001b[36mJobOperations.stream\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id=job_object.id)\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_requests_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_ops_helper.py:334\u001b[39m, in \u001b[36mstream_logs_until_completion\u001b[39m\u001b[34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[39m\n\u001b[32m    332\u001b[39m         file_handle.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[32m    335\u001b[39m             message=\u001b[33m\"\u001b[39m\u001b[33mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.format(json.dumps(error, indent=\u001b[32m4\u001b[39m)),\n\u001b[32m    336\u001b[39m             target=ErrorTarget.JOB,\n\u001b[32m    337\u001b[39m             no_personal_data_message=\u001b[33m\"\u001b[39m\u001b[33mException raised on failed job.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    338\u001b[39m             error_category=ErrorCategory.SYSTEM_ERROR,\n\u001b[32m    339\u001b[39m         )\n\u001b[32m    341\u001b[39m file_handle.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    342\u001b[39m file_handle.flush()\n",
      "\u001b[31mJobException\u001b[39m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"\\nError Code: ScriptExecution.StreamAccess.NotFound\\nNative Error: error in streaming from input data sources\\n\\tStreamError(NotFound)\\n=> stream not found\\n\\tNotFound\\nError Message: The requested stream was not found. Please make sure the request uri is correct.| session_id=c16c4644-1d93-409f-a029-f58354278591 User errors were found in at least one of the child runs.\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n} "
     ]
    }
   ],
   "source": [
    "def validate_dry_run_sweep_job(job: Job, backbone: str) -> None:\n",
    "    \"\"\"\n",
    "    Validate dry run sweep job completed successfully.\n",
    "    \n",
    "    Args:\n",
    "        job: Completed sweep job instance\n",
    "        backbone: Backbone model name for error messages\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if job.status != \"Completed\":\n",
    "        raise ValueError(f\"Dry run sweep job for {backbone} failed with status: {job.status}\")\n",
    "    \n",
    "    if not hasattr(job, \"trial_count\") or job.trial_count == 0:\n",
    "        raise ValueError(f\"Dry run sweep job for {backbone} produced no trials\")\n",
    "    \n",
    "    min_expected_trials = 1\n",
    "    if job.trial_count < min_expected_trials:\n",
    "        raise ValueError(\n",
    "            f\"Dry run sweep job for {backbone} only produced {job.trial_count} trial(s), \"\n",
    "            f\"expected at least {min_expected_trials}\"\n",
    "        )\n",
    "\n",
    "\n",
    "dry_run_completed_jobs = {}\n",
    "\n",
    "for backbone, sweep_job in dry_run_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_dry_run_sweep_job(completed_job, backbone)\n",
    "    dry_run_completed_jobs[backbone] = completed_job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: The Sweep (HPO)\n",
    "\n",
    "Submit a hyperparameter optimization sweep to systematically search for the best model configuration.\n",
    "\n",
    "**Note**: Currently using `smoke.yaml` for demonstration purposes (CPU-only setup). For production with GPU, switch to `prod.yaml` in the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_SWEEP_JOB_NAME = \"hpo-sweep\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hpo_sweep_job_for_backbone(\n",
    "    ml_client: MLClient,\n",
    "    script_path: Path,\n",
    "    data_asset: Data,\n",
    "    environment: Environment,\n",
    "    compute_cluster: str,\n",
    "    hpo_config: Dict[str, Any],\n",
    "    backbone: str,\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> sweep:\n",
    "    \"\"\"\n",
    "    Create a production HPO sweep job for a specific backbone model.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        script_path: Path to training script\n",
    "        data_asset: Registered data asset\n",
    "        environment: Training environment\n",
    "        compute_cluster: Compute cluster name\n",
    "        hpo_config: HPO configuration dictionary (from prod.yaml)\n",
    "        backbone: Backbone model name (e.g., \"distilbert\", \"deberta\")\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        sweep: Azure ML Sweep Job definition\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If training script does not exist\n",
    "    \"\"\"\n",
    "    if not script_path.exists():\n",
    "        raise FileNotFoundError(f\"Training script not found: {script_path}\")\n",
    "    \n",
    "    hpo_config_without_backbone = {\n",
    "        \"search_space\": {\n",
    "            k: v for k, v in hpo_config[\"search_space\"].items() if k != \"backbone\"\n",
    "        }\n",
    "    }\n",
    "    search_space_without_backbone = create_search_space(hpo_config_without_backbone)\n",
    "    \n",
    "    command_args = (\n",
    "        f\"--data-asset ${{{{inputs.data}}}} \"\n",
    "        f\"--config-dir ../config \"\n",
    "        f\"--backbone {backbone} \"\n",
    "        f\"--learning-rate ${{{{search_space.learning_rate}}}} \"\n",
    "        f\"--batch-size ${{{{search_space.batch_size}}}} \"\n",
    "        f\"--dropout ${{{{search_space.dropout}}}} \"\n",
    "        f\"--weight-decay ${{{{search_space.weight_decay}}}}\"\n",
    "    )\n",
    "    \n",
    "    default_datastore = ml_client.datastores.get_default()\n",
    "    if \"/paths/\" in data_asset.path:\n",
    "        relative_path = data_asset.path.split(\"/paths/\", 1)[1].rstrip('/')\n",
    "        data_path = f\"azureml://datastores/{default_datastore.name}/paths/{relative_path}\"\n",
    "    else:\n",
    "        data_path = data_asset.path.rstrip('/')\n",
    "    \n",
    "    data_input = Input(type=\"uri_folder\", path=data_path)\n",
    "    \n",
    "    trial_job = command(\n",
    "        code=\"../src\",\n",
    "        command=f\"python {script_path.name} {command_args}\",\n",
    "        inputs={\n",
    "            \"data\": data_input,\n",
    "        },\n",
    "        environment=environment,\n",
    "        compute=compute_cluster,\n",
    "    )\n",
    "    \n",
    "    early_termination = None\n",
    "    if \"early_termination\" in hpo_config:\n",
    "        et_config = hpo_config[\"early_termination\"]\n",
    "        if et_config[\"policy\"] == \"bandit\":\n",
    "            from azure.ai.ml.sweep import BanditPolicy\n",
    "            early_termination = BanditPolicy(\n",
    "                evaluation_interval=et_config[\"evaluation_interval\"],\n",
    "                slack_factor=et_config[\"slack_factor\"],\n",
    "                delay_evaluation=et_config[\"delay_evaluation\"],\n",
    "            )\n",
    "    \n",
    "    sweep_job = sweep(\n",
    "        trial=trial_job,\n",
    "        search_space=search_space_without_backbone,\n",
    "        sampling_algorithm=hpo_config[\"sampling\"][\"algorithm\"],\n",
    "        objective=hpo_config[\"objective\"],\n",
    "        limits={\n",
    "            \"max_trials\": hpo_config[\"sampling\"][\"max_trials\"],\n",
    "            \"timeout_minutes\": hpo_config[\"sampling\"][\"timeout_minutes\"],\n",
    "        },\n",
    "        early_termination_policy=early_termination,\n",
    "        experiment_name=f\"{configs['env']['logging']['experiment_name']}-{backbone}\",\n",
    "        tags={**config_metadata, \"job_type\": \"hpo_sweep\", \"backbone\": backbone},\n",
    "        display_name=f\"{HPO_SWEEP_JOB_NAME}-{backbone}\",\n",
    "        description=f\"Production hyperparameter optimization sweep for {backbone}\",\n",
    "    )\n",
    "    \n",
    "    return sweep_job\n",
    "\n",
    "\n",
    "# Verify compute cluster is still accessible\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "backbone_values = configs[\"hpo\"][\"search_space\"][\"backbone\"][\"values\"]\n",
    "hpo_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    hpo_sweep_jobs[backbone] = create_hpo_sweep_job_for_backbone(\n",
    "        ml_client=ml_client,\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        hpo_config=configs[\"hpo\"],\n",
    "        backbone=backbone,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: affable_lock_kq3m7lv8q0\n",
      "Web View: https://ml.azure.com/runs/affable_lock_kq3m7lv8q0?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2025-12-14T20:59:14.7855269Z][GENERATOR][DEBUG]Sampled 4 jobs from search space \n",
      "[2025-12-14T20:59:15.0622216Z][SCHEDULER][INFO]Scheduling job, id='affable_lock_kq3m7lv8q0_0' \n",
      "[2025-12-14T20:59:15.1250909Z][SCHEDULER][INFO]Scheduling job, id='affable_lock_kq3m7lv8q0_2' \n",
      "[2025-12-14T20:59:15.1271654Z][SCHEDULER][INFO]Scheduling job, id='affable_lock_kq3m7lv8q0_1' \n",
      "[2025-12-14T20:59:15.2609215Z][SCHEDULER][INFO]Scheduling job, id='affable_lock_kq3m7lv8q0_3' \n",
      "[2025-12-14T20:59:15.7165172Z][SCHEDULER][INFO]Successfully scheduled a job. Id='affable_lock_kq3m7lv8q0_2' \n",
      "[2025-12-14T20:59:15.6842051Z][SCHEDULER][INFO]Successfully scheduled a job. Id='affable_lock_kq3m7lv8q0_1' \n",
      "[2025-12-14T20:59:15.7910039Z][SCHEDULER][INFO]Successfully scheduled a job. Id='affable_lock_kq3m7lv8q0_3' \n",
      "[2025-12-14T20:59:15.7890889Z][SCHEDULER][INFO]Successfully scheduled a job. Id='affable_lock_kq3m7lv8q0_0' \n",
      "[2025-12-14T20:59:45.2880114Z][GENERATOR][DEBUG]Setting all jobs generated as True, reason : Max number of jobs reached \n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_ops_helper.py:272\u001b[39m, in \u001b[36mstream_logs_until_completion\u001b[39m\u001b[34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[39m\n\u001b[32m    271\u001b[39m file_handle.flush()\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_wait_before_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll_start_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m _current_details = run_operations.get_run_details(job_name)  \u001b[38;5;66;03m# TODO use FileWatcher\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mJobException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m hpo_completed_jobs = {}\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m backbone, sweep_job \u001b[38;5;129;01min\u001b[39;00m hpo_sweep_jobs.items():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     completed_job = \u001b[43msubmit_and_wait_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msweep_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     validate_hpo_sweep_job(completed_job, backbone)\n\u001b[32m     31\u001b[39m     hpo_completed_jobs[backbone] = completed_job\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36msubmit_and_wait_for_job\u001b[39m\u001b[34m(ml_client, job)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03mSubmit job and wait for completion.\u001b[39;00m\n\u001b[32m    131\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    RuntimeError: If job fails\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m submitted_job = ml_client.jobs.create_or_update(job)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmitted_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m completed_job = ml_client.jobs.get(submitted_job.name)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completed_job.status != \u001b[33m\"\u001b[39m\u001b[33mCompleted\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:858\u001b[39m, in \u001b[36mJobOperations.stream\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id=job_object.id)\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_requests_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_ops_helper.py:350\u001b[39m, in \u001b[36mstream_logs_until_completion\u001b[39m\u001b[34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    344\u001b[39m     error_message = (\n\u001b[32m    345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDetails for canceling the run can be found here: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    349\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[32m    351\u001b[39m         message=error_message,\n\u001b[32m    352\u001b[39m         target=ErrorTarget.JOB,\n\u001b[32m    353\u001b[39m         no_personal_data_message=error_message,\n\u001b[32m    354\u001b[39m         error_category=ErrorCategory.USER_ERROR,\n\u001b[32m    355\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mJobException\u001b[39m: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run"
     ]
    }
   ],
   "source": [
    "def validate_hpo_sweep_job(job: Job, backbone: str) -> None:\n",
    "    \"\"\"\n",
    "    Validate HPO sweep job completed successfully with sufficient trials.\n",
    "    \n",
    "    Args:\n",
    "        job: Completed sweep job instance\n",
    "        backbone: Backbone model name for error messages\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if job.status != \"Completed\":\n",
    "        raise ValueError(f\"HPO sweep job for {backbone} failed with status: {job.status}\")\n",
    "    \n",
    "    if not hasattr(job, \"trial_count\") or job.trial_count == 0:\n",
    "        raise ValueError(f\"HPO sweep job for {backbone} produced no trials\")\n",
    "    \n",
    "    min_expected_trials = 5\n",
    "    if job.trial_count < min_expected_trials:\n",
    "        raise ValueError(\n",
    "            f\"HPO sweep job for {backbone} only produced {job.trial_count} trial(s), \"\n",
    "            f\"expected at least {min_expected_trials}\"\n",
    "        )\n",
    "\n",
    "\n",
    "hpo_completed_jobs = {}\n",
    "\n",
    "for backbone, sweep_job in hpo_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_hpo_sweep_job(completed_job, backbone)\n",
    "    hpo_completed_jobs[backbone] = completed_job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "BEST_CONFIG_KEY = \"best_config\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trial_from_sweep(ml_client: MLClient, sweep_job: Job, objective_metric: str, goal: str) -> Optional[Job]:\n",
    "    \"\"\"\n",
    "    Get the best trial run from a completed sweep job.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        sweep_job: Completed sweep job\n",
    "        objective_metric: Metric name to optimize (e.g., \"macro-f1\")\n",
    "        goal: Optimization goal (\"maximize\" or \"minimize\")\n",
    "        \n",
    "    Returns:\n",
    "        Job: Best trial run, or None if no trials found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        trials = ml_client.jobs.list(parent_job_name=sweep_job.name)\n",
    "        trials_list = list(trials)\n",
    "        \n",
    "        if not trials_list:\n",
    "            return None\n",
    "        \n",
    "        best_trial = None\n",
    "        best_value = None\n",
    "        \n",
    "        for trial in trials_list:\n",
    "            if trial.status != \"Completed\":\n",
    "                continue\n",
    "            \n",
    "            if not hasattr(trial, \"metrics\") or not trial.metrics:\n",
    "                continue\n",
    "            \n",
    "            if objective_metric not in trial.metrics:\n",
    "                continue\n",
    "            \n",
    "            metric_value = trial.metrics[objective_metric]\n",
    "            \n",
    "            if best_value is None:\n",
    "                best_value = metric_value\n",
    "                best_trial = trial\n",
    "            elif goal == \"maximize\" and metric_value > best_value:\n",
    "                best_value = metric_value\n",
    "                best_trial = trial\n",
    "            elif goal == \"minimize\" and metric_value < best_value:\n",
    "                best_value = metric_value\n",
    "                best_trial = trial\n",
    "        \n",
    "        return best_trial\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_trial_configuration(trial: Job) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract configuration from a trial run.\n",
    "    \n",
    "    Args:\n",
    "        trial: Trial job instance\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extracted configuration including hyperparameters and metadata\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"trial_name\": trial.name,\n",
    "        \"trial_id\": trial.id,\n",
    "        \"backbone\": trial.tags.get(\"backbone\", \"unknown\"),\n",
    "        \"hyperparameters\": {},\n",
    "        \"metrics\": {},\n",
    "        \"dataset_version\": trial.tags.get(\"data_version\", configs[\"data\"][\"version\"]),\n",
    "    }\n",
    "    \n",
    "    if hasattr(trial, \"hyperparameters\") and trial.hyperparameters:\n",
    "        config[\"hyperparameters\"] = dict(trial.hyperparameters)\n",
    "    \n",
    "    if hasattr(trial, \"metrics\") and trial.metrics:\n",
    "        config[\"metrics\"] = dict(trial.metrics)\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def select_best_configuration(\n",
    "    ml_client: MLClient,\n",
    "    hpo_completed_jobs: Dict[str, Job],\n",
    "    hpo_config: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select the best configuration across all backbone sweep jobs.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        hpo_completed_jobs: Dictionary of completed sweep jobs by backbone\n",
    "        hpo_config: HPO configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        dict: Best configuration with all extracted information\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no valid trials found or selection fails\n",
    "    \"\"\"\n",
    "    objective_metric = hpo_config[\"objective\"][\"metric\"]\n",
    "    goal = hpo_config[\"objective\"][\"goal\"]\n",
    "    \n",
    "    best_trial = None\n",
    "    best_value = None\n",
    "    best_backbone = None\n",
    "    \n",
    "    for backbone, sweep_job in hpo_completed_jobs.items():\n",
    "        trial = get_best_trial_from_sweep(ml_client, sweep_job, objective_metric, goal)\n",
    "        \n",
    "        if trial is None:\n",
    "            continue\n",
    "        \n",
    "        if not hasattr(trial, \"metrics\") or objective_metric not in trial.metrics:\n",
    "            continue\n",
    "        \n",
    "        metric_value = trial.metrics[objective_metric]\n",
    "        \n",
    "        if best_value is None:\n",
    "            best_value = metric_value\n",
    "            best_trial = trial\n",
    "            best_backbone = backbone\n",
    "        elif goal == \"maximize\" and metric_value > best_value:\n",
    "            best_value = metric_value\n",
    "            best_trial = trial\n",
    "            best_backbone = backbone\n",
    "        elif goal == \"minimize\" and metric_value < best_value:\n",
    "            best_value = metric_value\n",
    "            best_trial = trial\n",
    "            best_backbone = backbone\n",
    "    \n",
    "    if best_trial is None:\n",
    "        raise ValueError(\"No valid trials found in any sweep job\")\n",
    "    \n",
    "    best_config = extract_trial_configuration(best_trial)\n",
    "    best_config[\"selection_criteria\"] = {\n",
    "        \"metric\": objective_metric,\n",
    "        \"goal\": goal,\n",
    "        \"best_value\": best_value,\n",
    "        \"backbone\": best_backbone,\n",
    "    }\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "\n",
    "best_configuration = select_best_configuration(\n",
    "    ml_client=ml_client,\n",
    "    hpo_completed_jobs=hpo_completed_jobs,\n",
    "    hpo_config=configs[\"hpo\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_best_configuration(best_config: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Log the best configuration selection for reproducibility.\n",
    "    \n",
    "    Args:\n",
    "        best_config: Best configuration dictionary\n",
    "    \"\"\"\n",
    "    selection_criteria = best_config[\"selection_criteria\"]\n",
    "    \n",
    "    print(f\"Best Configuration Selected:\")\n",
    "    print(f\"  Backbone: {selection_criteria['backbone']}\")\n",
    "    print(f\"  Metric: {selection_criteria['metric']} = {selection_criteria['best_value']:.4f}\")\n",
    "    print(f\"  Hyperparameters: {best_config['hyperparameters']}\")\n",
    "    print(f\"  Dataset Version: {best_config['dataset_version']}\")\n",
    "    print(f\"  Trial: {best_config['trial_name']}\")\n",
    "\n",
    "\n",
    "log_best_configuration(best_configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TRAINING_JOB_NAME = \"final-training\"\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_training_config(\n",
    "    best_config: Dict[str, Any],\n",
    "    train_config: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build final training configuration by merging best HPO config with train.yaml defaults.\n",
    "    \n",
    "    Args:\n",
    "        best_config: Best configuration from HPO selection\n",
    "        train_config: Training defaults from train.yaml\n",
    "        \n",
    "    Returns:\n",
    "        dict: Final training configuration\n",
    "    \"\"\"\n",
    "    hyperparameters = best_config.get(\"hyperparameters\", {})\n",
    "    \n",
    "    final_config = {\n",
    "        \"backbone\": best_config[\"backbone\"],\n",
    "        \"learning_rate\": hyperparameters.get(\"learning_rate\", train_config[\"training\"][\"learning_rate\"]),\n",
    "        \"batch_size\": hyperparameters.get(\"batch_size\", train_config[\"training\"][\"batch_size\"]),\n",
    "        \"dropout\": hyperparameters.get(\"dropout\", train_config[\"training\"].get(\"dropout\", 0.1)),\n",
    "        \"weight_decay\": hyperparameters.get(\"weight_decay\", train_config[\"training\"][\"weight_decay\"]),\n",
    "        \"epochs\": train_config[\"training\"][\"epochs\"],\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"early_stopping_enabled\": False,\n",
    "        \"use_combined_data\": True,\n",
    "    }\n",
    "    \n",
    "    return final_config\n",
    "\n",
    "\n",
    "final_training_config = build_final_training_config(best_configuration, configs[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_training_job(\n",
    "    ml_client: MLClient,\n",
    "    script_path: Path,\n",
    "    data_asset: Data,\n",
    "    environment: Environment,\n",
    "    compute_cluster: str,\n",
    "    final_config: Dict[str, Any],\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> command:\n",
    "    \"\"\"\n",
    "    Create final training Azure ML Command Job with best HPO configuration.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        script_path: Path to training script\n",
    "        data_asset: Registered data asset\n",
    "        environment: Training environment\n",
    "        compute_cluster: Compute cluster name\n",
    "        final_config: Final training configuration\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        command: Azure ML Command Job definition\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If training script does not exist\n",
    "    \"\"\"\n",
    "    if not script_path.exists():\n",
    "        raise FileNotFoundError(f\"Training script not found: {script_path}\")\n",
    "    \n",
    "    command_args = (\n",
    "        f\"--data-asset ${{{{inputs.data}}}} \"\n",
    "        f\"--config-dir ../config \"\n",
    "        f\"--backbone {final_config['backbone']} \"\n",
    "        f\"--learning-rate {final_config['learning_rate']} \"\n",
    "        f\"--batch-size {final_config['batch_size']} \"\n",
    "        f\"--dropout {final_config['dropout']} \"\n",
    "        f\"--weight-decay {final_config['weight_decay']} \"\n",
    "        f\"--epochs {final_config['epochs']} \"\n",
    "        f\"--random-seed {final_config['random_seed']} \"\n",
    "        f\"--early-stopping-enabled {str(final_config['early_stopping_enabled']).lower()} \"\n",
    "        f\"--use-combined-data {str(final_config['use_combined_data']).lower()}\"\n",
    "    )\n",
    "    \n",
    "    default_datastore = ml_client.datastores.get_default()\n",
    "    if \"/paths/\" in data_asset.path:\n",
    "        relative_path = data_asset.path.split(\"/paths/\", 1)[1].rstrip('/')\n",
    "        data_path = f\"azureml://datastores/{default_datastore.name}/paths/{relative_path}\"\n",
    "    else:\n",
    "        data_path = data_asset.path.rstrip('/')\n",
    "    \n",
    "    data_input = Input(type=\"uri_folder\", path=data_path)\n",
    "    \n",
    "    job = command(\n",
    "        code=\"../src\",\n",
    "        command=f\"python {script_path.name} {command_args}\",\n",
    "        inputs={\n",
    "            \"data\": data_input,\n",
    "        },\n",
    "        environment=environment,\n",
    "        compute=compute_cluster,\n",
    "        experiment_name=configs[\"env\"][\"logging\"][\"experiment_name\"],\n",
    "        tags={\n",
    "            **config_metadata,\n",
    "            \"job_type\": \"final_training\",\n",
    "            \"backbone\": final_config[\"backbone\"],\n",
    "            \"best_trial\": best_configuration[\"trial_name\"],\n",
    "            \"best_metric_value\": str(best_configuration[\"selection_criteria\"][\"best_value\"]),\n",
    "        },\n",
    "        display_name=FINAL_TRAINING_JOB_NAME,\n",
    "        description=\"Final production training with best HPO configuration\",\n",
    "    )\n",
    "    \n",
    "    return job\n",
    "\n",
    "\n",
    "final_training_job = create_final_training_job(\n",
    "    ml_client=ml_client,\n",
    "    script_path=TRAINING_SCRIPT_PATH,\n",
    "    data_asset=data_asset,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=compute_cluster_name,\n",
    "    final_config=final_training_config,\n",
    "    configs=configs,\n",
    "    config_metadata=config_metadata,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_final_training_job(job: Job) -> None:\n",
    "    \"\"\"\n",
    "    Validate final training job completed successfully with required outputs.\n",
    "    \n",
    "    Args:\n",
    "        job: Completed job instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if job.status != \"Completed\":\n",
    "        raise ValueError(f\"Final training job failed with status: {job.status}\")\n",
    "    \n",
    "    if not hasattr(job, \"outputs\") or not job.outputs:\n",
    "        raise ValueError(\"Final training job produced no outputs\")\n",
    "    \n",
    "    required_outputs = [\"checkpoint\"]\n",
    "    for output_name in required_outputs:\n",
    "        if output_name not in job.outputs:\n",
    "            raise ValueError(f\"Final training job missing required output: {output_name}\")\n",
    "\n",
    "\n",
    "final_training_completed_job = submit_and_wait_for_job(ml_client, final_training_job)\n",
    "validate_final_training_job(final_training_completed_job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION_SCRIPT_PATH = Path(\"../src/convert_to_onnx.py\")\n",
    "CONVERSION_JOB_NAME = \"model-conversion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_output_from_training_job(training_job: Job):\n",
    "    \"\"\"\n",
    "    Get checkpoint output object from completed training job.\n",
    "    \n",
    "    Args:\n",
    "        training_job: Completed training job\n",
    "        \n",
    "    Returns:\n",
    "        Checkpoint output object\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If checkpoint not found in job outputs\n",
    "    \"\"\"\n",
    "    if not hasattr(training_job, \"outputs\") or not training_job.outputs:\n",
    "        raise ValueError(\"Training job produced no outputs\")\n",
    "    \n",
    "    if \"checkpoint\" not in training_job.outputs:\n",
    "        raise ValueError(\"Training job missing 'checkpoint' output\")\n",
    "    \n",
    "    return training_job.outputs[\"checkpoint\"]\n",
    "\n",
    "\n",
    "checkpoint_output = get_checkpoint_output_from_training_job(final_training_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversion_job(\n",
    "    ml_client: MLClient,\n",
    "    script_path: Path,\n",
    "    checkpoint_output,\n",
    "    environment: Environment,\n",
    "    compute_cluster: str,\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    "    best_config: Dict[str, Any],\n",
    ") -> command:\n",
    "    \"\"\"\n",
    "    Create Azure ML Command Job for model conversion to ONNX with int8 quantization.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        script_path: Path to conversion script\n",
    "        checkpoint_output: Checkpoint output from training job\n",
    "        environment: Training environment (reused for conversion)\n",
    "        compute_cluster: CPU compute cluster name\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        best_config: Best configuration from HPO selection\n",
    "        \n",
    "    Returns:\n",
    "        command: Azure ML Command Job definition\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If conversion script does not exist\n",
    "    \"\"\"\n",
    "    if not script_path.exists():\n",
    "        raise FileNotFoundError(f\"Conversion script not found: {script_path}\")\n",
    "    \n",
    "    command_args = (\n",
    "        f\"--checkpoint-path ${{{{inputs.checkpoint}}}} \"\n",
    "        f\"--config-dir ../config \"\n",
    "        f\"--backbone {best_config['backbone']} \"\n",
    "        f\"--output-dir ${{{{outputs.onnx_model}}}} \"\n",
    "        f\"--quantize-int8 \"\n",
    "        f\"--run-smoke-test\"\n",
    "    )\n",
    "    \n",
    "    job = command(\n",
    "        code=\"../src\",\n",
    "        command=f\"python {script_path.name} {command_args}\",\n",
    "        inputs={\n",
    "            \"checkpoint\": checkpoint_output,\n",
    "        },\n",
    "        outputs={\n",
    "            \"onnx_model\": None,\n",
    "        },\n",
    "        environment=environment,\n",
    "        compute=compute_cluster,\n",
    "        experiment_name=configs[\"env\"][\"logging\"][\"experiment_name\"],\n",
    "        tags={\n",
    "            **config_metadata,\n",
    "            \"job_type\": \"model_conversion\",\n",
    "            \"backbone\": best_config[\"backbone\"],\n",
    "            \"source_training_job\": final_training_completed_job.name,\n",
    "            \"quantization\": \"int8\",\n",
    "        },\n",
    "        display_name=CONVERSION_JOB_NAME,\n",
    "        description=\"Convert PyTorch checkpoint to optimized ONNX model (int8 quantized)\",\n",
    "    )\n",
    "    \n",
    "    return job\n",
    "\n",
    "\n",
    "conversion_cluster_name = configs[\"env\"][\"compute\"][\"conversion_cluster\"]\n",
    "conversion_job = create_conversion_job(\n",
    "    ml_client=ml_client,\n",
    "    script_path=CONVERSION_SCRIPT_PATH,\n",
    "    checkpoint_output=checkpoint_output,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=conversion_cluster_name,\n",
    "    configs=configs,\n",
    "    config_metadata=config_metadata,\n",
    "    best_config=best_configuration,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_conversion_job(job: Job) -> None:\n",
    "    \"\"\"\n",
    "    Validate conversion job completed successfully with required ONNX model output.\n",
    "    \n",
    "    Args:\n",
    "        job: Completed job instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if job.status != \"Completed\":\n",
    "        raise ValueError(f\"Conversion job failed with status: {job.status}\")\n",
    "    \n",
    "    if not hasattr(job, \"outputs\") or not job.outputs:\n",
    "        raise ValueError(\"Conversion job produced no outputs\")\n",
    "    \n",
    "    if \"onnx_model\" not in job.outputs:\n",
    "        raise ValueError(\"Conversion job missing required output: onnx_model\")\n",
    "    \n",
    "    onnx_output = job.outputs[\"onnx_model\"]\n",
    "    if hasattr(onnx_output, \"path\"):\n",
    "        onnx_path = onnx_output.path\n",
    "    elif isinstance(onnx_output, str):\n",
    "        onnx_path = onnx_output\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ONNX output type: {type(onnx_output)}\")\n",
    "    \n",
    "    if not onnx_path or not onnx_path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid ONNX model path: {onnx_path}\")\n",
    "\n",
    "\n",
    "conversion_completed_job = submit_and_wait_for_job(ml_client, conversion_job)\n",
    "validate_conversion_job(conversion_completed_job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-5: Model Registration (The Handover)\n",
    "\n",
    "Register the optimized ONNX model in Azure ML Model Registry with full metadata for production deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "MODEL_NAME = \"resume-ner-onnx\"\n",
    "PROD_STAGE = \"prod\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onnx_model_path(conversion_job: Job) -> str:\n",
    "    \"\"\"\n",
    "    Get ONNX model path from completed conversion job.\n",
    "    \n",
    "    Args:\n",
    "        conversion_job: Completed conversion job\n",
    "        \n",
    "    Returns:\n",
    "        str: ONNX model path (Azure ML datastore URI)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If ONNX model not found in job outputs\n",
    "    \"\"\"\n",
    "    if not hasattr(conversion_job, \"outputs\") or not conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job produced no outputs\")\n",
    "    \n",
    "    if \"onnx_model\" not in conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job missing 'onnx_model' output\")\n",
    "    \n",
    "    onnx_output = conversion_job.outputs[\"onnx_model\"]\n",
    "    \n",
    "    if hasattr(onnx_output, \"path\"):\n",
    "        return onnx_output.path\n",
    "    elif isinstance(onnx_output, str):\n",
    "        return onnx_output\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ONNX output type: {type(onnx_output)}\")\n",
    "\n",
    "\n",
    "onnx_model_path = get_onnx_model_path(conversion_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_version(best_config: Dict[str, Any], config_hashes: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Compute deterministic model version from configuration hashes.\n",
    "    \n",
    "    Args:\n",
    "        best_config: Best configuration from HPO selection\n",
    "        config_hashes: Configuration hashes dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Model version string\n",
    "    \"\"\"\n",
    "    version_components = [\n",
    "        config_hashes[\"data\"],\n",
    "        config_hashes[\"model\"],\n",
    "        config_hashes[\"train\"],\n",
    "        best_config[\"backbone\"],\n",
    "    ]\n",
    "    version_str = \"_\".join(version_components)\n",
    "    version_hash = hashlib.sha256(version_str.encode()).hexdigest()[:CONFIG_HASH_LENGTH]\n",
    "    return f\"v{version_hash}\"\n",
    "\n",
    "\n",
    "model_version = compute_model_version(best_configuration, config_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_production_model(\n",
    "    ml_client: MLClient,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    model_path: str,\n",
    "    best_config: Dict[str, Any],\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Register optimized ONNX model in Azure ML Model Registry.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        model_name: Model name in registry\n",
    "        model_version: Model version\n",
    "        model_path: Path to ONNX model (Azure ML datastore URI)\n",
    "        best_config: Best configuration from HPO selection\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        Model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If model path is invalid\n",
    "    \"\"\"\n",
    "    if not model_path or not model_path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid ONNX model path: {model_path}\")\n",
    "    \n",
    "    selection_criteria = best_config[\"selection_criteria\"]\n",
    "    \n",
    "    model_description = (\n",
    "        f\"Production ONNX model for Resume NER. \"\n",
    "        f\"Backbone: {selection_criteria['backbone']}, \"\n",
    "        f\"Metric: {selection_criteria['metric']}={selection_criteria['best_value']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    model_tags = {\n",
    "        **config_metadata,\n",
    "        \"stage\": PROD_STAGE,\n",
    "        \"backbone\": selection_criteria[\"backbone\"],\n",
    "        \"metric\": selection_criteria[\"metric\"],\n",
    "        \"metric_value\": str(selection_criteria[\"best_value\"]),\n",
    "        \"dataset_version\": best_config[\"dataset_version\"],\n",
    "        \"model_format\": \"onnx\",\n",
    "        \"quantization\": \"int8\",\n",
    "        \"source_training_job\": final_training_completed_job.name,\n",
    "        \"source_conversion_job\": conversion_completed_job.name,\n",
    "    }\n",
    "    \n",
    "    model = Model(\n",
    "        name=model_name,\n",
    "        version=model_version,\n",
    "        description=model_description,\n",
    "        path=model_path,\n",
    "        tags=model_tags,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        existing_model = ml_client.models.get(name=model_name, version=model_version)\n",
    "        return existing_model\n",
    "    except ResourceNotFoundError:\n",
    "        return ml_client.models.create_or_update(model)\n",
    "\n",
    "\n",
    "registered_model = register_production_model(\n",
    "    ml_client=ml_client,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_version=model_version,\n",
    "    model_path=onnx_model_path,\n",
    "    best_config=best_configuration,\n",
    "    configs=configs,\n",
    "    config_metadata=config_metadata,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_registered_model(model: Model) -> None:\n",
    "    \"\"\"\n",
    "    Validate registered model has required metadata and tags.\n",
    "    \n",
    "    Args:\n",
    "        model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    required_tags = [\"stage\", \"backbone\", \"metric\", \"dataset_version\"]\n",
    "    for tag in required_tags:\n",
    "        if tag not in model.tags:\n",
    "            raise ValueError(f\"Registered model missing required tag: {tag}\")\n",
    "    \n",
    "    if model.tags.get(\"stage\") != PROD_STAGE:\n",
    "        raise ValueError(f\"Model stage must be '{PROD_STAGE}', got: {model.tags.get('stage')}\")\n",
    "    \n",
    "    if not model.path or not model.path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid model path: {model.path}\")\n",
    "\n",
    "\n",
    "validate_registered_model(registered_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Registered Successfully:\")\n",
    "print(f\"  Name: {registered_model.name}\")\n",
    "print(f\"  Version: {registered_model.version}\")\n",
    "print(f\"  Path: {registered_model.path}\")\n",
    "print(f\"  Stage: {registered_model.tags.get('stage')}\")\n",
    "print(f\"  Backbone: {registered_model.tags.get('backbone')}\")\n",
    "print(f\"  Metric: {registered_model.tags.get('metric')} = {registered_model.tags.get('metric_value')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
